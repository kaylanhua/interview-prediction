{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from typing import Dict, Tuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaylahuang/opt/anaconda3/envs/mercor/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "five_examples = [\n",
    "    {\n",
    "        \"question\": \"It sounds like you've implemented a straightforward tokenization strategy for your project, leveraging both default and custom tokenizers to handle the specific needs of your data. Given this experience, could you explain the concept of transfer learning in the context of natural language processing and how it might be applied to your pseudocode to code generation task?\",\n",
    "        \"answer\": \"\"\"\n",
    "short transfer learning is a very common process not just when LP but also for A computer vision as well, but transfer learning has gained its popularity in llms right now. So the point of transfer learning is when you have a particular deep learning model you train it on lots of data set the right. For example, let's take the ULM fit paper, which I know very summary because briefly because that was very pivotal. So this particular paper just took a large language model to some extent Transformers and training it on Wikipedia, right? So I just change the prediction next for prediction and sometimes even fill in the blacks prediction, right? So the idea is just to train the model on a large Corpus of data set. What this does is it generalize a model on a data set and for NLP Publishers, at least make sure that the model. Knows a lot of things about the language like English. It was a few string together sentences. And so give them this when you take this particular model and then you find your network to a particular task you bringing the experience in the weights of the model from previous huge task and then making sure it's focused on us. For example, we take the first one that is what a trained on Wikipedia data set. He was data set for next what prediction and we take these weights and we change the head of the classification of the transformer change it to something else. Say maybe a sentiment prediction very simple. This is subset of this because that's a entire language model that's predicting next to it. But this one. It just uses a few sentences and so on. To protect what the sentiment is now where this previous knowledge. It becomes very easy and the training converges very easy simpler in this particular product. So what we usually do is we freeze a few weeks not all the layers maybe Top Gear layers and then we find unit and this basically uses previous information and then fine tunes and better. So this makes the train faster and the train converges better.\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"That's a comprehensive explanation, thank you. Now, considering your experience with NLP and the development of a chatbot for real estate agents, could you walk me through how you approached the challenge of training and fine-tuning the LLM model for this specific domain, and what kind of performance metrics you found most valuable in evaluating the chatbot's effectiveness?\",\n",
    "        \"answer\": \"\"\"\n",
    "Right. So in this scenario, we incorporated a rasa NLP based model, which was an intent and entity based model. It had a two-part solution for fine tuning Incorporated to or two sections. The first was to train the model for the intent recognition when we had just a handful of intents that is to identify. Whether a query was a listing based query whether the query required solution for real estate agents or whether open houses or mortgage related questions. These were the questions that we needed intent recognition for so it became fairly easy to provide examples of intense belonging to each of these in each of these classes each of these labels and along with that for for the entities that we wanted to extract. We had already composed already had an extensive database wherein we recorded all sorts of Different entities that were relevant in different queries like locations language is that language is that real estate agents spoke or specific? Zip codes are neighborhoods where the listings were located in the kind of community that the particular address was a part of Etc. So all of these different parts different entities became the training data for us and Using these data we find the rasa and you along with the rasa and will be libraries itself. We utilize these this data to find you in the model and just better support our use case which was the compass spot. So the bot operated in this manner wherein it could identify messages on the agency the post that agents made in a LinkedIn Lake Network, which was called Asian feed and it answered or recommended solutions to their queries as comments another section. Another front where the compost bought was very handy was on in real estate agent agent base slack groups wherein it could identify the entrance of the messages of the queries that uses posted and then answer these queries with a bunch of recommendations by identifying the intense of these queries and then providing some a list of top 10 recommendations for each of these queries.\n",
    "\"\"\",\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"question\": \"Great! I see from your resume that you've worked on a project involving one-shot learning for medical image analysis. Could you walk me through the approach you took for this project and how you implemented it using TensorFlow?\",\n",
    "        \"answer\": \"\"\"\n",
    "Sure, so the current problem that I faced doing that particular time was we had a particular data set of value of just a few hundred of labeled data set for something called chromosome analysis. So we had to build a model for classification regarding that so we couldn't go with the conventional class CNN because the problem is with such less dated always under fits and it's very difficult to generalize and the option that I took was going with contrastive learning. Which turned out to be very well at good and generalizing compared to a CNN based approach. So the approach is very simple. We had around 100 100 to 200 label data. So I built a service network, which is basically two Networks. Of the same network. So what this does is it takes an image as an input and gives you an output as an embedding. And the idea is that you give it pairs of images sometimes even triplet if you using triplet loss. So I used to close in this case. So it took three images One images called anchor image. The other image is an image, which is similar to this anchor image, right? The other image is something that's not so it's not the same classes jacket image. So what it does is it tries to create embeddings such that the distance or The euclidean distance you can say between the embeddings of the ones in the same class is minimized and the embedded distance between the embeddings of the ones and the different class is a maximized. So at the end when you convert an image with a Samus Network into embedding space it tries to minimize the distance between inter-class and Max is a difference between indoor. inter class so this is how much short learning for you used and the main benefit in this particular thing was that we were able to generalize very well, even though we had a very less data set compared to training a deep CNN which was not able to generalize later on when we got more data Weasley shifted to deep sea animals, but with this limited data one shot learning with Sam's networks worked great.\n",
    "\"\"\",\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"question\": \"Great! I see from your resume that you've worked on AI solutions for financial models at Riskfuel. Can you walk me through the approach you took to develop and deploy these AI solutions, particularly in terms of the models you chose and the challenges you faced with deployment?\",\n",
    "        \"answer\": \"\"\"\n",
    "Up, right? So the models that I developed for risk fuel corresponded to financial asset pricing or non-standard vanilla options effectively. What we did was we would replace a Monte Carlo based pricer for a bank with a regression based or a standard rectangular and that for basically solving a regression problem. What would happen is is that because it's not because it is neural network-based. It was extremely fast compared to a Monte Carlo based Chrysler and because of the matrix multiplication and nonlinear activation functions, which are easily parallelizable. You can reduce computation time from let's say a few minutes for each option price down to like less than 70 milliseconds the types of training or the models that we worked on were resnet models with residual skip connections or if we had to do generative modeling. We use something called a variational auto encoder.\n",
    "\"\"\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_examples = [\n",
    "    {\n",
    "        \"question\": \"Deploying multiple smaller models in parallel for Confidence Code calculation is a clever strategy. How do you plan to handle the integration of these multiple models' outputs and the Confidence Code calculation into a cohesive and easily interpretable response for the end user?\",\n",
    "        \"answer\": \"\"\"\n",
    "So I think in different models, like let's say we have Mistral Gemma and llama we can deploy them. There are already API Services which provide you access to different these different models. In initialized different instances for you, so that can be directly done or we can deploy these models using AWS AWS specifically instances that can be used to deploy large language models. and using these Services we can apply these models easily and if there are a lot of users interacting with these models, so AWS also provides auto-scaling autoscaling methods that automatically scales the number of requests that can be handled by a particular system. So and all the pre-processing that is done, like combining model outputs from combining outputs from different model can be done on like using a different API and kind of Ah, like creating a simple fast API based method for handling requests. So this basic first API will get the input from user create a prompt send this prompt create different prompts for different models hit different apis which are which have different models deployed on them get the results from their and then this calculates a score and create a separate. And create the final output or what we can do is once we get outputs from the smaller models, we can combine them together and ask another large language model to summarize those results or to giving these outputs how to better. What is the best response that can be provided to the users giving outputs and scores from different small skin models.\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Great! Let's dive into your resume. I see you've done some interesting work with IoT malware detection and face mask detection using deep learning. Can you elaborate on the specific deep learning models you used for these projects and why you chose them?,\",\n",
    "        \"answer\": \"\"\"\n",
    "        So first of all in my iot malware detection project, I use the I use the Deep learning CNN model which which is I used to know I tried I tried and tried to implement various models such as vgt 16 PG 19 and instruction V3 and out of them gdg, 16 gave the best accuracy. So I did some hyperparameter tuning and the and the atom Optimizer of bg16 gave the best result which was 96.3% accuracy. And in my face mask detection. I used last night 50 grass that 101 that's that institution. No Inception weekly and vgg 16 and we did 19 out of which again, we need 16 the iron model if the best I am super Optimizer, which was 19917.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"It sounds like you've had a comprehensive experience with web scraping, data cleaning, and model integration. You mentioned using the RAG (Retrieval-Augmented Generation) technique for question answering. Could you elaborate on how you implemented RAG in your project and what challenges you faced while integrating it with the data from your database?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Are retrievable augmented generation is basically we retrieve the data suppose if we take any website, we scrape the data and then clean it and then we split the data into small small chunks of sizes. And these chunks could be done using many techniques one will be like recursive character testing and there is token testing for the first time we have used with recursive character testing technique where we split the characters and until some special characters are engaged in the test, like multiple level lines symbols or something like that and these small small chunks are then I can if I if you want a general overview, they will be like 150 token or size in length. And these are given to add as 002 that will convert the test into embeddings and these embeddings are stored as postgres vectors. And whenever user gives a query in our chatbot, the chatbot is again converted as an embedding using the add as 002 and this same embedding is to the other posters DB and we made calls in the Prisma. Okay, and the similar to between the various available in the TV is done with this user query and whatever the top five we specifically chose the top five chunks which are similar to this user query. Are achieved and they were sent again back to charge EBT for generalizing them. Like we may not know like if we have got five 150 character test. After length, so these are given to rgbt for enhancing the user readability like take this five chunks of content and make it meaningful. So that meaningful answers are given back to the user and like this we have done with that static website based answers real-time answers like crypto price. And what is the latest defi data like exchanging information particular coin prices on different exchanges. Please kind of answers it can generate.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Certainly! In Python, the \"\"yield\"\" keyword is used in the context of defining a generator function. When you use \"\"yield\"\" in a function instead of \"\"return\"\", it transforms the function into a generator. The key difference is that \"\"return\"\" terminates a function and sends a specified value back to its caller, while \"\"yield\"\" pauses the function, saves its state, and then continues from where it left off the next time it's called. This ability to pause and resume execution is what makes generators memory efficient, as they don't need to store all their values in memory at once. Instead, they generate values on the fly, which can significantly reduce memory usage, especially with large datasets or computations. Given this explanation, how might you now consider using the \"\"yield\"\" keyword in your Python projects, particularly in the context of AI and ML where memory efficiency is a concern?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Yes, so I said that I'm doing a project related to Active Learning methodologies right in this actual learning methodologies. What we do is we don't train the model with the entire data set first. We train with the small Point small set of data set. We calculate the representative less information less scores using uncertainty sampling method and we Give it to the user to label them. And again, we'll retrain with the new label dataset in this country text finding information and representativeness, which is a combination of entropy marginal difference and tsne pay us similarities everything. We have to pass the many images to those functions so which is not so memory efficient. So I would like to use this New Concept which is you said as a converting the function to the generator using the ill function, I would definitely use this concept in my actual learning project which can minimize so much of my memory usage because I was using literally a 10000 300 images for my training data so I can leverage my memory efficiency by using this generator concept. Well, so thank you for that. Europe\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Great! I see from your resume that you've been working on improving demand forecasting at THOUCENTRIC. Can you tell me about a specific machine learning model you've implemented for this purpose and how you integrated it with the existing system?,\",\n",
    "        \"answer\": \"\"\"\n",
    "        See the project is for a client PMI and the objective is to find the optimal with the objective is to predict an optimal value of the illicit trade that will happen in the next three years and the data that we had was that annual level. So we had around 10 or 12 year data depending upon the markets. So for let's say for some Market. We have 10 here data while for other we have eight here data something like this. So the objective is to just predict illicit trade that will happen in the next couple of years. For this what we did was we we cannot use the complex model like LG Boost or xgboost since we have very less data points. We have around 10 data points. So the only way is to go with the regression model. We had tried with various regression model and we finally opted for the Richard creation. Now what we did over there was we created a lot of simulations on different features. We had also done an extensive feature engineering depending upon the market. So let's say there's a market which has a factory of which is a few factors that generate those. Is it tobacco's or counterfeit products then what we did was We Gather the data. From the beginning the data and asking them that what should admit which rate the factories are getting closed. So using that metric we forecasted for that is specific market now since 2020 was a covid year. We had thought of using an using a new feature as a covid. So let's say it is a category Feature Feature like anomaly yes, or false so for 2021 and 22 early we had used as true. To check with that model like how it performs like since it is a normally ER people right go with the cheaper variant rather than the original one, or if you take into account the annual household median income at data that we got it from Oxford social economic data. it gives the data for how European market and Yeah. also, we are taking into account the GDP per capita of a Nation since the it's a high. Since this let's say if the GDP is increasing constantly over in here, then most most likely that these condition of people will increase the living condition of people will increase and hence. They will buy. They will buy a original product rather than buying a counterfeit one or illicit one. Apart from it since the major things that the issue that we faced was in certain markets where the the effect of covid was quite large. So for that we had done a few extensive. Feature engineering which had included the median household income and there was one more factor for European markets like they had open borders. So let's say a protein has been manufactured in nation a they can just take the product in the cartridge and ship it to Nation B. And set it over there that let's say 50% cheap price. So we had also taking into account of the price and the data of the neighboring countries presents neighboring countries for this. Yeah, and that's it for\n",
    "        \"\"\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_five_formatter(question):\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
    "    )\n",
    "\n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=five_examples,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=\"\"\"\n",
    "Now, using the kind of quality and type of speech presented in the examples above, please answer the following question with the same level of detail and quality. Format it as if you were speaking out loud:\n",
    "\n",
    "Question: {input}\"\"\",\n",
    "        input_variables=[\"input\"],\n",
    "    )\n",
    "\n",
    "    formatter = prompt.format(input=question)\n",
    "    \n",
    "    return formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_three_formatter(question):\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    #  Use a few technical terms, but do not answer as an expert. Answer at around the level of a mediocre new engineer that is a little bit unsure of their response. \n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=three_examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"\"\"You are an interviewee who is giving a response to an interview question. Here's are some examples of how you should answer: \"\"\",\n",
    "        suffix=\"\"\"\n",
    "Now, using the kind of quality and type of speech presented in the examples above, please answer the following question as a mediocre new engineer. Keep the response strictly under 200 words, format it as if you were speaking out loud, and use run on sentences:\n",
    "\n",
    "Question: {input}\"\"\",\n",
    "        input_variables=[\"input\"],\n",
    "    )\n",
    "\n",
    "    formatter = prompt.format(input=question)\n",
    "    \n",
    "    return formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Utilizing the Mahalanobis distance and p-values for coherence testing is a sophisticated method. How did you determine the threshold for the p-value to establish the coherence of the data sources? Additionally, in the context of financial data, were there specific features or patterns you focused on to ensure that the synthetic data remained representative of the original data, especially in terms of its utility for training AI models?\"\n",
    "\n",
    "five_formatter = create_five_formatter(question)\n",
    "three_formatter = create_three_formatter(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nUh, so for the Mahalanobis distance and the p-values, uh, we uh, determined uh, the threshold uh, kind of uh, based on uh, uh, trial and error, uh, you know, like uh, we tried different uh, uh, uh, p-values and uh, looked at uh, the results uh, and uh, uh, uh, kind of uh, you know, took uh, uh, a uh, uh, an average, I guess? Uh, and uh, uh, we also uh, looked at uh, some uh, uh, uh, previous uh, uh, uh, research uh, that uh, kind of uh, um, you know, uh, used uh, similar uh, uh, uh, uh, uh, thresholds and uh, uh, kind of uh, you know, uh, uh, uh, took that uh, as a uh, uh, uh, uh, uh, starting point, uh, and uh, uh, made some uh, uh, adjustments uh, based on uh, our uh, uh, uh, uh, uh, specific uh, uh, uh, uh, uh, uh, uh, data uh, uh, and uh, as for'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "# Initialize the model\n",
    "llm = OpenAI(temperature=0.9)\n",
    "# Get the response\n",
    "three_formatter = create_three_formatter(question)\n",
    "three = llm.invoke(three_formatter)\n",
    "# five = llm.invoke(five_formatter)\n",
    "three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "three = \"Well, when using Mahalanobis distance we look at the p-value, right? And it can be tricky because there isn't like a specific set threshold for the p-value. You kinda gotta determine that yourself based on the data and what makes sense for your specific context. So for this project, I analyzed the data sources and looked at patterns in the financial data to determine the appropriate p-value threshold. And it was definitely a challenge because we wanted to make sure the synthetic data was coherent with the original data so that it could be used for training AI models. I mean, that's the whole point, right? So we focused on features and patterns in the data that were most relevant to the financial context, like maybe patterns in stock prices or interest rates or stock market fluctuations. And we made sure that the synthetic data matched up with these features and patterns to maintain its representation of the original data. It was definitely a lot of trial and error, but in the end, we were able to find a good balance between coherence and utility for AI model training.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = \"So specifically we used heuristics from Finance such as a volatility surface Arbitrage check. So volatility surface must have arbitrage-free must be arbitrage-free. So we would have checks for calendar Arbitrage and Butterfly Arbitrage that we would check to make sure that our generative data was correctly. And coherently generated the p-values were done empirically so I would create inflated data that I knew for a fact did not come from the distribution and I would check to see whether or not it's just specific p-value was able to reject AKA x amount of the time and then I would permute the data either leave some in or take some out and recalculate the p-value to see if I would go back and get the exact same decision for that value. And in some summary we came up with 0.025, which is a lot stronger than 0.05 as a metric because data from Financial authorities are Financial distributions tend to be skewed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "def llm_bool(question, responses):\n",
    "    \n",
    "    # If the second response contains more key technical terms than the first, return \"Comparable\". \n",
    "    # \"\"\"Determine the quality of the second response to a technical question. An ideal response should meaningfully discusses key technologies, methodologies, and frameworks, but also chooses an optimal level of technicality for their given use case. Return \"Worse\" if the second response is not as good as the first response. Return \"Comparable\" if the second response is comparable first response. Do not judge the response based on quality of speech, but on the technical content. \"\"\"\n",
    "# 3. **Clarity**: Is the response clear and easy to understand, given the appropriate level of technicality for the audience?\n",
    "    \n",
    "    \n",
    "    \n",
    "#     long_prompt = \"\"\"Evaluate the technical quality of the second response to a technical question compared to the first response. An ideal response should meaningfully discuss key technologies, methodologies, and frameworks. Consider the following criteria for evaluation:\n",
    "\n",
    "# 1. **Depth**: Does the response provide sufficient detail and technical depth?\n",
    "# 2. **Length**: Does the response give a thorough answer?\n",
    "# 3. **Accuracy**: Are the technologies, methodologies, and frameworks mentioned correctly and appropriately used?\n",
    "\n",
    "# Return \"Worse\" if the second response is lacking in these aspects compared to the first response. Return \"Comparable\" if the second response meets the criteria more so than the first response. Return \"Better\" if the second response is better and more technical than the first response. Do not judge the responses based on the quality of speech but strictly on the technical content.\n",
    "\n",
    "# Example Evaluation:\n",
    "# First Response: Describes how a neural network can be used for image classification, mentioning specific layers and activation functions.\n",
    "# Second Response: Discusses vague objectives without mentioning specific techniques.\n",
    "\n",
    "# In this example, the second response would be \"Worse\" due to lack of depth and technical details.\"\"\"\n",
    "\n",
    "    long_prompt = \"\"\"\n",
    "    Evaluate the technical quality of the two responses to a technical question. Focus on the use of technical terms, the clarity of the explanation, and the depth of the approach discussed. An ideal response should meaningfully discuss key technologies, methodologies, and frameworks, and choose an optimal level of technicality for the given use case. Consider the following criteria for evaluation:\n",
    "\n",
    "    1. **Technical Terms:** Does the response correctly use relevant technical terms and concepts? The better response will include many more technical terms.\n",
    "    2. **Clarity:** Is the explanation clear and easy to understand?\n",
    "    3. **Depth:** Does the response provide sufficient detail and technical depth in describing the approach? A shorter response will likely be worse than a longer one. \n",
    "\n",
    "    Based on these criteria, determine which response is better. Return \"Response 1 is better\" if the first response is better, \"Response 2 is better\" if the second response is better, and \"Comparable\" if the responses are of equal quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    options = {\n",
    "        \"description\": long_prompt,\n",
    "        \"enum\": [\"Response 1 is better\", \"Response 2 is better\", \"Comparable\"]\n",
    "        # \"enum\": [\"Better\", \"Comparable\", \"Worse\"]\n",
    "    }\n",
    "    description, enum = options[\"description\"], options[\"enum\"]\n",
    "\n",
    "    tagging_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "    You are given a question and two responses to a technical interview question to compare. \n",
    "\n",
    "    Only compare based on the properties mentioned in the 'Comparison' function.\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    First Response:\n",
    "    {first_resp}\n",
    "    \n",
    "    Second Response:\n",
    "    {second_resp}\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    class Comparison(BaseModel):\n",
    "        rating: str = Field(\n",
    "            description=description,\n",
    "            enum=enum,\n",
    "        )\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\").with_structured_output(\n",
    "        Comparison\n",
    "    )\n",
    "\n",
    "    chain = tagging_prompt | llm\n",
    "\n",
    "    result = chain.invoke({\"question\": question, \"first_resp\": responses[0], \"second_resp\": responses[1]})\n",
    "    \n",
    "    return result.rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response 1 is better'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_bool(question, responses=[three, original])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: \n",
      "\n",
      "Uh yeah, sure. So like, for the columns to index, I basically checked like which columns were most used in my queries and like also which columns had like a lot of unique values, you know, like not a lot of repeating values, so it would make it faster to search for them. And then, you know, like, I just indexed those columns and like, it definitely made a difference in the performance, you know, like, the queries ran much faster, which is great, you know, like, who wants to wait around for a query to finish? Not me. So yeah, it was definitely a good decision to index those columns, you know, like, it's just common sense, right? Like, we want things to be quick and efficient, so we gotta do what we gotta do. And, like, I didn't really face any challenges or anything, it was all pretty straightforward. So yeah, indexing definitely helped a lot, I would say. Yeah.\n",
      "\n",
      "True Response: making indexing far clearer.\n",
      "\n",
      "\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Ah yes, JMeter is definitely a popular tool for API load testing. And yeah, my experience with microservices architecture has definitely come in handy when it comes to designing communication protocols between the services. So, what I generally do is I first analyze the traffic patterns and the expected load on the services. Based on that, I choose the appropriate communication protocol, whether it's REST or RPC, to ensure smooth and reliable data exchange. And in a high-traffic scenario, I make sure to set up load balancers in front of the services to distribute the load evenly. And I also implement caching mechanisms to reduce the load on the services. Additionally, I use message queues to handle asynchronous requests and batch processing. And of course, I always keep an eye on the performance metrics and make necessary adjustments to optimize the data exchange process. Overall, the key is to have a well-designed and well-tested communication protocol in place to handle high traffic smoothly and efficiently.\n",
      "\n",
      "True Response: So we have utilized the, APIs endpoints and, the controllers we have as mentioned, have user risk controllers and the services are specifically, given the business, things capsulated between them, and we wrote the controllers that accept the, request given by the clients and the the response can be the controllers make use of the services, and then they will be, giving the, response to the client.\n",
      "\n",
      "\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Uh, well, I guess first we tried to split the data into smaller chunks because like, you know, when the datasets are like super super big, it can like slow down your script, so like, yeah, smaller chunks seem to help. And then, uh, we also tried using some special tools, like uh, Pandas and NumPy to help with the processing and stuff. And um, we also, uh, looked into ways to improve the performance of our algorithm, like hyperparameter tuning, and uh, using different optimization techniques like atom optimizer. And uh, yeah, uh, we also tried to make sure our code was well-organized and efficient, like uh, using proper data structures and avoiding unnecessary operations. And like uh, we did some testing and debugging to make sure everything was running smoothly. And if we encountered any issues, we would uh, try to uh, optimize our code or uh, maybe even use a different model or algorithm. So yeah, those were some strategies we used to uh, handle large datasets and uh, maintain optimal performance of our Python scripts. \n",
      "\n",
      "True Response: Us so when we talk about python, I just worked with a python Spyder. I was using as IDE after that to store the data I was using SQL which contains all the data, which we were importing to python as a DB file after that. We were using some inputs from CAD generated, which was in the form of Excel or these kind of input. I was storing in the Excel or DB and we were importing to Python and we were getting the image plotted by the help of matplot. Thank you.\n",
      "\n",
      "\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Oh yeah, Docker and Kubernetes, I mean they're both great tools for managing and running containers, right, but yeah, Docker is more like virtualizing your applications and managing the resources, while Kubernetes, well, it's all about orchestrating and managing those containers at scale, so yeah, I get it, I totally understand the difference, I mean, I have experience with both, and when it comes to security for Docker, well, I make sure to use secure base images, you know, with no vulnerabilities or anything, and I also limit the number of users who have access to the containers, and I mean, for building Docker images, yeah I have some best practices, like, I always keep the images lightweight and only include necessary dependencies, and I also use version control to keep track of any changes made, and, oh yeah, never ever store sensitive information in plain text, always use environment variables, you know, to keep things secure, so yeah, those are some of the things I do to ensure security and best practices for Docker containers.\n",
      "\n",
      "True Response: Oh, I'm not work on Docker security. But yeah, when I push my images to Docker Hub, I keep in mind that my login ID and password are not shared with any inappropriate user. So that's the only security I used. Also. I have also used some security security purpose in my application. For example in my project in my nodejs project. What I've done is I have used a key and I have put them in a DOT EnV file. So that cannot be patched know from application since I put them in file. So security is provided in my application. Oh, yeah.\n",
      "\n",
      "\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Oh yeah sure. So in this specific project, we were working on optimizing a query related to customer data. The query was quite complex with multiple joins and filters. And the database we were using did not have any indexes defined for the columns being used in the query. So to improve the performance of the query, we decided to implement B-tree indexing on the columns used in the query. We also used hashing on certain columns to further optimize the join operations. Before implementing these indexing strategies, the query was taking around 3 minutes to execute. But after implementing the B-tree and hashing techniques, the query execution time was reduced to just 30 seconds. So that's a significant improvement in performance. And not just that, the overall CPU usage and memory usage also decreased after implementing the indexing. This helped with the overall efficiency of the system. So yeah, that's how we applied hashing and B-tree indexing to optimize a complex query and it was really effective in improving the performance.\n",
      "\n",
      "True Response: hashing techniques.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "top_half = pd.read_csv(\"../data/train_valid_all.csv\")\n",
    "top_half = top_half[[\"Label\", \"Question\", \"Response\", \"Prediction\", \"Valid\"]]\n",
    "# top_half = top_half.groupby('Label').apply(lambda x: x.sample(n=1, random_state=1)).reset_index(drop=True)\n",
    "# top_half = top_half[top_half['Label'] == 5]\n",
    "\n",
    "# sample 1s\n",
    "run = top_half[top_half['Label'] == 1].sample(n=5, random_state=2)\n",
    "\n",
    "# sample 2s\n",
    "# top_half = top_half[top_half['Label'] == 2].sample(n=5, random_state=1)\n",
    "\n",
    "# sample 3s\n",
    "# top_half = top_half[top_half['Label'] == 3].sample(n=5, random_state=1)\n",
    "\n",
    "# sample 4 and 5\n",
    "# top_half = top_half[top_half['Label'] >= 4].sample(n=5, random_state=1)\n",
    "\n",
    "def apply_llm_bool_to_dataframe(df):\n",
    "    def process_row(row):\n",
    "        question = row[\"Question\"]\n",
    "        three_formatter = create_three_formatter(question)\n",
    "        # Run llm.invoke multiple times for each row\n",
    "        \n",
    "        results = []\n",
    "        for _ in range(5):\n",
    "            \n",
    "            generated = llm.invoke(three_formatter)\n",
    "            result = llm_bool(question, responses=[generated, row[\"Response\"]])\n",
    "            results.append(result)\n",
    "            \n",
    "            if _ == 0:\n",
    "                print(\"Generated Response:\", generated)\n",
    "                print(\"\\nTrue Response:\", row[\"Response\"])\n",
    "                print(\"\\n\\n\")\n",
    "        \n",
    "            \n",
    "        row[\"three_bool\"] = results\n",
    "        # Print one of the three generated responses and the true response\n",
    "        \n",
    "        return row\n",
    "\n",
    "    return df.apply(process_row, axis=1)\n",
    "\n",
    "top_half = apply_llm_bool_to_dataframe(run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Response 1 is better', 'Response 2 is better', 'Response 1 is better', 'Response 2 is better', 'Response 2 is better']\n",
      "['Response 1 is better', 'Comparable', 'Response 2 is better', 'Response 1 is better', 'Response 1 is better']\n",
      "['Response 1 is better', 'Comparable', 'Response 1 is better', 'Comparable', 'Comparable']\n",
      "['Response 1 is better', 'Response 1 is better', 'Response 1 is better', 'Response 1 is better', 'Response 1 is better']\n",
      "['Response 1 is better', 'Response 1 is better', 'Response 1 is better', 'Response 1 is better', 'Response 1 is better']\n"
     ]
    }
   ],
   "source": [
    "for i in top_half[\"three_bool\"]:\n",
    "    print(i)\n",
    "# one_top_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Question</th>\n",
       "      <th>Response</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Valid</th>\n",
       "      <th>three_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>It's clear you have a good understanding of in...</td>\n",
       "      <td>making indexing far clearer.</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>[Response 1 is better, Response 2 is better, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Using JMeter for API load testing is a solid a...</td>\n",
       "      <td>So we have utilized the, APIs endpoints and, t...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>[Response 1 is better, Comparable, Response 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>1</td>\n",
       "      <td>Given your experience with integrating Python ...</td>\n",
       "      <td>Us so when we talk about python, I just worked...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>[Response 1 is better, Comparable, Response 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>You've given a clear distinction between Docke...</td>\n",
       "      <td>Oh, I'm not work on Docker security. But yeah,...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>[Response 1 is better, Response 1 is better, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1</td>\n",
       "      <td>It seems like you were about to explain how yo...</td>\n",
       "      <td>hashing techniques.</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>[Response 1 is better, Response 1 is better, R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                           Question  \\\n",
       "40       1  It's clear you have a good understanding of in...   \n",
       "10       1  Using JMeter for API load testing is a solid a...   \n",
       "604      1  Given your experience with integrating Python ...   \n",
       "17       1  You've given a clear distinction between Docke...   \n",
       "598      1  It seems like you were about to explain how yo...   \n",
       "\n",
       "                                              Response  Prediction  Valid  \\\n",
       "40                        making indexing far clearer.           2   True   \n",
       "10   So we have utilized the, APIs endpoints and, t...           2   True   \n",
       "604  Us so when we talk about python, I just worked...           2   True   \n",
       "17   Oh, I'm not work on Docker security. But yeah,...           2   True   \n",
       "598                                hashing techniques.           2   True   \n",
       "\n",
       "                                            three_bool  \n",
       "40   [Response 1 is better, Response 2 is better, R...  \n",
       "10   [Response 1 is better, Comparable, Response 2 ...  \n",
       "604  [Response 1 is better, Comparable, Response 1 ...  \n",
       "17   [Response 1 is better, Response 1 is better, R...  \n",
       "598  [Response 1 is better, Response 1 is better, R...  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mercor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
