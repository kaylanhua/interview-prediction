sure, so for the production of machine learning and AI models the most important thing is the backend that I use for that for machine learning the backend Frameworks exist are flask. Or Django and there are other platforms also, so deploying machine learning and a models in production requirement requires careful planning and consideration to ensure they perform well and are scalable model development before deployment. I thoroughly developed and evaluate the machine learning modeling sure. It meets the desired performance metrics in generalization capability capabilities. So after I design the model architecture with them with scalability in mind considering factors such as Computational efficiency memory usage and parallelization capabilities techniques like model parallelism distributed training and batch processing are employed to handle large volumes of data and scale the model to meet the production demands. I configure the deployment infrastructure including Cloud platforms like AWS as your or on premises services to support their deployment and execution of the machine learning model.

Right. So gcp is Google Cloud platform and that this platform gives us a lot of models in build on it. So we have these models like Mistral Mistral Moe mixture of experts and lot of other models which are already inbuilt deployed on this so that to make use of these models is pretty easy for us. There is all in a you know, in a overall project. There are a lot of things to you know handle like there is scaling models scaling when to you know, use a lot of gpus when to decrease the GPU. It depends on the number of people are there so that all these kinds of things are already taken care by the gcp so that we can we can try to focus on a lot of other important stuff the Technical Machine learning stuff. So gcp does all these things like we have these vertex AI which has these all inbuilt morals in on it and we try To use them and also on top of it I guess gcp is a partnered with BLM BLM is a library which hosts these model much more better. It has these techniques of fast attention, which can make the model more faster and it can it can improve the inference time. So using gcp what we did is we have these all model techniques and all these optimization all these you know, Load balancing techniques and all is already included in it. And on top of that in the gcp platform. We can also have these databases also we can map to our model. So there are different databases. We are using in Google one is a blob storage where we can store all the CSV PDFs and all in a pocket such that and there are a lot of other storages like we can have logs we can have all these logs if there is an issue. We log all those issues and Trace them whenever we need them. We are using the gcp platform to do these logs and we also also save all the model answers so that we can you know Trace back that day, you know, we have improved the model and and we can compare our previous Model results and our new model results and check if we are improving and there are a lot of other metrics also like observation tools which gcp builds us like observation like, you know the inference Time the you know the load time how many if there is a any downtime for these models and there are a lot of other metrics and observation tools which gcp has using all these things all together and gcp gives us one environment where all these three all these things can come together and gcp also provides these interfaces where we can do a development environment. We can do the testing and we can do the production environment so that If you don't, you know mess up the production we are doing all the development in the development environment. And whenever we have something new we can push it to the testing and that you know, we can do all the testing and the testing environment and when every test case is passes we can push it to the production environment and we can do this in a cicd pipeline where continuous continuous integration and continuous deployment. Is there once we pushed in our get gcp will understand. Hey, you are pushed it's going to go automatically go into the testing environment test it out and automatically fall as clear as it can, you know do all these production all these take things take a lot of time. But if we include gcp in inside the picture these things, you know easily we can equip this thing's much more faster and better and it also provides these kubernetes cluster it also have these Docker registry where we can you know, push all the Dockers. And it also has all these kind of everything it has observability logs the in the kubernetes scaling and storage database is and all these together comes in to the gcp. So I have worked on a lot of things in gcp.

Oh, yes. So this was started as an initiative back then we didn't even know if this would work. So problem was like I have I have many friends in the sales team. So they always come to me like hey, we have to spend two to three hours in preparing sales deck. So that's when I tried to figure something out and then I tried to pitch it to my co-founder Asif. Like if there's anything we can try and that's when it tried to go with the GPT model. Of the open AI so what happened was so we went back then we didn't even know if they should work. So. We integrated like we started giving random Pro like small prompts direct prompts to the deputy model back. Then we were using the free version gt3.5. So we started with random prompts sword from which were more direct and we tried to get some output using the open a API key. So it back then the problem was like we didn't get any good results like which we can directly include in the sales deck. so then we decided to go with the one sort of few sort approaches like giving some huge cases that are already solved solved in quantify and like using to generate responses based on these examples. So that's when we try to use a few sort. But again like we were using just think like we were putting everything in just one prompt like we were stacking everything only in one prop. So that's when we got the like we first issue up the token limit output like this is certain limitation on the total number of tokens that you can pass through the model. So we test that challenge and in order to take all that we decided to like there are separate sections. There are data data data sets like what are the data? Like what is the current state of the project? What is the expected future instead like design instead of the project but the technical expertise required to complete the project like and what would be the data required completely to complete the project who are the stakeholders like we have this different sections which which are to be populated in the cell state. So right so we what we did was we tried to use a separate prompts for each section like for data section. We had a specific from for skill section we had specific from So similarly we try to do it but again like while trying to do it we faced with the problem of the like since we were using the free version of the openai API, so there's what there's a quota limit and that got exited. So then we had to buy the premium version of the openai API and that's when we got access to the GPD form model. So right now what you're doing is we are integrating GPT form model and all so with the help of apps script like we are automating the task updating the sales deck. So previously the task. Which was manually done like it was taking around 3 hours. So we are able to do it or wrap it under one minute one to two minutes. So I believe like that's a 98% efficiency goes so that's how we We we are able we were able to help ourselves team. Yeah.