,Unnamed: 0.1,Unnamed: 0,Input Text,Label,Question,Response,zero_shot,Valid,zero_shot_4o
0,0,0,"Interviewer: Achieving state-of-the-art results is impressive. How do you envision further improving the system's capability to handle a wider range of financial questions and document types, considering the limitations of dataset-specific fine-tuning?  


Interviewee: So one of the things that can be done is query expansion that is being used and retrieval augmented generate generation tasks a lot. So what happens is given a query you use another model like a large language model to expand this query identifying topics that can be helpful in answering this question or writing several writing this query in a different way. And using this expanded query to create a vector representation and this helps it to retrieve much better documents and it helps to actually Identify some topics or context that was not even trained on a data set specific fine tuning task and for prompting I think newer models are very very good on numerical reasoning. And the questions that are covered in the finger data set actually are more specific to final question answering or people who are really interested in financial question answering. So if there is a use case or if there is there a few questions that doesn't answer correctly, then we can like enhance the problem, but I think you actually covers a lot of different types of financial questions. Or we can also integrate other data sets that are related to financial question answering and integrate them into the dynamically generated prompts.",4,"Achieving state-of-the-art results is impressive. How do you envision further improving the system's capability to handle a wider range of financial questions and document types, considering the limitations of dataset-specific fine-tuning?","So one of the things that can be done is query expansion that is being used and retrieval augmented generate generation tasks a lot. So what happens is given a query you use another model like a large language model to expand this query identifying topics that can be helpful in answering this question or writing several writing this query in a different way. And using this expanded query to create a vector representation and this helps it to retrieve much better documents and it helps to actually Identify some topics or context that was not even trained on a data set specific fine tuning task and for prompting I think newer models are very very good on numerical reasoning. And the questions that are covered in the finger data set actually are more specific to final question answering or people who are really interested in financial question answering. So if there is a use case or if there is there a few questions that doesn't answer correctly, then we can like enhance the problem, but I think you actually covers a lot of different types of financial questions. Or we can also integrate other data sets that are related to financial question answering and integrate them into the dynamically generated prompts.",4,True,2
1,1,1,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,1,False,1
2,2,2,"Interviewer: Testing the security of a system, especially one involving wireless communication, can be challenging. Could you discuss any specific testing methodologies or tools you used to verify the security and reliability of your wireless traffic light system during development? Additionally, how did the results of these tests influence the design of your system?  

Interviewee: During the development phase of a radio frequency Communication System. We approach testing the security by conducting Radiologists testing and evaluating. We Simon We simulating various synchronizes to identify to identify or not and weakness in the system. We also provide penetration testing to assess a system resting against an organized access to ensure the systems security against an organized access. We implemented several measurements We Trust wrong we request wrong and protocol to protect the transmitter transmitted data. We also implemented Access Control mechanisms such as security organized and other indication process to ensure the only organized can communicate with the system furthermore. We regularly updated and Pat. This is a system software address any known security while while they're both consider regular security edited and assignments to identify and interesting enemy. Potential security is by talking this measures we aim to create a rope a robust and secure radio frequency communication system that provided the confidential integration a liability that transmitted data. Let me know if you have any further questions.",2,"Testing the security of a system, especially one involving wireless communication, can be challenging. Could you discuss any specific testing methodologies or tools you used to verify the security and reliability of your wireless traffic light system during development? Additionally, how did the results of these tests influence the design of your system?",During the development phase of a radio frequency Communication System. We approach testing the security by conducting Radiologists testing and evaluating. We Simon We simulating various synchronizes to identify to identify or not and weakness in the system. We also provide penetration testing to assess a system resting against an organized access to ensure the systems security against an organized access. We implemented several measurements We Trust wrong we request wrong and protocol to protect the transmitter transmitted data. We also implemented Access Control mechanisms such as security organized and other indication process to ensure the only organized can communicate with the system furthermore. We regularly updated and Pat. This is a system software address any known security while while they're both consider regular security edited and assignments to identify and interesting enemy. Potential security is by talking this measures we aim to create a rope a robust and secure radio frequency communication system that provided the confidential integration a liability that transmitted data. Let me know if you have any further questions.,3,True,1
3,3,3,"Interviewer: That's a great use of Docker for ensuring consistency across different development environments and for facilitating collaboration. It's clear that you understand the value of containerization in software development. Now, considering your experience with both front-end and back-end technologies, how do you ensure that the communication between the client-side and server-side is secure, especially when handling sensitive data? Can you discuss the security measures you've implemented in your projects?  

Interviewee: I'm sure the bay the main like this project of differently. It's kind of a very it's a prototype kind of that we worked on which is soon the video prior to me. I had to test it. The main aim was offered was once it gets successful in our College. We kind of have other college. We'll see if other colleges too can make need of use out of it and the very basic level of security that we are doing was it's like the JWT token itself. For example, it was very focused on two main aspects authorized authentication and authorization like authentication is basically a fire if I am if I'm the king I have a castle and I want to different people to come and visit my castle. Authentication is whether this person has been invited to my castle of to my party. Yes or no. That is how I catch these authentication. Okay, once that person has entered my after authentication is an what all he can do in my castle is authorization. So to have that enabled was basically I was using my JWT which itself had 256 bit of encryption and I was for example if person is signing in he has a context statement on the front ending of context state which what we call which returns a JWT token to our to our client site which itself can be like they really comes with a functionality where you can you know, kind of you can you can Average time to live or you can say expiration date, right? It comes with its expiration date. We can set the expression date so we can say that to have a better secure mechanism as for now. It's like I can give for one user login. I can give a maximum of 10 minutes of expression that itself can be, you know, after 10 minutes the P might the person might have to know the authenticate about putting into his password that too can be no bypass if we can like there's a second implementation. It's a furthermore implementation that I can do is refreshing the token and reasoning in the refresh token and I'm sharing it to my client again to make sure the person who's using. My website is the same person who logged in they should not be different. It's like kind of that I'm not not be a very nice idea but definitely have taken on security code this semester and learning various aspects of security. Which out for the like to implement in my projects ahead.",2,"That's a great use of Docker for ensuring consistency across different development environments and for facilitating collaboration. It's clear that you understand the value of containerization in software development. Now, considering your experience with both front-end and back-end technologies, how do you ensure that the communication between the client-side and server-side is secure, especially when handling sensitive data? Can you discuss the security measures you've implemented in your projects?","I'm sure the bay the main like this project of differently. It's kind of a very it's a prototype kind of that we worked on which is soon the video prior to me. I had to test it. The main aim was offered was once it gets successful in our College. We kind of have other college. We'll see if other colleges too can make need of use out of it and the very basic level of security that we are doing was it's like the JWT token itself. For example, it was very focused on two main aspects authorized authentication and authorization like authentication is basically a fire if I am if I'm the king I have a castle and I want to different people to come and visit my castle. Authentication is whether this person has been invited to my castle of to my party. Yes or no. That is how I catch these authentication. Okay, once that person has entered my after authentication is an what all he can do in my castle is authorization. So to have that enabled was basically I was using my JWT which itself had 256 bit of encryption and I was for example if person is signing in he has a context statement on the front ending of context state which what we call which returns a JWT token to our to our client site which itself can be like they really comes with a functionality where you can you know, kind of you can you can Average time to live or you can say expiration date, right? It comes with its expiration date. We can set the expression date so we can say that to have a better secure mechanism as for now. It's like I can give for one user login. I can give a maximum of 10 minutes of expression that itself can be, you know, after 10 minutes the P might the person might have to know the authenticate about putting into his password that too can be no bypass if we can like there's a second implementation. It's a furthermore implementation that I can do is refreshing the token and reasoning in the refresh token and I'm sharing it to my client again to make sure the person who's using. My website is the same person who logged in they should not be different. It's like kind of that I'm not not be a very nice idea but definitely have taken on security code this semester and learning various aspects of security. Which out for the like to implement in my projects ahead.",2,True,1
4,4,4,"Interviewer: Given the complexity of parameter sweeps and the need to identify the optimal configuration, how did you manage the computational resources required for such extensive testing? Additionally, can you discuss any specific tools or techniques you used to automate or streamline the process of running and analyzing these parameter sweeps, especially considering the large-scale nature of the EPYC server environment? 


Interviewee: So the resources were managed in such a way that everyone working at AMD in our team was assigned one server to us. So I was assigned a server of my own. So I did not need to share this, system or resources with anyone else. Also, to ensure that the resources are being used optimally, I used to develop code and scripts in the daytime. And when I used to log off during the night, I used to run, my workloads or my parameter sweeps so that the next day when I log in back, I have the data ready with me, so which I can analyze further. And develop my scripts accordingly. Also, to make sure since there are a lot of parameter suites to be done, what I did was I wrote wrapper scripts, which iterate through, every data point in the sweep. And, it generates data accordingly. So with the help of a single run script, or a single wrapper script, I was able to generate all the data which was required for me.",2,"Given the complexity of parameter sweeps and the need to identify the optimal configuration, how did you manage the computational resources required for such extensive testing? Additionally, can you discuss any specific tools or techniques you used to automate or streamline the process of running and analyzing these parameter sweeps, especially considering the large-scale nature of the EPYC server environment?","So the resources were managed in such a way that everyone working at AMD in our team was assigned one server to us. So I was assigned a server of my own. So I did not need to share this, system or resources with anyone else. Also, to ensure that the resources are being used optimally, I used to develop code and scripts in the daytime. And when I used to log off during the night, I used to run, my workloads or my parameter sweeps so that the next day when I log in back, I have the data ready with me, so which I can analyze further. And develop my scripts accordingly. Also, to make sure since there are a lot of parameter suites to be done, what I did was I wrote wrapper scripts, which iterate through, every data point in the sweep. And, it generates data accordingly. So with the help of a single run script, or a single wrapper script, I was able to generate all the data which was required for me.",3,True,2
5,5,5,"Interviewer: That's impressive. Dealing with computational resources and data labeling can indeed be challenging. Regarding the DeepStream application you used for model training and deployment, what specific features or functionalities did you find most useful in the context of your project, and were there any limitations you encountered with DeepStream that you had to work around?  

Interviewee: Using deep stream and you will be very enhanced our productivity of bounding box during a model rendering. So and the challenge is we face is a frame rate in the Deep stream. The frame rate is on is compared to other few other tools and Technologies we have right now. The frame rate is very low. So as of our budget are limitations we should use data stream. So we use that in future. We are thinking to upgrade our tools and technology so with we can increase the frame rate of the predictions.",2,"That's impressive. Dealing with computational resources and data labeling can indeed be challenging. Regarding the DeepStream application you used for model training and deployment, what specific features or functionalities did you find most useful in the context of your project, and were there any limitations you encountered with DeepStream that you had to work around?",Using deep stream and you will be very enhanced our productivity of bounding box during a model rendering. So and the challenge is we face is a frame rate in the Deep stream. The frame rate is on is compared to other few other tools and Technologies we have right now. The frame rate is very low. So as of our budget are limitations we should use data stream. So we use that in future. We are thinking to upgrade our tools and technology so with we can increase the frame rate of the predictions.,2,True,1
6,6,6,"Interviewer: Using synthetic data for calibration is a smart approach to initially perfect the system. How do you plan to transition from synthetic data to real-world data for calibration, and what are the expected challenges in this transition?  

Interviewee: Yeah, so first and foremost challenge that we are going to face is like knowing the location of the cameras with respect to the real world a center coordinate. So in synthetic data, like the Unreal Engine we all we can just click the camera and we know what is 3D location is but actually but in actual like real words scenario, we cannot measure accurately the distance of the campus from the center of something into 31. So this is the first challenge that we are going to face and we are tend to solve this by using the PNP problems like like we will have certain 20 to 30. 3D points in the ground and also with corresponding points in the image plane. So with the help of these points you will be able to accurately Define the 3D coordinates of the camera with respect to the real world using PNC. So we are assuming we will be able to handle this. But let's see what happens.",2,"Using synthetic data for calibration is a smart approach to initially perfect the system. How do you plan to transition from synthetic data to real-world data for calibration, and what are the expected challenges in this transition?","Yeah, so first and foremost challenge that we are going to face is like knowing the location of the cameras with respect to the real world a center coordinate. So in synthetic data, like the Unreal Engine we all we can just click the camera and we know what is 3D location is but actually but in actual like real words scenario, we cannot measure accurately the distance of the campus from the center of something into 31. So this is the first challenge that we are going to face and we are tend to solve this by using the PNP problems like like we will have certain 20 to 30. 3D points in the ground and also with corresponding points in the image plane. So with the help of these points you will be able to accurately Define the 3D coordinates of the camera with respect to the real world using PNC. So we are assuming we will be able to handle this. But let's see what happens.",3,True,2
7,7,7,"Interviewer: Given the challenges with hardware setup and the lack of community support, how did you approach debugging and resolving issues on the QCS 610 board? Additionally, what specific considerations did you have to keep in mind while porting the software to ensure it ran effectively on the embedded platform? 


Interviewee: So the challenge so the way we debug these issues was to look at the logs generated while we are installing the the software and if there are any warnings generated. We also consulted people who have worked on similar hardware before, to get their insights on what might be the issue and how do we resolve them. Coming to, coming to making sure that the software is ported correctly, we again looked at the warnings generated, and we defined very, very basic unit test cases. To make sure that the porting is complete.",2,"Given the challenges with hardware setup and the lack of community support, how did you approach debugging and resolving issues on the QCS 610 board? Additionally, what specific considerations did you have to keep in mind while porting the software to ensure it ran effectively on the embedded platform?","So the challenge so the way we debug these issues was to look at the logs generated while we are installing the the software and if there are any warnings generated. We also consulted people who have worked on similar hardware before, to get their insights on what might be the issue and how do we resolve them. Coming to, coming to making sure that the software is ported correctly, we again looked at the warnings generated, and we defined very, very basic unit test cases. To make sure that the porting is complete.",3,True,1
8,8,8,"Interviewer: It seems there might have been a connection issue with your response. Could you please elaborate on the challenges you faced in implementing supervised debiasing in your LLM model, and how you addressed them?  

Interviewee: Basically, the main challenge which I had faced is during the memorized memory efficient training process. So as we the model is a very large one we try to you know. quantize this",1,"It seems there might have been a connection issue with your response. Could you please elaborate on the challenges you faced in implementing supervised debiasing in your LLM model, and how you addressed them?","Basically, the main challenge which I had faced is during the memorized memory efficient training process. So as we the model is a very large one we try to you know. quantize this",2,True,1
9,9,9,"Interviewer: Data subsampling is a practical approach to managing large datasets when computational resources are limited. It's good to hear that you focus on maintaining data diversity to preserve the model's capabilities. When you're subsampling, how do you ensure that the reduced dataset still captures the necessary diversity and complexity of the original data? Could you also explain how you validate the model's performance on the full dataset after training on the subsampled data to ensure it generalizes well?  

Interviewee: here when we subsample the data, it's something it could be sometimes a human process where we could actually chose to pick and pluck it out of that huge Corpus of data and choose what we have to do with the training or sometimes we can generate generate some scripts where we get to chose. Or scrape the data from the internet from various sources which in a limited manner so that we could actually pass those data itself having the constraints of the data variety. and then later we while performing validation and all we could we could do that in the Unseen data or so that we could Actually get to know how the language model is actually in this case is performing on the answers. I mean, it's performing its facts. I mean getting it producing its facts or information on the data could have never seen.",2,"Data subsampling is a practical approach to managing large datasets when computational resources are limited. It's good to hear that you focus on maintaining data diversity to preserve the model's capabilities. When you're subsampling, how do you ensure that the reduced dataset still captures the necessary diversity and complexity of the original data? Could you also explain how you validate the model's performance on the full dataset after training on the subsampled data to ensure it generalizes well?","here when we subsample the data, it's something it could be sometimes a human process where we could actually chose to pick and pluck it out of that huge Corpus of data and choose what we have to do with the training or sometimes we can generate generate some scripts where we get to chose. Or scrape the data from the internet from various sources which in a limited manner so that we could actually pass those data itself having the constraints of the data variety. and then later we while performing validation and all we could we could do that in the Unseen data or so that we could Actually get to know how the language model is actually in this case is performing on the answers. I mean, it's performing its facts. I mean getting it producing its facts or information on the data could have never seen.",2,True,1
10,10,10,"Interviewer: You've provided a clear distinction between supervised and unsupervised learning. Could you now describe a scenario where you would choose to use ensemble learning, and explain how it might improve the performance of a machine learning model? 

Interviewee: Can you please repeat the question?",1,"You've provided a clear distinction between supervised and unsupervised learning. Could you now describe a scenario where you would choose to use ensemble learning, and explain how it might improve the performance of a machine learning model?",Can you please repeat the question?,3,False,1
11,11,11,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,1,False,1
12,12,12,"Interviewer: It's clear that you've leveraged the AI capabilities within Google Cloud, particularly the AI Platform and its integration with LLMs, to streamline the process of entity extraction from documents. This has significantly reduced the time and effort required compared to traditional methods. Given your experience with both Python logic and LLMs, how do you approach the trade-off between using pre-trained models like GPT-3 and fine-tuning them for specific tasks, especially in the context of document processing?  

Interviewee: how you",1,"It's clear that you've leveraged the AI capabilities within Google Cloud, particularly the AI Platform and its integration with LLMs, to streamline the process of entity extraction from documents. This has significantly reduced the time and effort required compared to traditional methods. Given your experience with both Python logic and LLMs, how do you approach the trade-off between using pre-trained models like GPT-3 and fine-tuning them for specific tasks, especially in the context of document processing?",how you,1,False,1
13,13,13,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,1,False,1
14,14,14,"Interviewer: That's a fascinating approach to tackling the challenge of idiomatic expressions in NLP. The use of adapters to differentiate between literal and idiomatic usage, and the cross-linguistic evaluation, provide valuable insights into the language-specific nature of idioms. Given your experience with this research, how would you approach implementing a feature in a production-level NLP system that needs to handle idiomatic expressions accurately? Can you also touch upon how you might leverage large language models to improve the system's understanding of idioms? 

Interviewee: Um, can I go back? I actually explained the project halfway. I was in the middle of explaining the second experiment of it.",1,"That's a fascinating approach to tackling the challenge of idiomatic expressions in NLP. The use of adapters to differentiate between literal and idiomatic usage, and the cross-linguistic evaluation, provide valuable insights into the language-specific nature of idioms. Given your experience with this research, how would you approach implementing a feature in a production-level NLP system that needs to handle idiomatic expressions accurately? Can you also touch upon how you might leverage large language models to improve the system's understanding of idioms?","Um, can I go back? I actually explained the project halfway. I was in the middle of explaining the second experiment of it.",3,False,1
15,15,15,"Interviewer: That's a solid architecture using Streamlit and FastAPI for the web interface and model serving. When you pass the user input through the model, how did you ensure the model's predictions were efficient and timely? Did you implement any specific strategies for optimizing the model's inference time, such as using a pre-trained model or leveraging hardware acceleration?  

Interviewee: Yes, so there was a specific model distilled bird. It is quite lightweight and is able to run on the The end it is quite fast. So it generally takes around 8 to 10 seconds or even less than that depending upon the size of the reviews size of the queries that user asked so I mostly use that model for this prediction.",2,"That's a solid architecture using Streamlit and FastAPI for the web interface and model serving. When you pass the user input through the model, how did you ensure the model's predictions were efficient and timely? Did you implement any specific strategies for optimizing the model's inference time, such as using a pre-trained model or leveraging hardware acceleration?","Yes, so there was a specific model distilled bird. It is quite lightweight and is able to run on the The end it is quite fast. So it generally takes around 8 to 10 seconds or even less than that depending upon the size of the reviews size of the queries that user asked so I mostly use that model for this prediction.",2,True,1
16,16,16,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,3,False,1
17,17,17,"Interviewer: Understood, your experience with Google Cloud is certainly valuable. Could you elaborate on the specific Google Cloud services you found most beneficial for machine learning projects, and perhaps how you've utilized them in conjunction with large language models?  

Interviewee: Oh, seriously, I can do that. Yeah, that's again a very good question. So. In Google Cloud, we have an AI platform called vertex are platform. That's an integrated AI platform which includes all the machine learning tools. That one would need to, you know, like build models for different projects. So for llm, so let me give highlighted this example. So if you go through my resume, I have worked on two different projects one of them being raced on and another one of them being MB bank. So during Graystone with back then the llm Watson that evolved. So we like we had to spend months working on this project like trying to build python logic to you know to extract the entities accurately. But the MB bank project, which was also some best on similar use case. Like then we have to first use the Google Docs here to extract the entities from the PDF from specially the invoices. And then run the llms on them to extract our required entities. So if you see the use case like all the like both are almost similar use case, but here. our like we we didn't have to spend months of building the python logic in order to tackle different scenarios because our llm was able to handle the OCR output and we just passing a specific instructions to the llm in our prompt like hey, I need this this this entity's in my output in Json format. So the llm was able to find those specific outputs and it was able to provide us with the So, yes, llm has helped quite a lot to in like hell increasing the efficiency of our national Landing. Oh, yeah missile Landing projects.",3,"Understood, your experience with Google Cloud is certainly valuable. Could you elaborate on the specific Google Cloud services you found most beneficial for machine learning projects, and perhaps how you've utilized them in conjunction with large language models?","Oh, seriously, I can do that. Yeah, that's again a very good question. So. In Google Cloud, we have an AI platform called vertex are platform. That's an integrated AI platform which includes all the machine learning tools. That one would need to, you know, like build models for different projects. So for llm, so let me give highlighted this example. So if you go through my resume, I have worked on two different projects one of them being raced on and another one of them being MB bank. So during Graystone with back then the llm Watson that evolved. So we like we had to spend months working on this project like trying to build python logic to you know to extract the entities accurately. But the MB bank project, which was also some best on similar use case. Like then we have to first use the Google Docs here to extract the entities from the PDF from specially the invoices. And then run the llms on them to extract our required entities. So if you see the use case like all the like both are almost similar use case, but here. our like we we didn't have to spend months of building the python logic in order to tackle different scenarios because our llm was able to handle the OCR output and we just passing a specific instructions to the llm in our prompt like hey, I need this this this entity's in my output in Json format. So the llm was able to find those specific outputs and it was able to provide us with the So, yes, llm has helped quite a lot to in like hell increasing the efficiency of our national Landing. Oh, yeah missile Landing projects.",3,True,2
18,18,18,"Interviewer: Given your experience with deploying models on mobile devices, how did you optimize your models for TensorFlow Lite, specifically in terms of architecture or layer pruning to reduce model size while maintaining accuracy?  

Interviewee: Yeah, actually our first we had we researched about it. We wanted a model which had this is having very good accuracy and also with very lightweight so we use efficient before model for that. So that model is already is very light model and the Motorcycle was 14 MB. So that was fine for the Android. So I used that and so and also we use the pretend models of fine tune. We use our two layers and then we added two layers into the office network based more Affinity user as a base model and in that we train the final five layers so we can have a very light model because we were not training all the layers of the model of the pretend model of the office net. So we use only last four layers and we added two layers tool then Slayer in that data and we find the model so we got a very lightweight model for that",2,"Given your experience with deploying models on mobile devices, how did you optimize your models for TensorFlow Lite, specifically in terms of architecture or layer pruning to reduce model size while maintaining accuracy?","Yeah, actually our first we had we researched about it. We wanted a model which had this is having very good accuracy and also with very lightweight so we use efficient before model for that. So that model is already is very light model and the Motorcycle was 14 MB. So that was fine for the Android. So I used that and so and also we use the pretend models of fine tune. We use our two layers and then we added two layers into the office network based more Affinity user as a base model and in that we train the final five layers so we can have a very light model because we were not training all the layers of the model of the pretend model of the office net. So we use only last four layers and we added two layers tool then Slayer in that data and we find the model so we got a very lightweight model for that",3,True,1
19,19,19,"Interviewer: Logging input and output data, along with token consumption, is essential for performance monitoring. It's good to hear that you've used Streamlit's built-in authentication features. For the username and password implementation, did you use any specific encryption or hashing techniques to store and compare the passwords, and how did you manage the security of the authentication process to prevent common vulnerabilities like brute force attacks or password sniffing?  

Interviewee: Okay, it was a POC demo. So we did not have to for Implement that much level of security on the authentication. But yeah, there was basic encryption in the streamlit authenticator and it was doing the work on the back back side.",2,"Logging input and output data, along with token consumption, is essential for performance monitoring. It's good to hear that you've used Streamlit's built-in authentication features. For the username and password implementation, did you use any specific encryption or hashing techniques to store and compare the passwords, and how did you manage the security of the authentication process to prevent common vulnerabilities like brute force attacks or password sniffing?","Okay, it was a POC demo. So we did not have to for Implement that much level of security on the authentication. But yeah, there was basic encryption in the streamlit authenticator and it was doing the work on the back back side.",2,True,1
20,20,20,"Interviewer: Balancing computational requirements with model performance and interpretability is indeed a key consideration. Given your experience with both traditional machine learning algorithms and deep learning models, how do you approach the decision of when to use a traditional ML algorithm versus a deep learning model for a given problem, especially in the context of limited data availability?  


Interviewee: Yeah, so sometimes there are I have worked with normal basic foundational machine learning algorithms and also deep learning algorithms. So in case it depends on the type of data set we are working on in the end because if it depends on the type of data set in the sense, if it's linear, I would definitely go with linear regression and I would test out the accuracy on how I get what accuracy I get and then I would I would figure out my way to support Vector machine decision tree and random forest regressor. And then if it is a more if it is a problem statement where I need more computational power and that's the only place that I would go for deep learning because I don't want to increase the computational power and lost the interpretability for a simple. Problem if it's a more complex problem, I would go with a deep learning because even if we can do it with a foundational machine learning algorithms 2, but probably will of course we will get more accuracy in the Deep learning algorithms. It depends on the data set and the complexity of the problem and if we have limited number of data and the problem is complex. I will go with a deep learning if it's less number of data and the problem is okay. I can do the feature engineering and data augmentation and I can populate the data set and I can still work with the foundational algorithms to reduce the complexity and computational Power.",3,"Balancing computational requirements with model performance and interpretability is indeed a key consideration. Given your experience with both traditional machine learning algorithms and deep learning models, how do you approach the decision of when to use a traditional ML algorithm versus a deep learning model for a given problem, especially in the context of limited data availability?","Yeah, so sometimes there are I have worked with normal basic foundational machine learning algorithms and also deep learning algorithms. So in case it depends on the type of data set we are working on in the end because if it depends on the type of data set in the sense, if it's linear, I would definitely go with linear regression and I would test out the accuracy on how I get what accuracy I get and then I would I would figure out my way to support Vector machine decision tree and random forest regressor. And then if it is a more if it is a problem statement where I need more computational power and that's the only place that I would go for deep learning because I don't want to increase the computational power and lost the interpretability for a simple. Problem if it's a more complex problem, I would go with a deep learning because even if we can do it with a foundational machine learning algorithms 2, but probably will of course we will get more accuracy in the Deep learning algorithms. It depends on the data set and the complexity of the problem and if we have limited number of data and the problem is complex. I will go with a deep learning if it's less number of data and the problem is okay. I can do the feature engineering and data augmentation and I can populate the data set and I can still work with the foundational algorithms to reduce the complexity and computational Power.",3,True,2
21,21,21,"Interviewer: It's clear that you have a strong understanding of reinforcement learning and have delved into improving existing algorithms. Could you elaborate on the specific challenges you faced with the continuous action space and how the quantile truncated distribution approach addressed these challenges? Additionally, how did you manage the increased computational resources required for this method?  

Interviewee: How could you please repeat the first part of the question?",1,"It's clear that you have a strong understanding of reinforcement learning and have delved into improving existing algorithms. Could you elaborate on the specific challenges you faced with the continuous action space and how the quantile truncated distribution approach addressed these challenges? Additionally, how did you manage the increased computational resources required for this method?",How could you please repeat the first part of the question?,4,False,1
22,22,22,"Interviewer: Great. Could you provide some specific examples of enhancements or expansions you would consider for this tool? 


Interviewee: Sure. Here's some specific enhancement or expansions, like, real time data retrieval, enhanced personalized personalization, multimodal input output, advanced analytical and insights. Integration and external tools, improved context awareness, natural language understanding improvements, interactive tutorials and learning modules, and customizable templates and response collaborative features, advanced programming support, accessibility enhancement. Here's some imply implementing these enhancements would significantly broaden the tool's utility and make it more valuable across a variety of use cases.",2,Great. Could you provide some specific examples of enhancements or expansions you would consider for this tool?,"Sure. Here's some specific enhancement or expansions, like, real time data retrieval, enhanced personalized personalization, multimodal input output, advanced analytical and insights. Integration and external tools, improved context awareness, natural language understanding improvements, interactive tutorials and learning modules, and customizable templates and response collaborative features, advanced programming support, accessibility enhancement. Here's some imply implementing these enhancements would significantly broaden the tool's utility and make it more valuable across a variety of use cases.",3,True,1
23,23,23,"Interviewer: Thank you for sharing that. Can you provide more details on how you evaluated the performance of these models in your fraud detection tasks? What metrics did you use? 


Interviewee: Evaluation of models in fraud detection task and the mattresses that I have used are, like, there are some",1,Thank you for sharing that. Can you provide more details on how you evaluated the performance of these models in your fraud detection tasks? What metrics did you use?,"Evaluation of models in fraud detection task and the mattresses that I have used are, like, there are some",3,True,1
24,24,24,"Interviewer: Exploring AI-driven solutions for generating interactive content like images and videos to enhance student engagement is a great idea. How do you think AI could be used to tailor educational content to individual student's learning styles and pace, thereby further personalizing the learning experience?  

Interviewee: okay, so for this one and suppose if you Kind of give the previous date of the students as in like their performance. And since all of this is data points, you could technically create a machine learning model that kind of trajectory of these students and according to that. I'm pretty sure you can optimize AI models to teach at a certain Pace that suitable for a better tragically for these students.",2,"Exploring AI-driven solutions for generating interactive content like images and videos to enhance student engagement is a great idea. How do you think AI could be used to tailor educational content to individual student's learning styles and pace, thereby further personalizing the learning experience?","okay, so for this one and suppose if you Kind of give the previous date of the students as in like their performance. And since all of this is data points, you could technically create a machine learning model that kind of trajectory of these students and according to that. I'm pretty sure you can optimize AI models to teach at a certain Pace that suitable for a better tragically for these students.",3,True,1
25,25,25,"Interviewer: You've provided a clear distinction between the use cases for SQL and NoSQL databases. Now, considering your experience with OpenCV, could you describe a computer vision project you've worked on and explain how you handled any challenges related to image preprocessing or feature extraction? 

Interviewee: okay, so once we I am currently With the opencv pre-processing techniques. I applied those techniques in my lung X-ray disease classification problem also, like converting and grayscale image to RGB and RGB to grayscale image or so trying to adjust the pixels of an image and applying a filter in which is there in opencv. I've learned quite a few techniques in opencv or I'm learning quite a good I am currently at a research center imarat drdo as a AI ml machine learning intern there. So there I am learning opencv and using that opencv Library. I am learning object detection and object tracking and to Port those algorithms on the robot. So definitely in a in a matter of 15 to 20 days, I'll be learning open CV completely and Know like in the opencv. I have applied like data preprocessing techniques, like data augmentation or some converting a gray scale image to RGB or setting the pixels or getting some important data from from a particular image or reading and image plotting of images. So these basic functionality I know and the advanced functionality. I will be learning in a matter of 15 days at my organization drda where I'm doing a project on object tracking and object detection.",2,"You've provided a clear distinction between the use cases for SQL and NoSQL databases. Now, considering your experience with OpenCV, could you describe a computer vision project you've worked on and explain how you handled any challenges related to image preprocessing or feature extraction?","okay, so once we I am currently With the opencv pre-processing techniques. I applied those techniques in my lung X-ray disease classification problem also, like converting and grayscale image to RGB and RGB to grayscale image or so trying to adjust the pixels of an image and applying a filter in which is there in opencv. I've learned quite a few techniques in opencv or I'm learning quite a good I am currently at a research center imarat drdo as a AI ml machine learning intern there. So there I am learning opencv and using that opencv Library. I am learning object detection and object tracking and to Port those algorithms on the robot. So definitely in a in a matter of 15 to 20 days, I'll be learning open CV completely and Know like in the opencv. I have applied like data preprocessing techniques, like data augmentation or some converting a gray scale image to RGB or setting the pixels or getting some important data from from a particular image or reading and image plotting of images. So these basic functionality I know and the advanced functionality. I will be learning in a matter of 15 days at my organization drda where I'm doing a project on object tracking and object detection.",3,True,1
26,26,26,"Interviewer: Good use of HTTPS for data encryption. How about user authentication and authorization within the API? Did you implement any specific methods for that?  

Interviewee: Now I didn't Implement any user authentication. You'll just have a link if you click that link, you'll get to that page and you can upload an image and it will give you its prediction.",1,Good use of HTTPS for data encryption. How about user authentication and authorization within the API? Did you implement any specific methods for that?,"Now I didn't Implement any user authentication. You'll just have a link if you click that link, you'll get to that page and you can upload an image and it will give you its prediction.",2,True,1
27,27,27,"Interviewer: It's clear you've delved deep into the intricacies of the Stacking Regressor. Given your experience with this project, how do you approach the selection of models and their hyperparameters in a machine learning project to ensure the best possible accuracy and generalization to unseen data?  

Interviewee: Yes, John, actually as we all know the trial and error method is the first method that we all do in trial and error method. We have checked different regresses like a support Vector machine. And other regressors logistic regression linear regression rhinosaurus equation rigid regression lasso regression elastic net regressor too, but only decisionally request a random forest regressor and linear regression what meant to be together and you as a great prediction score because they are totally reliable to each other and they also have atomicity of their own here the data set we have is a bit of ambiguous. So we first in my project that we are doing I did a data feature selection process here. I use grammar species correlation method for categorical features and Pearson's correlation method for numerical features after doing the correlation things. I have a generated a heat map and according to the heat map. Any correlation that is more than 70% is removed. I mean any data set any data feature with the 70% correlation is remote in that way. I have downsized my In downsized the column from my data set and it helps me it helped me more in a efficient work of my predictors and also efficient predictions of my test data. in starting with lesser a hyper parameters hyperparameters means the parameters that that give more more stress to the prediction. I have choose them in such a way that they always give me a very good predicted values and test data is always give is always the is always the testing predictor. The testing outputs are always equal to the predicted outputs. If you come to the regression to regressor, I have all seen means minimum sample rate that I need to use minimum sample of the leaf notes that I need to use and when I come to the random forestry, I need to check the maximum depth. I need to use and minimum amount and in samples that I can take like how many trees of decision tree you're going to use in the random Forest. And using all this hyperparameter tuning. I have done my project a very well and it is it was a very great approachable and also commanded by my P u sine my guides and coordinators. Thank",2,"It's clear you've delved deep into the intricacies of the Stacking Regressor. Given your experience with this project, how do you approach the selection of models and their hyperparameters in a machine learning project to ensure the best possible accuracy and generalization to unseen data?","Yes, John, actually as we all know the trial and error method is the first method that we all do in trial and error method. We have checked different regresses like a support Vector machine. And other regressors logistic regression linear regression rhinosaurus equation rigid regression lasso regression elastic net regressor too, but only decisionally request a random forest regressor and linear regression what meant to be together and you as a great prediction score because they are totally reliable to each other and they also have atomicity of their own here the data set we have is a bit of ambiguous. So we first in my project that we are doing I did a data feature selection process here. I use grammar species correlation method for categorical features and Pearson's correlation method for numerical features after doing the correlation things. I have a generated a heat map and according to the heat map. Any correlation that is more than 70% is removed. I mean any data set any data feature with the 70% correlation is remote in that way. I have downsized my In downsized the column from my data set and it helps me it helped me more in a efficient work of my predictors and also efficient predictions of my test data. in starting with lesser a hyper parameters hyperparameters means the parameters that that give more more stress to the prediction. I have choose them in such a way that they always give me a very good predicted values and test data is always give is always the is always the testing predictor. The testing outputs are always equal to the predicted outputs. If you come to the regression to regressor, I have all seen means minimum sample rate that I need to use minimum sample of the leaf notes that I need to use and when I come to the random forestry, I need to check the maximum depth. I need to use and minimum amount and in samples that I can take like how many trees of decision tree you're going to use in the random Forest. And using all this hyperparameter tuning. I have done my project a very well and it is it was a very great approachable and also commanded by my P u sine my guides and coordinators. Thank",2,True,1
28,28,28,"Interviewer: Leveraging Azure Stream Analytics for real-time data processing is a great choice, and it's clear you've put a lot of thought into feature selection and engineering using a variety of techniques. The use of the AI Control platform for explainability is also commendable. Now, considering your experience with stakeholder management, can you describe a challenging situation where you had to communicate complex machine learning concepts to non-technical stakeholders, and how you ensured that they were on board with the project's objectives and outcomes?  

Interviewee: Okay, so the most important task for a particular project to be successful as stakeholder management so for doing that, let's say if we had developing a complex model, for example, we have developed a model for zero which is the largest client in Japan basically controls 51% of electricity more than 50% 50 or 60% of electricity in Japan. They created software for them out of most of the board members are non-technical and we need to specifically share them a lot of mattresses related to it. So there are some machine learning activities which basically data scientists and machine learning Engineers need to but there are a lot of there are a lot of business matrices. Let's say confusion Matrix. So let's say if they depend upon how efficiently we are able to basically tell that to use basically exploratory returns to basically show them the insight about how the data look like and how it is. Basically once the model is developed based upon it so there are some business mattresses that also needed to be shown to the customers. For example, let's say we can use classification metrics we can use error reporting we can use how much let's say if you are working on a A question answering system. So they are how much a question is answered is more important for that. We can use a lot of metrics like similarity scores and how similar the question and answers are how many question have been answered if we are using recommendation engine then conversion rate is the most important thing which is take voters to know so stakeholder management and machine learning is a crucial thing that need to be presented using power and presentation using our visualization. So there are a lot of things which we can use.",2,"Leveraging Azure Stream Analytics for real-time data processing is a great choice, and it's clear you've put a lot of thought into feature selection and engineering using a variety of techniques. The use of the AI Control platform for explainability is also commendable. Now, considering your experience with stakeholder management, can you describe a challenging situation where you had to communicate complex machine learning concepts to non-technical stakeholders, and how you ensured that they were on board with the project's objectives and outcomes?","Okay, so the most important task for a particular project to be successful as stakeholder management so for doing that, let's say if we had developing a complex model, for example, we have developed a model for zero which is the largest client in Japan basically controls 51% of electricity more than 50% 50 or 60% of electricity in Japan. They created software for them out of most of the board members are non-technical and we need to specifically share them a lot of mattresses related to it. So there are some machine learning activities which basically data scientists and machine learning Engineers need to but there are a lot of there are a lot of business matrices. Let's say confusion Matrix. So let's say if they depend upon how efficiently we are able to basically tell that to use basically exploratory returns to basically show them the insight about how the data look like and how it is. Basically once the model is developed based upon it so there are some business mattresses that also needed to be shown to the customers. For example, let's say we can use classification metrics we can use error reporting we can use how much let's say if you are working on a A question answering system. So they are how much a question is answered is more important for that. We can use a lot of metrics like similarity scores and how similar the question and answers are how many question have been answered if we are using recommendation engine then conversion rate is the most important thing which is take voters to know so stakeholder management and machine learning is a crucial thing that need to be presented using power and presentation using our visualization. So there are a lot of things which we can use.",3,True,1
29,29,29,"Interviewer: It's great to hear about your experience with different indexing techniques. Can you explain how you applied hashing or B-tree indexing in a specific assignment to optimize a query, and the measurable improvements in query performance that resulted from your optimization efforts?  


Interviewee: Okay. So",1,"It's great to hear about your experience with different indexing techniques. Can you explain how you applied hashing or B-tree indexing in a specific assignment to optimize a query, and the measurable improvements in query performance that resulted from your optimization efforts?",Okay. So,3,False,1
30,30,30,"Interviewer: Your use of Docker and Kubernetes for ensuring high availability is quite strategic. Now, let's delve into the technical skills listed on your resume. Starting with Python, can you explain how you've optimized Python code for performance, particularly in the context of your machine learning and deep learning projects?  

Interviewee: It so like in context of Michelin and deep learning the like we can we can evaluate like where the inference pipeline where it is taking time. So if it is in case in the case of General Loops or those things like we can we can first we can make sure like whether it is code is clean or not for cleanability. So once we have all those things like with then we can go and check with the inference time. The model is taking so in that like we can check like if there is no any other errors from the code side, so then it is like inference time like you can try to we can try to reduce it from the model site. So the apart from that like we need to make sure like Cody's in the courtroom standard format like with all other handling everything. So those those if we handle those things and we have less model interest time. than we will get faster response time from the",2,"Your use of Docker and Kubernetes for ensuring high availability is quite strategic. Now, let's delve into the technical skills listed on your resume. Starting with Python, can you explain how you've optimized Python code for performance, particularly in the context of your machine learning and deep learning projects?","It so like in context of Michelin and deep learning the like we can we can evaluate like where the inference pipeline where it is taking time. So if it is in case in the case of General Loops or those things like we can we can first we can make sure like whether it is code is clean or not for cleanability. So once we have all those things like with then we can go and check with the inference time. The model is taking so in that like we can check like if there is no any other errors from the code side, so then it is like inference time like you can try to we can try to reduce it from the model site. So the apart from that like we need to make sure like Cody's in the courtroom standard format like with all other handling everything. So those those if we handle those things and we have less model interest time. than we will get faster response time from the",2,True,1
31,31,31,"Interviewer: Utilizing the Bayesian Information Criterion (BIC) for Gaussian Mixture Model selection is a sophisticated approach. How did you integrate the results of the GARCH model back into the AI system for dynamic adjustment of the generative model, ensuring that the synthetic data's volatility characteristics remained in line with the original dataset?  


Interviewee: So this is where the variational autoencoder came into play. We trained one to generate volatility services. And what happens is in the vae you're able to take a surface and reduce it down to a vector of a specific length. That's much smaller than the actual surface parameterization and by doing some statistical distribution checking on the multivariate vector, you're able to get a distribution value of possible surfaces that you that could occur in the market and now you take your synthetic data that you generated you pass it into the late Vector space and then you can do some statistical Arbitrage checking like any other multivariate data set. So that's how we're able to do that.",2,"Utilizing the Bayesian Information Criterion (BIC) for Gaussian Mixture Model selection is a sophisticated approach. How did you integrate the results of the GARCH model back into the AI system for dynamic adjustment of the generative model, ensuring that the synthetic data's volatility characteristics remained in line with the original dataset?","So this is where the variational autoencoder came into play. We trained one to generate volatility services. And what happens is in the vae you're able to take a surface and reduce it down to a vector of a specific length. That's much smaller than the actual surface parameterization and by doing some statistical distribution checking on the multivariate vector, you're able to get a distribution value of possible surfaces that you that could occur in the market and now you take your synthetic data that you generated you pass it into the late Vector space and then you can do some statistical Arbitrage checking like any other multivariate data set. So that's how we're able to do that.",4,True,2
32,32,32,"Interviewer: It's great to hear how you leveraged TypeScript's performance and type safety features for your API work. Given your experience with both Python and TypeScript, how do you ensure that the APIs you develop are secure, especially when handling sensitive data such as crypto prices and exchange information? Can you walk me through the security measures you implemented in your API design?  

Interviewee: Essentially, what are the apis we have used? Those are open source apis. So anybody can access this data. So there is no particular need for ensuring safety or something in the project because everybody can access this like if you can just go to dayama website or you can just go to crypto. I mean coin website but what the efficiency we have achieved here is a user has to go to the website and enter a particular thing and how to sort out where he is, but with AI chatbot, which is actually a thing is automated queries needed and whatever he wants comes at his eyes without the need to call the API again go and now so I prepare responses are Json format. So Json format needs filtering and we have a filtering schema to do the same with the AI chart but that started yeah.",1,"It's great to hear how you leveraged TypeScript's performance and type safety features for your API work. Given your experience with both Python and TypeScript, how do you ensure that the APIs you develop are secure, especially when handling sensitive data such as crypto prices and exchange information? Can you walk me through the security measures you implemented in your API design?","Essentially, what are the apis we have used? Those are open source apis. So anybody can access this data. So there is no particular need for ensuring safety or something in the project because everybody can access this like if you can just go to dayama website or you can just go to crypto. I mean coin website but what the efficiency we have achieved here is a user has to go to the website and enter a particular thing and how to sort out where he is, but with AI chatbot, which is actually a thing is automated queries needed and whatever he wants comes at his eyes without the need to call the API again go and now so I prepare responses are Json format. So Json format needs filtering and we have a filtering schema to do the same with the AI chart but that started yeah.",2,True,1
33,33,33,"Interviewer: Certainly, take your time. When you're ready, I'd like to hear your explanation of the difference between TensorFlow constants and variables, and their respective use cases.  

Interviewee: yeah, but",1,"Certainly, take your time. When you're ready, I'd like to hear your explanation of the difference between TensorFlow constants and variables, and their respective use cases.","yeah, but",1,False,1
34,34,34,"Interviewer: You've provided a clear explanation of transfer learning and its application in NLP, particularly with ULMFiT. Now, could you describe a scenario where you would choose PyTorch over TensorFlow for a new project and explain your reasoning behind that choice? 

Interviewee: Oh when I faced with the choice where I'll have to implement a new paper research paper that I'm studying or thinking of the new architecture. Maybe I want to add a few more extra blocks to say an efficient Network which Gene net or resonate. I just want to add something else extra to it. I would choose python because It's very simple to add to it. So the you can just code each blocks as is and then you can add the separate ones and all of them are chained together, which means you can form the outputs. You can do lost backwards on the outputs as well. It's attached the chain. So I think whenever I need to add something change something because of the flexibility I mentioned earlier I would go for pytorch and also the main one remaining is the presence of good documentation and good community.",2,"You've provided a clear explanation of transfer learning and its application in NLP, particularly with ULMFiT. Now, could you describe a scenario where you would choose PyTorch over TensorFlow for a new project and explain your reasoning behind that choice?","Oh when I faced with the choice where I'll have to implement a new paper research paper that I'm studying or thinking of the new architecture. Maybe I want to add a few more extra blocks to say an efficient Network which Gene net or resonate. I just want to add something else extra to it. I would choose python because It's very simple to add to it. So the you can just code each blocks as is and then you can add the separate ones and all of them are chained together, which means you can form the outputs. You can do lost backwards on the outputs as well. It's attached the chain. So I think whenever I need to add something change something because of the flexibility I mentioned earlier I would go for pytorch and also the main one remaining is the presence of good documentation and good community.",2,True,1
35,35,35,"Interviewer: Correct, skewed data often warrants the use of median for imputation to mitigate the influence of outliers. Now, let's delve into the realm of Computer Vision, which you've also listed as a skill. Can you explain how you've applied transfer learning in a Computer Vision project, including the specific pre-trained models you've utilized and the new layers you've added for fine-tuning?  

Interviewee: Projects, you know in computer vision area like for learning purposes only I did computer vision projects and I have a very little knowledge about it like a very basic knowledge about it.",1,"Correct, skewed data often warrants the use of median for imputation to mitigate the influence of outliers. Now, let's delve into the realm of Computer Vision, which you've also listed as a skill. Can you explain how you've applied transfer learning in a Computer Vision project, including the specific pre-trained models you've utilized and the new layers you've added for fine-tuning?","Projects, you know in computer vision area like for learning purposes only I did computer vision projects and I have a very little knowledge about it like a very basic knowledge about it.",2,True,1
36,36,36,"Interviewer: Using Python and Pandas for data manipulation is a great choice. Since you've also mentioned proficiency in JavaScript, can you tell me about a complex problem you've solved using JavaScript, and which libraries or frameworks you used to assist you?  

Interviewee: So basically Java Java Script I used to I used to basically I used to work on JavaScript as as an angular developer. So firstly I feel difficulty to use JavaScript because angular is totally based on type script. So I I learned I learned the basics of JavaScript like Dom manipulation and other loops and other things then I shifted to type script which which have more functionality and more strict strictness then JavaScript. So I chose type script because it provides classes and other other main things. So my proficiency is in angular and I find that difficulty when I work on my e-commerce project, so I found some difficulty in there. So there is latest angular 17 is there and I currently I am working on that.",2,"Using Python and Pandas for data manipulation is a great choice. Since you've also mentioned proficiency in JavaScript, can you tell me about a complex problem you've solved using JavaScript, and which libraries or frameworks you used to assist you?","So basically Java Java Script I used to I used to basically I used to work on JavaScript as as an angular developer. So firstly I feel difficulty to use JavaScript because angular is totally based on type script. So I I learned I learned the basics of JavaScript like Dom manipulation and other loops and other things then I shifted to type script which which have more functionality and more strict strictness then JavaScript. So I chose type script because it provides classes and other other main things. So my proficiency is in angular and I find that difficulty when I work on my e-commerce project, so I found some difficulty in there. So there is latest angular 17 is there and I currently I am working on that.",2,True,1
37,37,37,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview. 

Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,1,False,1
38,38,38,"Interviewer: It's clear you had a thorough approach to maintaining code quality. Shifting focus to your coursework, you've listed Computer Architecture, Operating Systems, Machine Learning, Deep Learning, and Computer Vision. Can you discuss a project or concept from your Computer Architecture course that you found particularly challenging, and how you overcame this challenge? 


Interviewee: So the part in computer architecture, which was the most challenging for me, was when we were taught caching. Because caching as a concept is very complicated, and there are multiple layers involved to it. Also, since it involves hardware, I had to employ my previous knowledge of digital design to make sure that the concepts which I'm learning there are coherent with what I have learned before in digital design. That's one aspect which I found very confusing and computer architecture. And the solution was to read more books about it and watch more videos to understand exactly how it happens and try to correlate it with my previous knowledge to make sure everything is in at point.",2,"It's clear you had a thorough approach to maintaining code quality. Shifting focus to your coursework, you've listed Computer Architecture, Operating Systems, Machine Learning, Deep Learning, and Computer Vision. Can you discuss a project or concept from your Computer Architecture course that you found particularly challenging, and how you overcame this challenge?","So the part in computer architecture, which was the most challenging for me, was when we were taught caching. Because caching as a concept is very complicated, and there are multiple layers involved to it. Also, since it involves hardware, I had to employ my previous knowledge of digital design to make sure that the concepts which I'm learning there are coherent with what I have learned before in digital design. That's one aspect which I found very confusing and computer architecture. And the solution was to read more books about it and watch more videos to understand exactly how it happens and try to correlate it with my previous knowledge to make sure everything is in at point.",3,True,2
39,39,39,"Interviewer: Using Hugging Face's evaluation metrics for AI models is a robust way to assess performance across multiple dimensions. How do you envision further optimizing your system based on the feedback from these metrics, especially in the context of improving model accuracy without significantly increasing computational load?  

Interviewee: Like for this week and able to find tune the large language models, so it will take an input as an a query answer and the context and the prompt. So if you using this we can able to like we can able to generate a question and answer on the contacts. And after that we just give it to Anna a large language model and we can able to find you which will be more appropriate for the particular queue and decision.",1,"Using Hugging Face's evaluation metrics for AI models is a robust way to assess performance across multiple dimensions. How do you envision further optimizing your system based on the feedback from these metrics, especially in the context of improving model accuracy without significantly increasing computational load?","Like for this week and able to find tune the large language models, so it will take an input as an a query answer and the context and the prompt. So if you using this we can able to like we can able to generate a question and answer on the contacts. And after that we just give it to Anna a large language model and we can able to find you which will be more appropriate for the particular queue and decision.",2,True,1
40,40,40,"Interviewer: That's a very interesting application of computer vision and text processing. You mentioned using OpenCV for handling the PDFs with mathematical content. Could you tell me more about how you integrated OpenCV with other libraries or tools to achieve the full pipeline from PDF to HTML, especially when dealing with the challenges of mathematical expressions?  

Interviewee: Yeah, so what I am doing like we have lots of PDFs like normal test papers you can think of like SAT test papers or Jee test level. So what I'm doing is we have we have first a model which is a director on the electron to base model, which is useful layout passing. So what I'm doing is we are we are labeling everything on the PDF into different bounding boxes. Like if there is a paragraph then we are we are making a bounding box. For that paragraph and then if there is there a formulas like x square equals to something in between that and I'm using another bonding box all they take then using these labeling technique. I am able to separate text latex and I mean latest so after these we have only only text left in the PDF know and passing this open CV and passing this PDF through an opencv control where we can identify the controls based on the colors. Like if I want to read line by land and I will use Contour which has kernels only one person vertically and Lord like 200 kernels so that Can separate line by line from the text? So so now we have bounding boxes for each line and also bonding boxes for a text like latex and figure so so from this we can we have from this Body Works. We I pass it to the Tesla Model Tesla Model that OCR",2,"That's a very interesting application of computer vision and text processing. You mentioned using OpenCV for handling the PDFs with mathematical content. Could you tell me more about how you integrated OpenCV with other libraries or tools to achieve the full pipeline from PDF to HTML, especially when dealing with the challenges of mathematical expressions?","Yeah, so what I am doing like we have lots of PDFs like normal test papers you can think of like SAT test papers or Jee test level. So what I'm doing is we have we have first a model which is a director on the electron to base model, which is useful layout passing. So what I'm doing is we are we are labeling everything on the PDF into different bounding boxes. Like if there is a paragraph then we are we are making a bounding box. For that paragraph and then if there is there a formulas like x square equals to something in between that and I'm using another bonding box all they take then using these labeling technique. I am able to separate text latex and I mean latest so after these we have only only text left in the PDF know and passing this open CV and passing this PDF through an opencv control where we can identify the controls based on the colors. Like if I want to read line by land and I will use Contour which has kernels only one person vertically and Lord like 200 kernels so that Can separate line by line from the text? So so now we have bounding boxes for each line and also bonding boxes for a text like latex and figure so so from this we can we have from this Body Works. We I pass it to the Tesla Model Tesla Model that OCR",3,True,2
41,41,41,"Interviewer: Your approach to feature engineering and handling missing data through the salutation part of the name in the Titanic dataset is a great example of deriving meaningful information from seemingly unrelated features. In the context of handling categorical variables, like the ""Title"" feature you engineered from the name, what are some advanced techniques or methodologies you would consider for encoding or utilizing these categorical variables in a way that enhances the model's predictive capability?  


Interviewee: I have used Rejects and I have used a basic python formatting techniques basic python technique substring to get a substring from the string. That's the basic things from in Python. Since python is a really friendly user-friendly programming language. It helped me so much in deriving those important substrings of the feature that I was working. So basic python helped me do it without putting much of words and it but in the end of the day, it's us who should we who we who should decide what is important and what is not important by analyzing which I think analyzing and getting to know what will give you a meaningful information is more important than the python methods and python syntax. Like logic is more important than the syntax. I hope I answered your question.",1,"Your approach to feature engineering and handling missing data through the salutation part of the name in the Titanic dataset is a great example of deriving meaningful information from seemingly unrelated features. In the context of handling categorical variables, like the ""Title"" feature you engineered from the name, what are some advanced techniques or methodologies you would consider for encoding or utilizing these categorical variables in a way that enhances the model's predictive capability?","I have used Rejects and I have used a basic python formatting techniques basic python technique substring to get a substring from the string. That's the basic things from in Python. Since python is a really friendly user-friendly programming language. It helped me so much in deriving those important substrings of the feature that I was working. So basic python helped me do it without putting much of words and it but in the end of the day, it's us who should we who we who should decide what is important and what is not important by analyzing which I think analyzing and getting to know what will give you a meaningful information is more important than the python methods and python syntax. Like logic is more important than the syntax. I hope I answered your question.",2,True,1
42,42,42,"Interviewer: It seems you might need a moment to gather your thoughts. When you're prepared, please share your approach to evaluating the NLP model's performance, including any metrics you used beyond cosine similarity, and your strategies for managing false positives and false negatives. This will give me a better understanding of your problem-solving skills in the context of NLP.  

Interviewee: So exactly so we were using the Precision. Okay, and accuracy is cool. So right now the thing is that accuracy score like is what we can say false more towards the true positives and the false negatives one. So it will just give you the idea of using the true positive and false positive false negatives great. So increasing the accuracy is another task. It is one of the tasks obviously, but we had to take into account the recall also and the procedure so these are the techniques we used to like so just using one metric for this similarity search like cosine similarity in state of Wisconsin similarity accuracy precision and recover also used. I don't remember using F1 score actually. So yeah, these three words are you used?",2,"It seems you might need a moment to gather your thoughts. When you're prepared, please share your approach to evaluating the NLP model's performance, including any metrics you used beyond cosine similarity, and your strategies for managing false positives and false negatives. This will give me a better understanding of your problem-solving skills in the context of NLP.","So exactly so we were using the Precision. Okay, and accuracy is cool. So right now the thing is that accuracy score like is what we can say false more towards the true positives and the false negatives one. So it will just give you the idea of using the true positive and false positive false negatives great. So increasing the accuracy is another task. It is one of the tasks obviously, but we had to take into account the recall also and the procedure so these are the techniques we used to like so just using one metric for this similarity search like cosine similarity in state of Wisconsin similarity accuracy precision and recover also used. I don't remember using F1 score actually. So yeah, these three words are you used?",2,True,1
43,43,43,"Interviewer: Handling a memory leak and server issues during a release is a significant challenge. Given the complexity of Google's infrastructure, how did you approach the debugging process to pinpoint the source of the memory leak, and what tools or techniques did you find most effective in resolving such a critical issue?  


Interviewee: So I was the person here I didn't work on that specific bug because Google is a big company and different companies were handled by different, places I mean, different teams which are located in different offices. So I had to reach out to them who worked on that. Section and who are responsible for the memory Memorial League And they called. There was a SRE engineer and some other engineers which were direct point of contact for these kind of issues. So I just, send it to them in order to fix them, and they were usually working on that side.",1,"Handling a memory leak and server issues during a release is a significant challenge. Given the complexity of Google's infrastructure, how did you approach the debugging process to pinpoint the source of the memory leak, and what tools or techniques did you find most effective in resolving such a critical issue?","So I was the person here I didn't work on that specific bug because Google is a big company and different companies were handled by different, places I mean, different teams which are located in different offices. So I had to reach out to them who worked on that. Section and who are responsible for the memory Memorial League And they called. There was a SRE engineer and some other engineers which were direct point of contact for these kind of issues. So I just, send it to them in order to fix them, and they were usually working on that side.",2,True,1
44,44,44,"Interviewer: It's impressive how you've tackled the complexities of web scraping, especially with the variety of site structures and the challenges of multi-threading. Using Puppeteer clusters to handle exceptions and maintain a list of URLs for re-scraping is a smart solution for ensuring data integrity. Given your experience with Puppeteer and web scraping, how do you handle the potential legal and ethical issues that can arise from scraping data from websites, and what measures do you take to ensure compliance with terms of service and copyright laws?  

Interviewee: Yeah, whatever the websites we have created. We didn't scrape the without permissions our company got a strategic partnership with those websites and only those things we have created and integrated into our chatbot. And there is nothing that we have done outside of our elements.",1,"It's impressive how you've tackled the complexities of web scraping, especially with the variety of site structures and the challenges of multi-threading. Using Puppeteer clusters to handle exceptions and maintain a list of URLs for re-scraping is a smart solution for ensuring data integrity. Given your experience with Puppeteer and web scraping, how do you handle the potential legal and ethical issues that can arise from scraping data from websites, and what measures do you take to ensure compliance with terms of service and copyright laws?","Yeah, whatever the websites we have created. We didn't scrape the without permissions our company got a strategic partnership with those websites and only those things we have created and integrated into our chatbot. And there is nothing that we have done outside of our elements.",2,True,1
45,45,45,"Interviewer: Your approach to fine-tuning with a focus on ethical considerations in healthcare is commendable. The use of DPO for creating a guardrail to prevent the chatbot from providing medical advice is a critical application of AI. It's also interesting to hear about the different fine-tuning techniques, such as continuation and instruction-based fine-tuning, which are tailored to specific domains and user interactions. Given the complexity of these models and the potential for bias, how do you ensure fairness and ethical considerations in your NLP models, especially when dealing with sensitive topics like healthcare?  

Interviewee: Oh, all right. So that's you know and Healthcare you rightly said, you know for a healthcare. We need to have these all biases and we should not let the model go any wrong and that's where DPO comes in. Right? So DPO is such that it tries to reduce the bias the rejected answer and give most priority to the choose an answer. That's how the dpu train where you know, there's a right answer and there's a wrong answer and we try to you know deviate from the wrong answer and come close to the right answer so that we can reduce the biases in our model. That's one way to do it now, you know after doing it also we have these feedback mechanism to our llms what we do this what we do in this feedback mechanism is we try to take the feedback from a user where if you like the if you like the answer is going to give us a thumbs up and show some gratitude so that we understand. Hey the answer looks good, and if You dislikes it we all have this feedback very you can give us detail feedbacks and on the on that feedback. We train the model again such that it tries to, you know, also reduce the bias and and also be faithful to the customer if he's liking it or not. That's how we do it and there are a lot of other techniques and there are a lot of other metrics which we are very we have these McQ kind of a metrics what we do is we we Have a pre-trained set. Let's say we have around about 1000 questions, which we asked to these asked to the model and we already know the you know the answer to it. So it's kind of a super we converted this as a supervised metric such that given the 1000 questions. How many questions is the model is able to answer rightly and if it's not able to answer a few questions, we try to train it again such that it can also include those questions and these 1000 questions which we created on so that we can understand. These are all created. These are all questions on clinical data and medical kind of a domain so that we can understand what's going on and we can try to improve the biases of these elements.",3,"Your approach to fine-tuning with a focus on ethical considerations in healthcare is commendable. The use of DPO for creating a guardrail to prevent the chatbot from providing medical advice is a critical application of AI. It's also interesting to hear about the different fine-tuning techniques, such as continuation and instruction-based fine-tuning, which are tailored to specific domains and user interactions. Given the complexity of these models and the potential for bias, how do you ensure fairness and ethical considerations in your NLP models, especially when dealing with sensitive topics like healthcare?","Oh, all right. So that's you know and Healthcare you rightly said, you know for a healthcare. We need to have these all biases and we should not let the model go any wrong and that's where DPO comes in. Right? So DPO is such that it tries to reduce the bias the rejected answer and give most priority to the choose an answer. That's how the dpu train where you know, there's a right answer and there's a wrong answer and we try to you know deviate from the wrong answer and come close to the right answer so that we can reduce the biases in our model. That's one way to do it now, you know after doing it also we have these feedback mechanism to our llms what we do this what we do in this feedback mechanism is we try to take the feedback from a user where if you like the if you like the answer is going to give us a thumbs up and show some gratitude so that we understand. Hey the answer looks good, and if You dislikes it we all have this feedback very you can give us detail feedbacks and on the on that feedback. We train the model again such that it tries to, you know, also reduce the bias and and also be faithful to the customer if he's liking it or not. That's how we do it and there are a lot of other techniques and there are a lot of other metrics which we are very we have these McQ kind of a metrics what we do is we we Have a pre-trained set. Let's say we have around about 1000 questions, which we asked to these asked to the model and we already know the you know the answer to it. So it's kind of a super we converted this as a supervised metric such that given the 1000 questions. How many questions is the model is able to answer rightly and if it's not able to answer a few questions, we try to train it again such that it can also include those questions and these 1000 questions which we created on so that we can understand. These are all created. These are all questions on clinical data and medical kind of a domain so that we can understand what's going on and we can try to improve the biases of these elements.",3,True,2
46,46,46,"Interviewer: Thank you for sharing that detailed explanation. Can you elaborate on your role in developing the AI-powered Mental Health Fitness Tracker at Edunet Foundation? What specific technologies and methodologies did you employ? 


Interviewee: These are included in my project, and the code provides a comprehensive in the mental health digit recognition project. The main target features was a different mental illness",1,Thank you for sharing that detailed explanation. Can you elaborate on your role in developing the AI-powered Mental Health Fitness Tracker at Edunet Foundation? What specific technologies and methodologies did you employ?,"These are included in my project, and the code provides a comprehensive in the mental health digit recognition project. The main target features was a different mental illness",2,True,1
47,47,47,"Interviewer: The use of FID scores for evaluating the image quality post-compression is a solid approach. When it comes to the quantization of model weights, could you elaborate on the specific quantization techniques you applied, such as whether you used fixed-point or dynamic quantization, and how you determined the appropriate level of quantization to balance model size reduction with output fidelity?  


Interviewee: So, basically, for, reducing the model size, so the general,",1,"The use of FID scores for evaluating the image quality post-compression is a solid approach. When it comes to the quantization of model weights, could you elaborate on the specific quantization techniques you applied, such as whether you used fixed-point or dynamic quantization, and how you determined the appropriate level of quantization to balance model size reduction with output fidelity?","So, basically, for, reducing the model size, so the general,",3,False,1
48,48,48,"Interviewer: Understood, GPT-4 is indeed a powerful tool. It's great to hear that you've employed techniques like Chain of Thought prompting to maintain coherence and prevent hallucinations, especially with numerical data. Now, shifting focus a bit, I noticed you've also worked on a solution for document comparison using Google Cloud Platform. Can you describe the architecture you used for this solution and how you managed to achieve high accuracy in the comparison process?  

Interviewee: Yes, so if you are referring to the greatest and project. So what happened is so grossed out is a real estate company. So being in real estate and they they usually get high number of documents. So they have this three types of specific documents. One of them is upload. One of them is commitment and another one is loan agreement. So across this trade documents, they have to compare some of the entities and make sure that the entities are. Equal in all these three documents if there is any discrepancy that is found then the human in the loop pitch notified. So back, then we didn't have any other resources. So we used. the document OCR of Google Cloud platform and so using that to try to extract the entities in Json format and then we use Python logic in order to Find the Json output and get our specific entities and for tabular data. We were facing some problem. um, because the Google Cloud OCR it sometimes it reads the text row wise while other times it is the text in column wise so In that case it was difficult for us to face the tabular data. So in that case, we utilize the test direct OCR. And that worked pretty well. So yeah that that's that was my contribution in the project. So we were able like I was specifically working on the commitment part and the commitment and document part. So we were able to extract the entities from the PDF similarly the other message learning Engineers. They were like one of them was working on lawn agreement part while the other one was working on the upload document part. So once we got the output from each of these documents then we created a accuracy comparison using logic and anything that went below 95 below 9 like we said it was 90% So anything that went below 90% we like we were highlighting it as a discrepancy. Yeah.",3,"Understood, GPT-4 is indeed a powerful tool. It's great to hear that you've employed techniques like Chain of Thought prompting to maintain coherence and prevent hallucinations, especially with numerical data. Now, shifting focus a bit, I noticed you've also worked on a solution for document comparison using Google Cloud Platform. Can you describe the architecture you used for this solution and how you managed to achieve high accuracy in the comparison process?","Yes, so if you are referring to the greatest and project. So what happened is so grossed out is a real estate company. So being in real estate and they they usually get high number of documents. So they have this three types of specific documents. One of them is upload. One of them is commitment and another one is loan agreement. So across this trade documents, they have to compare some of the entities and make sure that the entities are. Equal in all these three documents if there is any discrepancy that is found then the human in the loop pitch notified. So back, then we didn't have any other resources. So we used. the document OCR of Google Cloud platform and so using that to try to extract the entities in Json format and then we use Python logic in order to Find the Json output and get our specific entities and for tabular data. We were facing some problem. um, because the Google Cloud OCR it sometimes it reads the text row wise while other times it is the text in column wise so In that case it was difficult for us to face the tabular data. So in that case, we utilize the test direct OCR. And that worked pretty well. So yeah that that's that was my contribution in the project. So we were able like I was specifically working on the commitment part and the commitment and document part. So we were able to extract the entities from the PDF similarly the other message learning Engineers. They were like one of them was working on lawn agreement part while the other one was working on the upload document part. So once we got the output from each of these documents then we created a accuracy comparison using logic and anything that went below 95 below 9 like we said it was 90% So anything that went below 90% we like we were highlighting it as a discrepancy. Yeah.",3,True,2
49,49,49,"Interviewer: Absolutely, that's a fantastic application of generators in the context of data augmentation for deep learning. Could you explain the concept of ""yield"" in Python and how it is used in the context of defining a generator function? How does the use of ""yield"" differ from ""return"" in a standard function, and what advantages does it offer in terms of memory efficiency and code execution?  

Interviewee: Mmm, I have implemented many a project but I have never come across as some function called yield, but I'm very much excited to know about it.",1,"Absolutely, that's a fantastic application of generators in the context of data augmentation for deep learning. Could you explain the concept of ""yield"" in Python and how it is used in the context of defining a generator function? How does the use of ""yield"" differ from ""return"" in a standard function, and what advantages does it offer in terms of memory efficiency and code execution?","Mmm, I have implemented many a project but I have never come across as some function called yield, but I'm very much excited to know about it.",1,False,1
50,50,50,"Interviewer: Tailoring the model deployment based on the specific device configuration is a thoughtful approach to ensuring optimal performance and user experience. Could you discuss the mechanisms or techniques you use within your code to dynamically adapt to the device configuration and load the appropriate version of the model for that device?  


Interviewee: So, once a model is loaded in the device and if, for example, for the image generation task, and the user makes a prompt, the prompt before is getting, embedded, the tokens being generated from the prompt. The configuration of that particular device will be obtained from the, SDK which is present there. So the propelling part will be done, and the the device details, like RAM or the, amount of memory present. So that'll be loaded. And depending upon the amount of RAM as well as the configuration of the hardware present in the device, then the model corresponding to that size will be loaded. And, that will then execute, further continue the process of image generation and the output will be generated. So there may be, like, multiple models For example, if the user is having a Snapdragon device, which is going to have, latest agent based Snapdragon device, It may it may have a model which has a better, output or a better, FID score compared to a Snapdragon GPU, which is, like, having the 4th generation or the 6th generation. So, basically, the model gets loaded into the device, based on the configuration. Of the hardware.",2,Tailoring the model deployment based on the specific device configuration is a thoughtful approach to ensuring optimal performance and user experience. Could you discuss the mechanisms or techniques you use within your code to dynamically adapt to the device configuration and load the appropriate version of the model for that device?,"So, once a model is loaded in the device and if, for example, for the image generation task, and the user makes a prompt, the prompt before is getting, embedded, the tokens being generated from the prompt. The configuration of that particular device will be obtained from the, SDK which is present there. So the propelling part will be done, and the the device details, like RAM or the, amount of memory present. So that'll be loaded. And depending upon the amount of RAM as well as the configuration of the hardware present in the device, then the model corresponding to that size will be loaded. And, that will then execute, further continue the process of image generation and the output will be generated. So there may be, like, multiple models For example, if the user is having a Snapdragon device, which is going to have, latest agent based Snapdragon device, It may it may have a model which has a better, output or a better, FID score compared to a Snapdragon GPU, which is, like, having the 4th generation or the 6th generation. So, basically, the model gets loaded into the device, based on the configuration. Of the hardware.",3,True,2
51,51,51,"Interviewer: Certainly. As you envision integrating different AI technologies like LipNet, ISS, and others, what challenges do you anticipate in terms of making these systems compatible, managing the diverse training data requirements, and handling the overall complexity of such a combined AI system?  

Interviewee: Yeah, it's a very big thing. We need to focus and it's a very big challenge for an A and ml engineer, you know, the input size the input size of from one layer to another layer should be should be equal or elsewhere that's tried and padding should be correct as we know the input size. I mean input size that we do from one layer to another layer is a very big Challenge and we need to put a great Focus to that from one neural network to another neural network them integrating and giving a flawless flawless job and work to be done. We need to put a central unit or a central neural network that can manage all these neural networks at a time with the good padding and input sizing so input sizing here. The challenges are made arise like the lip movement of a person is faster. And the AI of is the is architecture cannot track the face, but the lip net is tracking the lips. This is a very crucial thing. So we need to put the timing series the horizontal series of these two neural networks intact and make sure it has a global global global layer or a global pool where all these things can pull up and they can manage their timing series timing series and there are predictions and classifications. You know to come up over this we need to make more. More things and neural networks and neural architectures and we need to move forward even though there are many variables. It's all always about trial and error method and efficient use of sorry efficient use of our resources. By using all these methods and doing trial and error method, even though we have even though we may see many failures. We should not stop there and we should always see the greater picture and a broader level of mindset and make them make them happy.",2,"Certainly. As you envision integrating different AI technologies like LipNet, ISS, and others, what challenges do you anticipate in terms of making these systems compatible, managing the diverse training data requirements, and handling the overall complexity of such a combined AI system?","Yeah, it's a very big thing. We need to focus and it's a very big challenge for an A and ml engineer, you know, the input size the input size of from one layer to another layer should be should be equal or elsewhere that's tried and padding should be correct as we know the input size. I mean input size that we do from one layer to another layer is a very big Challenge and we need to put a great Focus to that from one neural network to another neural network them integrating and giving a flawless flawless job and work to be done. We need to put a central unit or a central neural network that can manage all these neural networks at a time with the good padding and input sizing so input sizing here. The challenges are made arise like the lip movement of a person is faster. And the AI of is the is architecture cannot track the face, but the lip net is tracking the lips. This is a very crucial thing. So we need to put the timing series the horizontal series of these two neural networks intact and make sure it has a global global global layer or a global pool where all these things can pull up and they can manage their timing series timing series and there are predictions and classifications. You know to come up over this we need to make more. More things and neural networks and neural architectures and we need to move forward even though there are many variables. It's all always about trial and error method and efficient use of sorry efficient use of our resources. By using all these methods and doing trial and error method, even though we have even though we may see many failures. We should not stop there and we should always see the greater picture and a broader level of mindset and make them make them happy.",2,True,1
52,52,52,"Interviewer: It's great to hear about your involvement in the core development aspects. Shifting focus back to your research projects, you mentioned using TensorFlow and PyTorch for model development. Can you discuss a specific feature or technique in either framework that you found particularly useful in the context of your research, and how it impacted the performance or efficiency of your models?  


Interviewee: Okay. So the PyTorch framework definitely one of the project I can mention you is the driver drowsiness detection, which was, done by me in my undergraduate studies where the project is about the capturing the facial features of the driver while he's driving. And if at all he goes if if at all his eyes gets closed for, like, some number of seconds, then alarm is given by telling him or, like, alarm alarm was given to make him alert. So in that case, I have used this Pytos and TensorFlow frameworks, which may help me in taking the facial features and, capturing the region of interest And then also, we did, like, like, how much threshold should be given and how continuously the monitor can be done.",2,"It's great to hear about your involvement in the core development aspects. Shifting focus back to your research projects, you mentioned using TensorFlow and PyTorch for model development. Can you discuss a specific feature or technique in either framework that you found particularly useful in the context of your research, and how it impacted the performance or efficiency of your models?","Okay. So the PyTorch framework definitely one of the project I can mention you is the driver drowsiness detection, which was, done by me in my undergraduate studies where the project is about the capturing the facial features of the driver while he's driving. And if at all he goes if if at all his eyes gets closed for, like, some number of seconds, then alarm is given by telling him or, like, alarm alarm was given to make him alert. So in that case, I have used this Pytos and TensorFlow frameworks, which may help me in taking the facial features and, capturing the region of interest And then also, we did, like, like, how much threshold should be given and how continuously the monitor can be done.",2,True,1
53,53,53,"Interviewer: That's a very proactive approach to research and development. Leveraging autoencoders and latent space representations for hyperspectral image classification is a cutting-edge technique. Can you walk me through how you evaluated the performance of your model after incorporating the autoencoder and the latent space projection? What metrics did you use to measure the improvement in accuracy, and how did you ensure that the model was generalizing well to unseen data?  

Interviewee: First I will split the data into training and testing and I will train my model using the training data and I will test that model on testing data, which is unseen data, and then I'll print classification report where I'll get accuracy F1 Pros a fun score Precision recall. So first I will use base model like CNN or simple gcn and I'll print the metrics and then I will use my mini gcn fusion based approaches and Latin space projector Latin Space Incorporated algorithms, and I'll print the Matrix accuracy precision recall a fun school and I'll see which one is whether my model is giving performing. Well on Baseline models or not.",2,"That's a very proactive approach to research and development. Leveraging autoencoders and latent space representations for hyperspectral image classification is a cutting-edge technique. Can you walk me through how you evaluated the performance of your model after incorporating the autoencoder and the latent space projection? What metrics did you use to measure the improvement in accuracy, and how did you ensure that the model was generalizing well to unseen data?","First I will split the data into training and testing and I will train my model using the training data and I will test that model on testing data, which is unseen data, and then I'll print classification report where I'll get accuracy F1 Pros a fun score Precision recall. So first I will use base model like CNN or simple gcn and I'll print the metrics and then I will use my mini gcn fusion based approaches and Latin space projector Latin Space Incorporated algorithms, and I'll print the Matrix accuracy precision recall a fun school and I'll see which one is whether my model is giving performing. Well on Baseline models or not.",3,True,1
54,54,54,"Interviewer: Certainly, let's pivot to Natural Language Processing (NLP). Can you discuss a project where you've worked with NLP, specifically focusing on the techniques or models you've used?  

Interviewee: Oh, yes while working on chatbot model I have I have used natural language processing. Like I have used a openai's API for let's say like I was creating a question answering tool similar to charity but on a particular on a particular topic like for example, like in India view GST, so if a person wants to know a few answers related to GST particular to GST, but he wants to know about the like I can say the recent amendments in the GST in the year 2024 he can ask to that chatbot and it will answer all the questions.",2,"Certainly, let's pivot to Natural Language Processing (NLP). Can you discuss a project where you've worked with NLP, specifically focusing on the techniques or models you've used?","Oh, yes while working on chatbot model I have I have used natural language processing. Like I have used a openai's API for let's say like I was creating a question answering tool similar to charity but on a particular on a particular topic like for example, like in India view GST, so if a person wants to know a few answers related to GST particular to GST, but he wants to know about the like I can say the recent amendments in the GST in the year 2024 he can ask to that chatbot and it will answer all the questions.",3,True,1
55,55,55,"Interviewer: You've provided a clear distinction between the use of processes and threads based on external service dependencies versus internal operations, which makes sense for managing different types of workloads. My question was about how this decision-making process, along with the hierarchy and the switching criteria, has impacted your system's performance and scalability. Could you discuss any notable changes in performance or scalability as a result of these strategic adjustments?  

Interviewee: Um, yes, so I'll tell you what happened when I came into the company. What we noticed was what I noticed. Was that every once in a while we would get some data processes. Okay. So our service is running our ml Services running and sometimes it will die on its own so not the entire service but some of its process will die on its own at that time. What I realized was firstly in their system where the code was running, right? So it was an Ubuntu and as we know in Linux we have everything is a file, right even the processes are mapped to a file. So there was some limit on the system which had to be Unleashed right and like we can you we can do it using you limit we can read about it in detail and we can do some other tricks as well. But I used the you limited trick so firstly I had to unleash the max processes that we can abuse in in operating system right after that. I realized that what happened. What's happening. Was that the processes? why they were dying, right so they were dying because we are creating. So let's say we created process process one and inside that we created another process right recursively. So while we are inside the recursive call, right the what happens is that A lot of the times because the consumer is setting their ideal, right? And another consumer wants to create another process the operating system will kill the idle process automatically. Why because there is a limit to the number of processes. That's why and even after I removed the limit. It's a 60 right? We can't abuse it anymore. So That was one of the things that sometimes we were not able to track why our processes our processes were dying all those we had a system where we will again rein initialize and create a new process ID based on the dead process. Okay, so it's not like we are losing something but yeah that is like that that's that was the thing that we noticed apart from that I think the code was highly scalable like we used to process. I guess five million documents a day, and this was without without GPU. Right, so we had GPU first for a lot of heavy tasks. But but for the tasks that I I have worked on there was no use of GPU there.",2,"You've provided a clear distinction between the use of processes and threads based on external service dependencies versus internal operations, which makes sense for managing different types of workloads. My question was about how this decision-making process, along with the hierarchy and the switching criteria, has impacted your system's performance and scalability. Could you discuss any notable changes in performance or scalability as a result of these strategic adjustments?","Um, yes, so I'll tell you what happened when I came into the company. What we noticed was what I noticed. Was that every once in a while we would get some data processes. Okay. So our service is running our ml Services running and sometimes it will die on its own so not the entire service but some of its process will die on its own at that time. What I realized was firstly in their system where the code was running, right? So it was an Ubuntu and as we know in Linux we have everything is a file, right even the processes are mapped to a file. So there was some limit on the system which had to be Unleashed right and like we can you we can do it using you limit we can read about it in detail and we can do some other tricks as well. But I used the you limited trick so firstly I had to unleash the max processes that we can abuse in in operating system right after that. I realized that what happened. What's happening. Was that the processes? why they were dying, right so they were dying because we are creating. So let's say we created process process one and inside that we created another process right recursively. So while we are inside the recursive call, right the what happens is that A lot of the times because the consumer is setting their ideal, right? And another consumer wants to create another process the operating system will kill the idle process automatically. Why because there is a limit to the number of processes. That's why and even after I removed the limit. It's a 60 right? We can't abuse it anymore. So That was one of the things that sometimes we were not able to track why our processes our processes were dying all those we had a system where we will again rein initialize and create a new process ID based on the dead process. Okay, so it's not like we are losing something but yeah that is like that that's that was the thing that we noticed apart from that I think the code was highly scalable like we used to process. I guess five million documents a day, and this was without without GPU. Right, so we had GPU first for a lot of heavy tasks. But but for the tasks that I I have worked on there was no use of GPU there.",3,True,2
56,56,56,"Interviewer: Implementing parallel processing for real-time inference is a solid strategy. Now, let's pivot to the technical skills listed on your resume. Starting with Python, could you explain how you would use the NumPy and Pandas libraries to optimize a data processing pipeline, particularly in the context of a large dataset?  

Interviewee: Yeah numpy we can use it for numerical values or to create an array of the given data set. I mean to perform several operations in RF formats pandas. We can have it to create a data frame or to access the CSV file or to access any of the files will use pandas.",2,"Implementing parallel processing for real-time inference is a solid strategy. Now, let's pivot to the technical skills listed on your resume. Starting with Python, could you explain how you would use the NumPy and Pandas libraries to optimize a data processing pipeline, particularly in the context of a large dataset?",Yeah numpy we can use it for numerical values or to create an array of the given data set. I mean to perform several operations in RF formats pandas. We can have it to create a data frame or to access the CSV file or to access any of the files will use pandas.,2,True,1
57,57,57,"Interviewer: Your approach to data integration using ETL processes and tools like Azure Data Lake, Azure Data Factory, and Apache Airflow for orchestration is comprehensive. The use of Azure Purview for data governance and lineage is also a key component in maintaining data quality and consistency. Can you tell me about a specific instance where you had to troubleshoot a data quality issue and how you used these tools to identify and resolve the problem?  

Interviewee: Okay. So basically to check data quality, we have used create our own python function so that we can check out data quality. So let's say once we have dated converts from source to sink. And then we need to check whether that data correctly transformed or not. We have created our own system that is basically be this is basically very many initial to check there are missing values or not. There are duplicates or not. There are outliers or not or inconsistent consistent data types and data accuracy. So for that we have created our own tool using python.",2,"Your approach to data integration using ETL processes and tools like Azure Data Lake, Azure Data Factory, and Apache Airflow for orchestration is comprehensive. The use of Azure Purview for data governance and lineage is also a key component in maintaining data quality and consistency. Can you tell me about a specific instance where you had to troubleshoot a data quality issue and how you used these tools to identify and resolve the problem?","Okay. So basically to check data quality, we have used create our own python function so that we can check out data quality. So let's say once we have dated converts from source to sink. And then we need to check whether that data correctly transformed or not. We have created our own system that is basically be this is basically very many initial to check there are missing values or not. There are duplicates or not. There are outliers or not or inconsistent consistent data types and data accuracy. So for that we have created our own tool using python.",3,True,1
58,58,58,"Interviewer: That's quite impressive, achieving such high accuracy across multiple models. Given your experience with various classification models, can you discuss a specific instance where you had to optimize a model's performance, perhaps by tuning hyperparameters or using a different evaluation metric?  

Interviewee: Oh, yes, so. Like in the recently in my projects where? I did a project on lung X-ray disease classification using deep learning and image processing there. I was getting the data. There I was getting a date of around 2 GB. So I tried a different data augmentation techniques to make that data set around 9 GB to increase the volume of the data so that my model can train effectively all so recently. I used some of the transfer learning methods where I removed where I combined two models like an ensemble model two neural networks for image classification and there I tried to change the weights of the CNN model and all so in as we do in transfer learning, I tried to remove the weight. I tried to remove the last layer of the model to join it with the next model this improved the accuracy of my model like the final model and improve the classification precision and accuracy also,",3,"That's quite impressive, achieving such high accuracy across multiple models. Given your experience with various classification models, can you discuss a specific instance where you had to optimize a model's performance, perhaps by tuning hyperparameters or using a different evaluation metric?","Oh, yes, so. Like in the recently in my projects where? I did a project on lung X-ray disease classification using deep learning and image processing there. I was getting the data. There I was getting a date of around 2 GB. So I tried a different data augmentation techniques to make that data set around 9 GB to increase the volume of the data so that my model can train effectively all so recently. I used some of the transfer learning methods where I removed where I combined two models like an ensemble model two neural networks for image classification and there I tried to change the weights of the CNN model and all so in as we do in transfer learning, I tried to remove the weight. I tried to remove the last layer of the model to join it with the next model this improved the accuracy of my model like the final model and improve the classification precision and accuracy also,",3,True,2
59,59,59,"Interviewer: Using NumPy for creating identity matrices and arrays is a fundamental application. How about the more advanced features of NumPy, such as universal functions (ufuncs) or custom ufuncs? Have you utilized these for any specific purposes in your projects? And in Pandas, regarding data manipulation and analysis, which methods have you found effective for optimizing performance, especially when dealing with complex operations on large datasets?  

Interviewee: For now, I don't have much deeper. I mean interpretation on this. Libraries all I used was outer layer of libraries which which is white commonly used so deep down having those libraries is I mean knowing those is good. I will surely look into it adding on to it numpy. I mean, I've used it for structured arrays array sorting or Broadcasting. to say",1,"Using NumPy for creating identity matrices and arrays is a fundamental application. How about the more advanced features of NumPy, such as universal functions (ufuncs) or custom ufuncs? Have you utilized these for any specific purposes in your projects? And in Pandas, regarding data manipulation and analysis, which methods have you found effective for optimizing performance, especially when dealing with complex operations on large datasets?","For now, I don't have much deeper. I mean interpretation on this. Libraries all I used was outer layer of libraries which which is white commonly used so deep down having those libraries is I mean knowing those is good. I will surely look into it adding on to it numpy. I mean, I've used it for structured arrays array sorting or Broadcasting. to say",2,True,1
60,60,60,"Interviewer: Certainly, granular route definition is key to managing different features and requests. In the context of handling large volumes of data or complex computations, what strategies did you employ within Flask to ensure efficient and responsive handling of API requests?  

Interviewee: A at this time I cannot recall.",1,"Certainly, granular route definition is key to managing different features and requests. In the context of handling large volumes of data or complex computations, what strategies did you employ within Flask to ensure efficient and responsive handling of API requests?",A at this time I cannot recall.,1,False,1
61,61,61,"Interviewer: Using statistical methods for distribution comparison is a robust approach to ensure the correctness of the C++ implementation. How did you leverage these statistical techniques to ensure that the synthetic data generated, and the original data manipulated through C++ operations, maintained the same statistical properties as the original dataset? What specific metrics or thresholds did you use to define the similarity between the original and manipulated/synthetic datasets?  


Interviewee: One of the best methods is something called the mahalanabis distribution or sorry. I'm not Helen this distance and effectively the methylene is distance is distributed with a chi-square value of certain Dimension and by doing so you can test whether or not the distances between certain points or the certain averages between the points came from the same distribution and then by calculating the p-value you can test whether or not it actually came from the distribution. So the p-value would tell you whether or not your data source was coherent.",2,"Using statistical methods for distribution comparison is a robust approach to ensure the correctness of the C++ implementation. How did you leverage these statistical techniques to ensure that the synthetic data generated, and the original data manipulated through C++ operations, maintained the same statistical properties as the original dataset? What specific metrics or thresholds did you use to define the similarity between the original and manipulated/synthetic datasets?",One of the best methods is something called the mahalanabis distribution or sorry. I'm not Helen this distance and effectively the methylene is distance is distributed with a chi-square value of certain Dimension and by doing so you can test whether or not the distances between certain points or the certain averages between the points came from the same distribution and then by calculating the p-value you can test whether or not it actually came from the distribution. So the p-value would tell you whether or not your data source was coherent.,3,True,1
62,62,62,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,1,False,1
63,63,63,"Interviewer: Given the diverse nature of your projects, how have you approached the design and architecture of your solutions to ensure they remain well-organized, and what strategies have you employed to handle the specific challenges associated with each project, particularly in terms of data handling, model complexity, and system performance?  

Interviewee: So I faced a lot challenges and I took help from some resources. Also some open resources and the challenges were was like when you come to start some new projects, it was difficult to start and what are the libraries I'm going to use how I will quote and how I will manage with that a state but everything like I just had that in Excel and I imported to Python and that's how like I important some python libraries and I want that. Thank you.",1,"Given the diverse nature of your projects, how have you approached the design and architecture of your solutions to ensure they remain well-organized, and what strategies have you employed to handle the specific challenges associated with each project, particularly in terms of data handling, model complexity, and system performance?","So I faced a lot challenges and I took help from some resources. Also some open resources and the challenges were was like when you come to start some new projects, it was difficult to start and what are the libraries I'm going to use how I will quote and how I will manage with that a state but everything like I just had that in Excel and I imported to Python and that's how like I important some python libraries and I want that. Thank you.",2,True,1
64,64,64,"Interviewer: Given the criticality of the data being handled, how did you approach the testing and validation of this API to ensure that it not only trained models effectively but also maintained the integrity and privacy of the data throughout the process?  

Interviewee: oh, so we actually developed some of the test cases where we checked whether the data is currently and data is only downloaded by automatically using API Keys which are private to us. So that's the main thing where we kept all the API keys and API addresses and links private where the calls will be forwarded only through a specific instance where that instance only has the access using the firewall Protection Service. We blocked all other access to other internet so that A hacker or anyone? Who should not access the data? They cannot access those apis and cannot disrupt us.",2,"Given the criticality of the data being handled, how did you approach the testing and validation of this API to ensure that it not only trained models effectively but also maintained the integrity and privacy of the data throughout the process?","oh, so we actually developed some of the test cases where we checked whether the data is currently and data is only downloaded by automatically using API Keys which are private to us. So that's the main thing where we kept all the API keys and API addresses and links private where the calls will be forwarded only through a specific instance where that instance only has the access using the firewall Protection Service. We blocked all other access to other internet so that A hacker or anyone? Who should not access the data? They cannot access those apis and cannot disrupt us.",3,True,1
65,65,65,"Interviewer: Can you describe a specific instance where you used SQL to retrieve data for a machine learning project, and how you ensured the data you retrieved was in a format that could be directly used for training your models?  

Interviewee: Sure. So there was this instance where I worked used SQL to for data retrieval SQL queries were used to extract the data from the databases to serve for the source of training and testing machine learning models. So there was this data set in the form of SQL queries of the hostel management system and from that hostile management system data set. I retrieved the query I retrieve using SQL queries the data set of various students and their corresponding. Hostels and then I use their data set to train my machine learning models, but because I had the features in the form of columns and then I use a coulombs to train a machine learning model from the data set.",2,"Can you describe a specific instance where you used SQL to retrieve data for a machine learning project, and how you ensured the data you retrieved was in a format that could be directly used for training your models?","Sure. So there was this instance where I worked used SQL to for data retrieval SQL queries were used to extract the data from the databases to serve for the source of training and testing machine learning models. So there was this data set in the form of SQL queries of the hostel management system and from that hostile management system data set. I retrieved the query I retrieve using SQL queries the data set of various students and their corresponding. Hostels and then I use their data set to train my machine learning models, but because I had the features in the form of columns and then I use a coulombs to train a machine learning model from the data set.",3,True,1
66,66,66,"Interviewer: The driver drowsiness detection project sounds like a practical application of AI. Could you elaborate on the specific neural network architecture you used for this project in PyTorch, and how you fine-tuned it to achieve the desired level of accuracy and real-time performance for drowsiness detection?  


Interviewee: Okay. The, recurrent neural network are used because, like, the feedback was helpful in, reducing the error rate in identifying the region of interest. And also, when, the camera will, the camera will capture the continuous pictures and the video stream from the video stream will be, taking the, series of images and check if at all the person eyes or closed or open by detecting the region of interest and various, pretrained model to detect if it is if the eyes are closed or open. If at all they are open and the tissue was, like, for, 4 to 5 seconds, if it all is more than 4 seconds, then the alarm will be giving him to alert that he getting drowsy. So this is how it will be really helpful in the real time. In avoiding accidents.",2,"The driver drowsiness detection project sounds like a practical application of AI. Could you elaborate on the specific neural network architecture you used for this project in PyTorch, and how you fine-tuned it to achieve the desired level of accuracy and real-time performance for drowsiness detection?","Okay. The, recurrent neural network are used because, like, the feedback was helpful in, reducing the error rate in identifying the region of interest. And also, when, the camera will, the camera will capture the continuous pictures and the video stream from the video stream will be, taking the, series of images and check if at all the person eyes or closed or open by detecting the region of interest and various, pretrained model to detect if it is if the eyes are closed or open. If at all they are open and the tissue was, like, for, 4 to 5 seconds, if it all is more than 4 seconds, then the alarm will be giving him to alert that he getting drowsy. So this is how it will be really helpful in the real time. In avoiding accidents.",2,True,1
67,67,67,"Interviewer: Your approach to handling such a complex problem with parallelization and ensemble modeling is quite innovative. Now, let's shift our focus to the technical skills listed on your resume. Starting with C++, can you explain how you would optimize memory usage in a C++ program that involves intensive data processing tasks? 

Interviewee: Okay, C plus plus is an object oriented programming so I can say that I can say that about not like C, we we can use methods we can not use pointers where the memory allocation does not take an active process here and later We can use new and free elements to detect the data here and available memory here and through the objects. We can easily in the access to the memory has object when an object is created and he is created and through through the tip we get access to the classes and other other functionals functions and methods so in this way, we can optimize the memory.",1,"Your approach to handling such a complex problem with parallelization and ensemble modeling is quite innovative. Now, let's shift our focus to the technical skills listed on your resume. Starting with C++, can you explain how you would optimize memory usage in a C++ program that involves intensive data processing tasks?","Okay, C plus plus is an object oriented programming so I can say that I can say that about not like C, we we can use methods we can not use pointers where the memory allocation does not take an active process here and later We can use new and free elements to detect the data here and available memory here and through the objects. We can easily in the access to the memory has object when an object is created and he is created and through through the tip we get access to the classes and other other functionals functions and methods so in this way, we can optimize the memory.",2,True,1
68,68,68,"Interviewer: The process of collecting and curating the prompts from domain experts to ensure clear and specific responses from the model is insightful. How do you envision scaling this approach to handle larger and more diverse datasets, while still maintaining the clarity and accuracy of the model's explanations?  

Interviewee: Okay, my critical domain expertise is with limited data. So we will develop models that is with very limited data consider. We recently developed a model with less than 3000 images. Please don't transfer learning and future learning. So ultimately my core domain is based on limited data processing. So process huge data was huge computation huge time and a lot more developers to develop the model but myself so country for the mortal and a simple model which is based on transfer learning a future learning using less data just 3000 images for the whole classification model and then the textual expression explanation with less than 100 input 100 reims and answers we can develop a model that is based on my skill and my domain expertise that is processing or developing a model with very limited data, even I can develop a model with less than three images. That is based on 0 shot learning.",2,"The process of collecting and curating the prompts from domain experts to ensure clear and specific responses from the model is insightful. How do you envision scaling this approach to handle larger and more diverse datasets, while still maintaining the clarity and accuracy of the model's explanations?","Okay, my critical domain expertise is with limited data. So we will develop models that is with very limited data consider. We recently developed a model with less than 3000 images. Please don't transfer learning and future learning. So ultimately my core domain is based on limited data processing. So process huge data was huge computation huge time and a lot more developers to develop the model but myself so country for the mortal and a simple model which is based on transfer learning a future learning using less data just 3000 images for the whole classification model and then the textual expression explanation with less than 100 input 100 reims and answers we can develop a model that is based on my skill and my domain expertise that is processing or developing a model with very limited data, even I can develop a model with less than three images. That is based on 0 shot learning.",2,True,1
69,69,69,"Interviewer: Optimizing instance scaling based on GPU utilization, model throughput, and overall system resource usage is a thoughtful approach to managing distributed training efficiently. How do you envision the future of AI and ML in terms of its impact on industries and society, and what role do you see yourself playing in this transformation?  

Interviewee: I basically think you know, the future will be where elements will be done by our own computers only. So rather I would feel like a future will be where a person need to be understanding both. You know, ml plus ml operation side like depth development as well as the deployment side into and cycle that helps, you know a person to gain an understanding how these models can give more value to our business and I as a person want to be like a full stack ml engineer which can be, you know, try to design the systems properly as well. As you know develop new new model techniques as well as deploy them for your real life full use cases so that I can add as much as value to our particular business. It might",2,"Optimizing instance scaling based on GPU utilization, model throughput, and overall system resource usage is a thoughtful approach to managing distributed training efficiently. How do you envision the future of AI and ML in terms of its impact on industries and society, and what role do you see yourself playing in this transformation?","I basically think you know, the future will be where elements will be done by our own computers only. So rather I would feel like a future will be where a person need to be understanding both. You know, ml plus ml operation side like depth development as well as the deployment side into and cycle that helps, you know a person to gain an understanding how these models can give more value to our business and I as a person want to be like a full stack ml engineer which can be, you know, try to design the systems properly as well. As you know develop new new model techniques as well as deploy them for your real life full use cases so that I can add as much as value to our particular business. It might",2,True,1
70,70,70,"Interviewer: Prioritizing tool independence and generalizability is a strategic approach to mitigate risks associated with tool dependencies. Could you elaborate on the process of evaluating security and licensing aspects of AI and ML tools? What specific criteria do you look for in terms of security features and licensing agreements to ensure the tools you use are secure and compliant with your data handling policies?  

Interviewee: Scully what we try to do is we try to go through the entire documentation of the particular and go through the source code if it is available and check if there is any data, you know privacy issues what are happening and we also take their licensing thing if it's like properly valid where they don't try to you know, illegally extract our personal companies data is some of the things we check it.",2,Prioritizing tool independence and generalizability is a strategic approach to mitigate risks associated with tool dependencies. Could you elaborate on the process of evaluating security and licensing aspects of AI and ML tools? What specific criteria do you look for in terms of security features and licensing agreements to ensure the tools you use are secure and compliant with your data handling policies?,"Scully what we try to do is we try to go through the entire documentation of the particular and go through the source code if it is available and check if there is any data, you know privacy issues what are happening and we also take their licensing thing if it's like properly valid where they don't try to you know, illegally extract our personal companies data is some of the things we check it.",2,True,1
71,71,71,"Interviewer: That's a practical application of SQL for data retrieval. When you were retrieving this data, how did you handle any data cleaning or preprocessing that was necessary to ensure the data was in a suitable format for training your machine learning models? Did you perform any joins, aggregations, or other SQL operations to prepare the data?  

Interviewee: Sure, so firstly for the data preprocessing tasks, I use the data I use the methods of data cleaning which by which are used in data analysis. So for data cleaning also, I used a few or two methods one was to take the mean of the value for the missing values. And another was to remove the Redundant or the duplicate data in the data cleaning process. and secondly the task of",2,"That's a practical application of SQL for data retrieval. When you were retrieving this data, how did you handle any data cleaning or preprocessing that was necessary to ensure the data was in a suitable format for training your machine learning models? Did you perform any joins, aggregations, or other SQL operations to prepare the data?","Sure, so firstly for the data preprocessing tasks, I use the data I use the methods of data cleaning which by which are used in data analysis. So for data cleaning also, I used a few or two methods one was to take the mean of the value for the missing values. And another was to remove the Redundant or the duplicate data in the data cleaning process. and secondly the task of",2,True,1
72,72,72,"Interviewer: Utilizing existing API services for model deployment and leveraging AWS for auto-scaling is a practical approach. How do you plan to ensure the security and integrity of the system, especially considering the use of multiple API calls and the potential vulnerabilities associated with such integrations?  


Interviewee: so there are a lot of steps that I think opening has already mentioned on their website what can be some of the things that can be integrated in a python-based API method so that people don't overuse it or Like suppose if you're doing sentiment classification using a large language model, you just need few tokens. You don't need it to generate an entire story. So local limiting. The number of tokens to be in the output can one of can be a helpful thing. Also, there are other methods like llama guard which helps in identifying toxicity in the model outputs that can be directly integrated into a service and I think so. Yeah. These can be the two steps that can be taken. as an initial basis",2,"Utilizing existing API services for model deployment and leveraging AWS for auto-scaling is a practical approach. How do you plan to ensure the security and integrity of the system, especially considering the use of multiple API calls and the potential vulnerabilities associated with such integrations?","so there are a lot of steps that I think opening has already mentioned on their website what can be some of the things that can be integrated in a python-based API method so that people don't overuse it or Like suppose if you're doing sentiment classification using a large language model, you just need few tokens. You don't need it to generate an entire story. So local limiting. The number of tokens to be in the output can one of can be a helpful thing. Also, there are other methods like llama guard which helps in identifying toxicity in the model outputs that can be directly integrated into a service and I think so. Yeah. These can be the two steps that can be taken. as an initial basis",3,True,1
73,73,73,"Interviewer: That's a fair assessment of middleware in the context of HTTP requests and authentication. Given your experience with using middleware for authentication, could you describe how you integrated middleware into your authentication process, and the specific role it played in handling authentication-related tasks within your application?  


Interviewee: So in my authentication process, I, I use the middleware in order to send the request I mean, to to get the the correct,",1,"That's a fair assessment of middleware in the context of HTTP requests and authentication. Given your experience with using middleware for authentication, could you describe how you integrated middleware into your authentication process, and the specific role it played in handling authentication-related tasks within your application?","So in my authentication process, I, I use the middleware in order to send the request I mean, to to get the the correct,",2,True,1
74,74,74,"Interviewer: You've made a very important point about the balance between using pre-trained models and the need for specificity in business use cases. Stakeholders often look for popular and high-demand solutions, but your approach to experimentation and proposing solutions based on results is crucial for driving value. When you're fine-tuning models or developing custom solutions, how do you measure the return on investment (ROI) for the additional time and resources required for these efforts? Can you give an example of how you've quantified the benefits of a custom model in a past project to demonstrate its value to stakeholders?  

Interviewee: Mechanic, please repeat the last part. So I can get that.",1,"You've made a very important point about the balance between using pre-trained models and the need for specificity in business use cases. Stakeholders often look for popular and high-demand solutions, but your approach to experimentation and proposing solutions based on results is crucial for driving value. When you're fine-tuning models or developing custom solutions, how do you measure the return on investment (ROI) for the additional time and resources required for these efforts? Can you give an example of how you've quantified the benefits of a custom model in a past project to demonstrate its value to stakeholders?","Mechanic, please repeat the last part. So I can get that.",2,False,1
75,75,75,"Interviewer: Could you describe how you integrated middleware into your authentication process, and the specific role it played in handling authentication-related tasks within your application?  


Interviewee: I mean, say yeah. So, I used the an, to authenticate and authorize users. I use the middleware to send the request to, to the Mongo database and error root with a",1,"Could you describe how you integrated middleware into your authentication process, and the specific role it played in handling authentication-related tasks within your application?","I mean, say yeah. So, I used the an, to authenticate and authorize users. I use the middleware to send the request to, to the Mongo database and error root with a",2,False,1
76,76,76,"Interviewer: That's a comprehensive approach to managing data and model drift. Could you elaborate on the specific statistical tests or techniques you used to detect model drift, and how you integrated these tests into your Continuous Training Pipeline for automated drift detection and response?  

Interviewee: Are basically for categorical features. We are just checking, you know, the more and median but for the continuous things we are taking the KL Divergence test. What is generally does is it just check the distribution of your training data as well as the production data. If you find the change in distribution, we can just say that is a drift happened for the particular feature and in case of embodying as of now, we are just checking the average of their model for the training and the production and the cosine similarities very high. Then we are saying there is a embedding drift happened. So we are actually using it open source tool which is called evidently. So we are hosting it as a service and we are keeping it as scheduler where it tries to you know trigger of every like, you know, one week time interval specific time interval it tries to take the production data for the particular interval and take the training data, which is already present in our DB in does this tally stations and brings out the report so We will just have a human in the loop where it it confirms whether there is a need of you know, it'll just do the statistics. Okay, there is a high chance that is a data reflect that will use the stats out of it. Then we make the call whether the you know training should happen or not. So if he thinks the reigning is necessary, he'll just you know trigger the starting process of by a Lambda of AWS, which has a trigger. So this is how we generated the process. a continuous loop process",3,"That's a comprehensive approach to managing data and model drift. Could you elaborate on the specific statistical tests or techniques you used to detect model drift, and how you integrated these tests into your Continuous Training Pipeline for automated drift detection and response?","Are basically for categorical features. We are just checking, you know, the more and median but for the continuous things we are taking the KL Divergence test. What is generally does is it just check the distribution of your training data as well as the production data. If you find the change in distribution, we can just say that is a drift happened for the particular feature and in case of embodying as of now, we are just checking the average of their model for the training and the production and the cosine similarities very high. Then we are saying there is a embedding drift happened. So we are actually using it open source tool which is called evidently. So we are hosting it as a service and we are keeping it as scheduler where it tries to you know trigger of every like, you know, one week time interval specific time interval it tries to take the production data for the particular interval and take the training data, which is already present in our DB in does this tally stations and brings out the report so We will just have a human in the loop where it it confirms whether there is a need of you know, it'll just do the statistics. Okay, there is a high chance that is a data reflect that will use the stats out of it. Then we make the call whether the you know training should happen or not. So if he thinks the reigning is necessary, he'll just you know trigger the starting process of by a Lambda of AWS, which has a trigger. So this is how we generated the process. a continuous loop process",3,True,2
77,77,77,"Interviewer: Great! I see from your resume that you've worked with the IQ-GPT team at Brain DAO. Can you tell me more about your role in developing the AI-based Chat Web Application and how you integrated Generative AI models into that project?  

Interviewee: Basically, I have worked as an AI engineer there. I have returned last case scrapers that periodically script data from the our partner websites and and we have given a GitHub workflow which will also clean the data when it was Scribd and converts it into an embeddings using GPT add a 002 model and the embedding source code in a remote cloud based Broadband personalized database and in using land chain, we have used the rag which is a trivial augmented augmented generation technique for answering the questions related to the data we have in our database, which is essentially focused on the crypto and regarding the other work. I had done there. I have done API integration which includes defy Lama coin. Where we have taken endpoints of every endpoint in available in the API. We have I have developed the prompts especially for every Endpoint that is available in the API. And also make sure if the elements and routine I used to much optimized the prompts and also provide the necessary data in the form of enumerators and all this work. I had done in typescript and for website. I have worked with property clusters, which is a popular Library. Um and regarding the part. I have worked with the openai models and integration is done using line chain, and the app is developed as a web and deployed on the bar and basically direct using the SGS framework.",3,Great! I see from your resume that you've worked with the IQ-GPT team at Brain DAO. Can you tell me more about your role in developing the AI-based Chat Web Application and how you integrated Generative AI models into that project?,"Basically, I have worked as an AI engineer there. I have returned last case scrapers that periodically script data from the our partner websites and and we have given a GitHub workflow which will also clean the data when it was Scribd and converts it into an embeddings using GPT add a 002 model and the embedding source code in a remote cloud based Broadband personalized database and in using land chain, we have used the rag which is a trivial augmented augmented generation technique for answering the questions related to the data we have in our database, which is essentially focused on the crypto and regarding the other work. I had done there. I have done API integration which includes defy Lama coin. Where we have taken endpoints of every endpoint in available in the API. We have I have developed the prompts especially for every Endpoint that is available in the API. And also make sure if the elements and routine I used to much optimized the prompts and also provide the necessary data in the form of enumerators and all this work. I had done in typescript and for website. I have worked with property clusters, which is a popular Library. Um and regarding the part. I have worked with the openai models and integration is done using line chain, and the app is developed as a web and deployed on the bar and basically direct using the SGS framework.",2,True,2
78,78,78,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,1,False,1
79,79,79,"Interviewer: Implementing sentiment analysis for movie recommendations is a compelling use case. What model or approach did you find most effective for sentiment analysis in the context of movie reviews? And regarding the integration challenges, how did you address them?  

Interviewee: so firstly I made the data collection process and pre-processed the text and then I extracted the features and for model selection. I used logistic regression and support Vector missions. These are classification algorithms. So the my Approach was like my app will suggest you whether this movie is good or bad. So. using my classification models I'll According to the user interest. I'll classify the movie. Either as a good one or a bad one.",2,"Implementing sentiment analysis for movie recommendations is a compelling use case. What model or approach did you find most effective for sentiment analysis in the context of movie reviews? And regarding the integration challenges, how did you address them?",so firstly I made the data collection process and pre-processed the text and then I extracted the features and for model selection. I used logistic regression and support Vector missions. These are classification algorithms. So the my Approach was like my app will suggest you whether this movie is good or bad. So. using my classification models I'll According to the user interest. I'll classify the movie. Either as a good one or a bad one.,3,True,1
80,80,80,"Interviewer: That's a solid approach to ensuring scalability and reliability. Shifting focus to your machine learning expertise, can you discuss how you've implemented statistical models in your projects and perhaps touch on a time when you had to optimize an ML pipeline for better performance? What specific techniques or tools did you use for optimization?  

Interviewee: So in CRM attacks, my major task issues to find the sales for the predictive modeling and to perform such a task and used mostly with the regression-based models. Like I used perform mostly a random code s model my machine learning algorithm. So it makes the in such a way. And I'm able to predict the one and also apply some statistical methods involved in my project so such as a suppose. Let's say the data is missing somewhere in so I can I'm using the daytime computation methods. Like I'm applying a mean methods somewhere in wherever the data is missing. So. Like this statistical models. Can I have applied in my projector so far? That's all in short about John.",1,"That's a solid approach to ensuring scalability and reliability. Shifting focus to your machine learning expertise, can you discuss how you've implemented statistical models in your projects and perhaps touch on a time when you had to optimize an ML pipeline for better performance? What specific techniques or tools did you use for optimization?","So in CRM attacks, my major task issues to find the sales for the predictive modeling and to perform such a task and used mostly with the regression-based models. Like I used perform mostly a random code s model my machine learning algorithm. So it makes the in such a way. And I'm able to predict the one and also apply some statistical methods involved in my project so such as a suppose. Let's say the data is missing somewhere in so I can I'm using the daytime computation methods. Like I'm applying a mean methods somewhere in wherever the data is missing. So. Like this statistical models. Can I have applied in my projector so far? That's all in short about John.",2,True,1
81,81,81,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,1,False,1
82,82,82,"Interviewer: Leveraging matrix multiplication and transposition for model design and data organization makes sense. How did the use of transposition specifically impact the design of your multi-output regression model, and were there any insights or challenges you encountered in this process that were particularly noteworthy?  

Interviewee: Are not really I mean once a new whatever is supposed to be doing, you know, after eliminating a bunch of other multi output models. There wasn't much of notable challenge or like anything significant that I can tell about it.",1,"Leveraging matrix multiplication and transposition for model design and data organization makes sense. How did the use of transposition specifically impact the design of your multi-output regression model, and were there any insights or challenges you encountered in this process that were particularly noteworthy?","Are not really I mean once a new whatever is supposed to be doing, you know, after eliminating a bunch of other multi output models. There wasn't much of notable challenge or like anything significant that I can tell about it.",1,True,1
83,83,83,"Interviewer: It's interesting to hear about the challenges you faced with named entity recognition, especially with homonyms like ""apple"". It's a common issue in NLP. How did you handle this? Did you use any specific techniques or tools to improve the model's understanding of context? 

Interviewee: Also at the time we have written our own scenarios only so we have trained our model by manual giving the manual inputs for the data on.",1,"It's interesting to hear about the challenges you faced with named entity recognition, especially with homonyms like ""apple"". It's a common issue in NLP. How did you handle this? Did you use any specific techniques or tools to improve the model's understanding of context?",Also at the time we have written our own scenarios only so we have trained our model by manual giving the manual inputs for the data on.,2,False,1
84,84,84,"Interviewer: I see how Docker was instrumental in ensuring compatibility across different environments for your project at Owl Hill. Now, could you explain the concept of Mean Average Precision (mAP) and how it is used to evaluate object detection models? 

Interviewee: Oh, okay. Sure. So as far as I can remember mean average Precision takes in consideration the procedure for president for all the images in a data set that we have predicted and then takes the average of that that Precision that will be an average person.",1,"I see how Docker was instrumental in ensuring compatibility across different environments for your project at Owl Hill. Now, could you explain the concept of Mean Average Precision (mAP) and how it is used to evaluate object detection models?","Oh, okay. Sure. So as far as I can remember mean average Precision takes in consideration the procedure for president for all the images in a data set that we have predicted and then takes the average of that that Precision that will be an average person.",2,True,1
85,85,85,"Interviewer: That's a good approach to leveraging TypeScript's static typing. Could you now explain the concept of interfaces in TypeScript and provide an example of how you've used interfaces to define custom types in your projects?  


Interviewee: So inter interfaces in task group allow us to define the structure of the objects. So we have to specify the properties and methods, for that, object. That's the that's how we do it.",1,That's a good approach to leveraging TypeScript's static typing. Could you now explain the concept of interfaces in TypeScript and provide an example of how you've used interfaces to define custom types in your projects?,"So inter interfaces in task group allow us to define the structure of the objects. So we have to specify the properties and methods, for that, object. That's the that's how we do it.",2,False,1
86,86,86,"Interviewer: Your insight into code optimization and scalability is valuable. Shifting focus to your ML and AI projects, could you describe the approach you took to ensure the ML models' scalability and efficiency, particularly in the context of the Copilot project?  

Interviewee: A can you make your partner?",1,"Your insight into code optimization and scalability is valuable. Shifting focus to your ML and AI projects, could you describe the approach you took to ensure the ML models' scalability and efficiency, particularly in the context of the Copilot project?",A can you make your partner?,1,False,1
87,87,87,"Interviewer: Cosine similarity is a solid choice for measuring the similarity of sentences. In addition to cosine similarity, were there any other metrics or techniques you used to validate the performance of your model, such as precision, recall, or F1 score, especially in the context of NLP tasks? And how did you handle any false positives or false negatives that might have arisen in your results?  

Interviewee: Right, so let me think about it and I'll give you answer.",1,"Cosine similarity is a solid choice for measuring the similarity of sentences. In addition to cosine similarity, were there any other metrics or techniques you used to validate the performance of your model, such as precision, recall, or F1 score, especially in the context of NLP tasks? And how did you handle any false positives or false negatives that might have arisen in your results?","Right, so let me think about it and I'll give you answer.",3,False,1
88,88,88,"Interviewer: I appreciate your clear explanation of transfer learning and its application across different domains. Could you now discuss the challenges and potential solutions for handling imbalanced datasets in the context of machine learning? 

Interviewee: So imbalance data is not anything new in machine learning. This is a very everyday problem. So when data is imbalanced it is up to the models. It's up to the person who is modeling the architecture who has to decide what Baseline results are they getting so number one. when we have imbalance data, we need to set up Baseline result about what data we have and we train existing architecture on it and we get a baseline result and then we start working on our imbalance data set number one in that if if it's a classification task, let's take there are two classes and Class A has 100 data points and Class B has two data points or 10 data points, right? So the class imbalance is very high. In this particular scenario a good example would be to do synthetically create new data points for class B. Something like a Smoothie smoothies very much used in machine learning which synthetically over samples minority data minority classes in the data set.",3,I appreciate your clear explanation of transfer learning and its application across different domains. Could you now discuss the challenges and potential solutions for handling imbalanced datasets in the context of machine learning?,"So imbalance data is not anything new in machine learning. This is a very everyday problem. So when data is imbalanced it is up to the models. It's up to the person who is modeling the architecture who has to decide what Baseline results are they getting so number one. when we have imbalance data, we need to set up Baseline result about what data we have and we train existing architecture on it and we get a baseline result and then we start working on our imbalance data set number one in that if if it's a classification task, let's take there are two classes and Class A has 100 data points and Class B has two data points or 10 data points, right? So the class imbalance is very high. In this particular scenario a good example would be to do synthetically create new data points for class B. Something like a Smoothie smoothies very much used in machine learning which synthetically over samples minority data minority classes in the data set.",3,True,2
89,89,89,"Interviewer: Ensuring that the model's resource requirements align with the capabilities of the target smartphone is a crucial step in the optimization process. Could you discuss the specific techniques or tools you use to profile a model's resource usage and performance in order to make these informed decisions about compression and deployment on different smartphone devices?  


Interviewee: Yeah. Basically, OPPO uses, 2 types of, hardware. Processors, basically, and one is from, Snapdragon, and another one is from Mediatek. So both these manufacturers, Snapdragon as well as Mediatek, they provide their own SDK, so where, like, you can just try to integrate your code and see or visualize the performance by profiling the devices. So where you will get to know how much load the model puts on the smartphone, and try to, manipulate or modify your code accordingly so that maximum load is not put on the smartphone, regular processes, and only a certain amount of load is, put on the smartphone. So that, the smartphone performs its expected functions, well, and it, tries to integrate new in newly integrated, applications easily.",2,Ensuring that the model's resource requirements align with the capabilities of the target smartphone is a crucial step in the optimization process. Could you discuss the specific techniques or tools you use to profile a model's resource usage and performance in order to make these informed decisions about compression and deployment on different smartphone devices?,"Yeah. Basically, OPPO uses, 2 types of, hardware. Processors, basically, and one is from, Snapdragon, and another one is from Mediatek. So both these manufacturers, Snapdragon as well as Mediatek, they provide their own SDK, so where, like, you can just try to integrate your code and see or visualize the performance by profiling the devices. So where you will get to know how much load the model puts on the smartphone, and try to, manipulate or modify your code accordingly so that maximum load is not put on the smartphone, regular processes, and only a certain amount of load is, put on the smartphone. So that, the smartphone performs its expected functions, well, and it, tries to integrate new in newly integrated, applications easily.",3,True,1
90,90,90,"Interviewer: That's fascinating, especially the custom data processing pipeline you mentioned. In terms of the machine learning models you applied to these different data types, how did you ensure the models were appropriately trained and validated, given the unique characteristics of each data type?  

Interviewee: Okay, we utilized explainable for verifying the models performance to be more precise. We utilized lime included with sap. So through which we verified by the modulus learning appropriate features or not considered if it is a door using the expander models and the explainer will show which are the features content for that particular classification. Through which the model which is explained or model cubes and visual mask that will help us to understand whether the model. Collected the appropriate features or not. The model is focusing on the region of interest. It's just out of the region of Interest then will tune the model for the or add more data or more background features through with some model can learn. this is pretty much about explainable a but we utilize customer models, which is experimental using seven different models, which use and explanation about the visual mask a textual explanation and also the heat map to Showcase whether the model learned a particular feature or not.",2,"That's fascinating, especially the custom data processing pipeline you mentioned. In terms of the machine learning models you applied to these different data types, how did you ensure the models were appropriately trained and validated, given the unique characteristics of each data type?","Okay, we utilized explainable for verifying the models performance to be more precise. We utilized lime included with sap. So through which we verified by the modulus learning appropriate features or not considered if it is a door using the expander models and the explainer will show which are the features content for that particular classification. Through which the model which is explained or model cubes and visual mask that will help us to understand whether the model. Collected the appropriate features or not. The model is focusing on the region of interest. It's just out of the region of Interest then will tune the model for the or add more data or more background features through with some model can learn. this is pretty much about explainable a but we utilize customer models, which is experimental using seven different models, which use and explanation about the visual mask a textual explanation and also the heat map to Showcase whether the model learned a particular feature or not.",3,True,1
91,91,91,"Interviewer: Sure. What are some trade-offs or compromises you might encounter during the implementation of the real-time sentiment analysis tool? 


Interviewee: Okay. So, like, there's a data collection challenges in the in which API rate limits. And mitigation, like, use multiple API keys accounts to distribute the load and implement efficient rate limiting logic.",2,Sure. What are some trade-offs or compromises you might encounter during the implementation of the real-time sentiment analysis tool?,"Okay. So, like, there's a data collection challenges in the in which API rate limits. And mitigation, like, use multiple API keys accounts to distribute the load and implement efficient rate limiting logic.",2,True,1
92,92,92,"Interviewer: Yes, if you could provide a code snippet or a more detailed example of how you defined the interface for the resume information in your MongoDB project, it would help illustrate your use of interfaces in TypeScript.  


Interviewee: Okay. For example, we, we, want an interface of some object. What is our name and resume? Then we had the brackets, curly brackets upon, and inside that, we give them the entity's name, like, what's the example, you know, for resume, whose whose resume is that? So we need name. Email, their resume. For example, we want the experiences we have so what we did, we took all that information and pull it in these each, entities, like, we put the education and all that. So we named it, and then once we were defining it, for example, I want, one resume, and I want to store everything in it. So I created an another video, for example, Ibrahim. Resume. And then we call that we we created a function in order to to use that interface, and call that interface specific object, object and assign those information. To that, the entity to the elements of that",2,"Yes, if you could provide a code snippet or a more detailed example of how you defined the interface for the resume information in your MongoDB project, it would help illustrate your use of interfaces in TypeScript.","Okay. For example, we, we, want an interface of some object. What is our name and resume? Then we had the brackets, curly brackets upon, and inside that, we give them the entity's name, like, what's the example, you know, for resume, whose whose resume is that? So we need name. Email, their resume. For example, we want the experiences we have so what we did, we took all that information and pull it in these each, entities, like, we put the education and all that. So we named it, and then once we were defining it, for example, I want, one resume, and I want to store everything in it. So I created an another video, for example, Ibrahim. Resume. And then we call that we we created a function in order to to use that interface, and call that interface specific object, object and assign those information. To that, the entity to the elements of that",2,True,1
93,93,93,"Interviewer: Fascinating. Your approach to model selection and optimization is quite thorough. Shifting focus to your internship on CV Parsing and Job Role Matching, could you explain the technical methods you employed for parsing and matching? How did you handle the unstructured nature of CVs and job descriptions?  

Interviewee: So first, I think my model on various job descriptions, you're using a large and elaborate data set with a wide array and range of skills and the corresponding jobs. And after that I use the libraries of regular expression in Python to cars to pass my resume and take out skills. And after that I used a Json file which had all this skills that one that could be extracted from a resume and then I matched the text of the resume with the Json file and after that I was able to extract all this bills and after that I had like a string and array I might say of all the required skills that I needed for the job description that I provided in the job description. And after that I matched them I matched the extract his resume to this job description and that is how I calculated if how much percent it would be suitable and and on top of that to be also I also implemented it further to to tell the person who whose resume is being passed that with job his resume suitable for",2,"Fascinating. Your approach to model selection and optimization is quite thorough. Shifting focus to your internship on CV Parsing and Job Role Matching, could you explain the technical methods you employed for parsing and matching? How did you handle the unstructured nature of CVs and job descriptions?","So first, I think my model on various job descriptions, you're using a large and elaborate data set with a wide array and range of skills and the corresponding jobs. And after that I use the libraries of regular expression in Python to cars to pass my resume and take out skills. And after that I used a Json file which had all this skills that one that could be extracted from a resume and then I matched the text of the resume with the Json file and after that I was able to extract all this bills and after that I had like a string and array I might say of all the required skills that I needed for the job description that I provided in the job description. And after that I matched them I matched the extract his resume to this job description and that is how I calculated if how much percent it would be suitable and and on top of that to be also I also implemented it further to to tell the person who whose resume is being passed that with job his resume suitable for",2,True,2
94,94,94,"Interviewer: You've outlined a comprehensive approach to model deployment, emphasizing the importance of backend frameworks like Flask or Django, as well as considerations for performance, generalization, and scalability. Your mention of cloud platforms and infrastructure configuration also shows a good understanding of the operational side of deploying machine learning models. Can you discuss a specific instance where you deployed a machine learning model using Flask or Django, and how you ensured that the model was able to handle real-time requests efficiently?  

Interviewee: Sure. So there was this project that I was working on in that firstly I performed the data preprocessing tasks and then I performed the feature extraction and finally I trained my model for the classification purposes. And after my model was trained and it was performing good classification on the evaluation metrics. It was time to deploy it using the backend framework. So the backend framework that I used was flask that involved building a sentiment analysis web application. So I integrated that train estimate sentiment analysis model with flask web application Frost provided to it and flexible framework for building restful apis allowing the model to be exposed as a web service in point that can receive in process HTTP request to ensure that the model could handle real-time request efficiently optimized the model in interphase process by preloading model. I preloaded their trained model into the memory when the flux application started to avoid the overhead of the loading the model for each request. I also minimize the overhead. Of requests by using lightweight and avoiding unnecessary pre-processing ships. I also improved the performance by implementing caching mechanisms using libraries like flask caching or redis.",3,"You've outlined a comprehensive approach to model deployment, emphasizing the importance of backend frameworks like Flask or Django, as well as considerations for performance, generalization, and scalability. Your mention of cloud platforms and infrastructure configuration also shows a good understanding of the operational side of deploying machine learning models. Can you discuss a specific instance where you deployed a machine learning model using Flask or Django, and how you ensured that the model was able to handle real-time requests efficiently?",Sure. So there was this project that I was working on in that firstly I performed the data preprocessing tasks and then I performed the feature extraction and finally I trained my model for the classification purposes. And after my model was trained and it was performing good classification on the evaluation metrics. It was time to deploy it using the backend framework. So the backend framework that I used was flask that involved building a sentiment analysis web application. So I integrated that train estimate sentiment analysis model with flask web application Frost provided to it and flexible framework for building restful apis allowing the model to be exposed as a web service in point that can receive in process HTTP request to ensure that the model could handle real-time request efficiently optimized the model in interphase process by preloading model. I preloaded their trained model into the memory when the flux application started to avoid the overhead of the loading the model for each request. I also minimize the overhead. Of requests by using lightweight and avoiding unnecessary pre-processing ships. I also improved the performance by implementing caching mechanisms using libraries like flask caching or redis.,4,True,2
95,95,95,"Interviewer: That's a significant improvement in computational efficiency. Regarding the deployment of these models, could you elaborate on how you containerized the AI solution for efficient deployment, especially in the context of handling large volumes of financial data? What specific challenges did you encounter with data management within the containers?  


Interviewee: Right, so effectively a client would give us a C plus plus pricer and we would basically build out a Docker image in the form of a exposed API via the container then we would scale this up using kubernetes on our clusters depending on what amount of pricing we needed for that day. and by doing so we were able to dynamically increase the amount of computational power that we had for requests for specific training data. So if a model that we were training required more training data, we would scale up prices. One of the things that we're particularly channeling was when we had to obscure financial data or keep it. You know encrypted in some form and we went around. Several iterations of the scheme where we would actually impose fake data first and then every x amount of steps, we would actually send out real data and only we knew whether or not the data that we were training on were real so we knew which ones which data points to skip or not. So we would obfuscate data by creating synthetic data and then only trained on real data. Given the fact that we knew which row entries were, correct.",4,"That's a significant improvement in computational efficiency. Regarding the deployment of these models, could you elaborate on how you containerized the AI solution for efficient deployment, especially in the context of handling large volumes of financial data? What specific challenges did you encounter with data management within the containers?","Right, so effectively a client would give us a C plus plus pricer and we would basically build out a Docker image in the form of a exposed API via the container then we would scale this up using kubernetes on our clusters depending on what amount of pricing we needed for that day. and by doing so we were able to dynamically increase the amount of computational power that we had for requests for specific training data. So if a model that we were training required more training data, we would scale up prices. One of the things that we're particularly channeling was when we had to obscure financial data or keep it. You know encrypted in some form and we went around. Several iterations of the scheme where we would actually impose fake data first and then every x amount of steps, we would actually send out real data and only we knew whether or not the data that we were training on were real so we knew which ones which data points to skip or not. So we would obfuscate data by creating synthetic data and then only trained on real data. Given the fact that we knew which row entries were, correct.",3,True,2
96,96,96,"Interviewer: It's clear that you have a good understanding of the importance of authentication and authorization in web applications, and your use of JWT with proper token management, including expiration and refresh tokens, is a strong security practice. As you continue to learn about security, what are some specific areas or technologies within the field of cybersecurity that you're particularly interested in exploring or implementing in your future projects?  

Interviewee: Um as for now, I cannot give a very brief explanation on this because like security field is something that I recently got very interested in it and I think exploring what all this field has to offer has itself to offer to me but there are different, you know mechanisms that I like what I have learned so far. What I would like to implement is basically, you know, we can have we can bring anonymity where we can have a proxy in the middle we can have some you can have a proxy or relay in the middle between our main server and the body called the client server so that when a client makes the request to the server, it's actually going to that really in the middle. So now the client things that has been connected to the servers actually connected to the relay in the same sense. We can say that the server things it is getting the request from the client and but it comes from the day itself. So it kind of maintains and on and it maintains and anonymity which gives itself. layer of security in itself So we can always do different, you know for different security purposes. We have to make sure like one of the aspects that I recently Learned was we need to have our websites TLS TLS encrypted to make sure we are using all those with the latest cryptographic algorithms. Like there's a lot that I learned and I'm kind of getting very override that right now, but basically TLS make sure that we have I don't know. I forgot the name of the God that it uses it but the mechanism itself is a very beautiful thing that you can act. It's very difficult. It's a protocol which is very difficult to break and you know. Get to know so definitely more on the anonymity aspect so that it's very hard for an attacker itself to reach the organization. I will definitely have a better firewall so that if any person is trying to you know, seems fishy somewhere out there he trying to kind of scan my available ports making requests again. And again, I have a file work on Twitter to blog that IP and you know, it's like that. I'll have a better firewall configured. I'll have anonymity there and I'll make sure that I don't I'm not really know whether to buffer out for attacks and different stats that are available out there. It's something like that.",2,"It's clear that you have a good understanding of the importance of authentication and authorization in web applications, and your use of JWT with proper token management, including expiration and refresh tokens, is a strong security practice. As you continue to learn about security, what are some specific areas or technologies within the field of cybersecurity that you're particularly interested in exploring or implementing in your future projects?","Um as for now, I cannot give a very brief explanation on this because like security field is something that I recently got very interested in it and I think exploring what all this field has to offer has itself to offer to me but there are different, you know mechanisms that I like what I have learned so far. What I would like to implement is basically, you know, we can have we can bring anonymity where we can have a proxy in the middle we can have some you can have a proxy or relay in the middle between our main server and the body called the client server so that when a client makes the request to the server, it's actually going to that really in the middle. So now the client things that has been connected to the servers actually connected to the relay in the same sense. We can say that the server things it is getting the request from the client and but it comes from the day itself. So it kind of maintains and on and it maintains and anonymity which gives itself. layer of security in itself So we can always do different, you know for different security purposes. We have to make sure like one of the aspects that I recently Learned was we need to have our websites TLS TLS encrypted to make sure we are using all those with the latest cryptographic algorithms. Like there's a lot that I learned and I'm kind of getting very override that right now, but basically TLS make sure that we have I don't know. I forgot the name of the God that it uses it but the mechanism itself is a very beautiful thing that you can act. It's very difficult. It's a protocol which is very difficult to break and you know. Get to know so definitely more on the anonymity aspect so that it's very hard for an attacker itself to reach the organization. I will definitely have a better firewall so that if any person is trying to you know, seems fishy somewhere out there he trying to kind of scan my available ports making requests again. And again, I have a file work on Twitter to blog that IP and you know, it's like that. I'll have a better firewall configured. I'll have anonymity there and I'll make sure that I don't I'm not really know whether to buffer out for attacks and different stats that are available out there. It's something like that.",3,True,1
97,97,97,"Interviewer: That's a robust approach to security. Now, let's delve into the technical skills listed on your resume. Starting with TensorFlow and PyTorch, could you explain the specific use cases where you would choose one framework over the other, considering factors such as model complexity, deployment requirements, and ease of use for the rest of the team?  

Interviewee: Oh sure. So as compared to tensorflow pytorch actually is a framework on top of the tensorflow which helps getting and downloading different models. So it has different functionalities with parallelization GPU. Only thing is we need to save the data in Python sensors inside of just a tensor in as compared to tensorflow. Also in the latest things pythons 2.0. It has a robust access where we can actually start the model training validate the training split the data according to our needs and there are lots of different features as compared to tensorflow which helps in training and evaluating different models saving the models pytorch is again internet Lee internally connected to hugging face Library where we can upload our trained models so that other people can access it very easily and it helps increase team coordination as well. So overall I feel that pytorch is much better as compared to 10 sir flow with ease to use of GPU and CPU with ram and also he's as compared to as a programmer for coding expert aspect.",1,"That's a robust approach to security. Now, let's delve into the technical skills listed on your resume. Starting with TensorFlow and PyTorch, could you explain the specific use cases where you would choose one framework over the other, considering factors such as model complexity, deployment requirements, and ease of use for the rest of the team?",Oh sure. So as compared to tensorflow pytorch actually is a framework on top of the tensorflow which helps getting and downloading different models. So it has different functionalities with parallelization GPU. Only thing is we need to save the data in Python sensors inside of just a tensor in as compared to tensorflow. Also in the latest things pythons 2.0. It has a robust access where we can actually start the model training validate the training split the data according to our needs and there are lots of different features as compared to tensorflow which helps in training and evaluating different models saving the models pytorch is again internet Lee internally connected to hugging face Library where we can upload our trained models so that other people can access it very easily and it helps increase team coordination as well. So overall I feel that pytorch is much better as compared to 10 sir flow with ease to use of GPU and CPU with ram and also he's as compared to as a programmer for coding expert aspect.,2,True,1
98,98,98,"Interviewer: It's a strategic move to leverage pre-trained language models like GPT-2 and GPT-3 to enhance the robustness of your NLP solution. When you transitioned to using these larger language models, how did you approach the fine-tuning process, and what specific techniques did you use to ensure that the model's outputs were aligned with the domain-specific requirements of legal language and conditional statements?  

Interviewee: Um, actually the thing is that I was not in the team in the team for the projector now because I had left the team for the another project so I cannot answer this question right now.",1,"It's a strategic move to leverage pre-trained language models like GPT-2 and GPT-3 to enhance the robustness of your NLP solution. When you transitioned to using these larger language models, how did you approach the fine-tuning process, and what specific techniques did you use to ensure that the model's outputs were aligned with the domain-specific requirements of legal language and conditional statements?","Um, actually the thing is that I was not in the team in the team for the projector now because I had left the team for the another project so I cannot answer this question right now.",1,False,1
99,99,99,"Interviewer: That's a strategic approach to maintaining the generator characteristics of your functions within the Active Learning loop. As you work on implementing these steps, what are some of the metrics or benchmarks you plan to use to evaluate the efficiency and effectiveness of your Active Learning model? How do you intend to ensure that the model continues to learn and improve its accuracy while also maintaining high levels of efficiency in terms of both computational resources and human labeling efforts?  

Interviewee: Yes, that's a very good application. It's going to be for my actual learning project. Actually, we are doing a brain tumor detection a brain tumor classification which classifies a brain tumor into four classes. So we implemented this strategy normally by combining informativeness and representatives to leverage and to maintain or to monitor the accuracy my model gains. I implemented a new method called as Dynamic thresholding which has never done before. So I said that I'll keep a threshold and I'll compare the uncertainties course with this threshold every time right? So sometimes what happens means so the number of the samples for a particular group will be very high. So which a human can't label and it is very inefficient and in Some Loops. The number of images will be very low even if the labeling process is good. Then the a model can't gain some accuracy from the newly appended data set. So what I did is first, I will fix a small value of threshold or a in the other constant where the threshold is inversely proportional to the informative and representativeness Heil fixed a very high value of threshold so that my model can choose minimum number of samples. So if the number of samples is not at least 5% of my existing unlabeled data, then I'll change the threshold by decreasing. Some value from it. So as this process goes on until I get sufficient sufficient in the sense. Not too much not too low so sufficient number of samples these Dynamic thresholding will be running on and at a particular point. I'll get a sufficient number of samples so that I can label them and I can append to the existing data and retrain my model to gain us higher accuracies in this method. We followed the same and out of 10,000 to 300 images say it is 100% We gain 96% of Ocarina with just seven percent of data. So this is how I improve my data efficiency using Dynamic thresholding concept.",4,"That's a strategic approach to maintaining the generator characteristics of your functions within the Active Learning loop. As you work on implementing these steps, what are some of the metrics or benchmarks you plan to use to evaluate the efficiency and effectiveness of your Active Learning model? How do you intend to ensure that the model continues to learn and improve its accuracy while also maintaining high levels of efficiency in terms of both computational resources and human labeling efforts?","Yes, that's a very good application. It's going to be for my actual learning project. Actually, we are doing a brain tumor detection a brain tumor classification which classifies a brain tumor into four classes. So we implemented this strategy normally by combining informativeness and representatives to leverage and to maintain or to monitor the accuracy my model gains. I implemented a new method called as Dynamic thresholding which has never done before. So I said that I'll keep a threshold and I'll compare the uncertainties course with this threshold every time right? So sometimes what happens means so the number of the samples for a particular group will be very high. So which a human can't label and it is very inefficient and in Some Loops. The number of images will be very low even if the labeling process is good. Then the a model can't gain some accuracy from the newly appended data set. So what I did is first, I will fix a small value of threshold or a in the other constant where the threshold is inversely proportional to the informative and representativeness Heil fixed a very high value of threshold so that my model can choose minimum number of samples. So if the number of samples is not at least 5% of my existing unlabeled data, then I'll change the threshold by decreasing. Some value from it. So as this process goes on until I get sufficient sufficient in the sense. Not too much not too low so sufficient number of samples these Dynamic thresholding will be running on and at a particular point. I'll get a sufficient number of samples so that I can label them and I can append to the existing data and retrain my model to gain us higher accuracies in this method. We followed the same and out of 10,000 to 300 images say it is 100% We gain 96% of Ocarina with just seven percent of data. So this is how I improve my data efficiency using Dynamic thresholding concept.",3,True,2
