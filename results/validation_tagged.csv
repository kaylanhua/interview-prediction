,Unnamed: 0,Input Text,Label,Question,Response,zero_shot,Valid
0,0,"Interviewer: Achieving state-of-the-art results is impressive. How do you envision further improving the system's capability to handle a wider range of financial questions and document types, considering the limitations of dataset-specific fine-tuning?  


Interviewee: So one of the things that can be done is query expansion that is being used and retrieval augmented generate generation tasks a lot. So what happens is given a query you use another model like a large language model to expand this query identifying topics that can be helpful in answering this question or writing several writing this query in a different way. And using this expanded query to create a vector representation and this helps it to retrieve much better documents and it helps to actually Identify some topics or context that was not even trained on a data set specific fine tuning task and for prompting I think newer models are very very good on numerical reasoning. And the questions that are covered in the finger data set actually are more specific to final question answering or people who are really interested in financial question answering. So if there is a use case or if there is there a few questions that doesn't answer correctly, then we can like enhance the problem, but I think you actually covers a lot of different types of financial questions. Or we can also integrate other data sets that are related to financial question answering and integrate them into the dynamically generated prompts.",4,"Achieving state-of-the-art results is impressive. How do you envision further improving the system's capability to handle a wider range of financial questions and document types, considering the limitations of dataset-specific fine-tuning?","So one of the things that can be done is query expansion that is being used and retrieval augmented generate generation tasks a lot. So what happens is given a query you use another model like a large language model to expand this query identifying topics that can be helpful in answering this question or writing several writing this query in a different way. And using this expanded query to create a vector representation and this helps it to retrieve much better documents and it helps to actually Identify some topics or context that was not even trained on a data set specific fine tuning task and for prompting I think newer models are very very good on numerical reasoning. And the questions that are covered in the finger data set actually are more specific to final question answering or people who are really interested in financial question answering. So if there is a use case or if there is there a few questions that doesn't answer correctly, then we can like enhance the problem, but I think you actually covers a lot of different types of financial questions. Or we can also integrate other data sets that are related to financial question answering and integrate them into the dynamically generated prompts.",4,True
1,1,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,3,False
2,2,"Interviewer: Testing the security of a system, especially one involving wireless communication, can be challenging. Could you discuss any specific testing methodologies or tools you used to verify the security and reliability of your wireless traffic light system during development? Additionally, how did the results of these tests influence the design of your system?  

Interviewee: During the development phase of a radio frequency Communication System. We approach testing the security by conducting Radiologists testing and evaluating. We Simon We simulating various synchronizes to identify to identify or not and weakness in the system. We also provide penetration testing to assess a system resting against an organized access to ensure the systems security against an organized access. We implemented several measurements We Trust wrong we request wrong and protocol to protect the transmitter transmitted data. We also implemented Access Control mechanisms such as security organized and other indication process to ensure the only organized can communicate with the system furthermore. We regularly updated and Pat. This is a system software address any known security while while they're both consider regular security edited and assignments to identify and interesting enemy. Potential security is by talking this measures we aim to create a rope a robust and secure radio frequency communication system that provided the confidential integration a liability that transmitted data. Let me know if you have any further questions.",2,"Testing the security of a system, especially one involving wireless communication, can be challenging. Could you discuss any specific testing methodologies or tools you used to verify the security and reliability of your wireless traffic light system during development? Additionally, how did the results of these tests influence the design of your system?",During the development phase of a radio frequency Communication System. We approach testing the security by conducting Radiologists testing and evaluating. We Simon We simulating various synchronizes to identify to identify or not and weakness in the system. We also provide penetration testing to assess a system resting against an organized access to ensure the systems security against an organized access. We implemented several measurements We Trust wrong we request wrong and protocol to protect the transmitter transmitted data. We also implemented Access Control mechanisms such as security organized and other indication process to ensure the only organized can communicate with the system furthermore. We regularly updated and Pat. This is a system software address any known security while while they're both consider regular security edited and assignments to identify and interesting enemy. Potential security is by talking this measures we aim to create a rope a robust and secure radio frequency communication system that provided the confidential integration a liability that transmitted data. Let me know if you have any further questions.,3,True
3,3,"Interviewer: That's a great use of Docker for ensuring consistency across different development environments and for facilitating collaboration. It's clear that you understand the value of containerization in software development. Now, considering your experience with both front-end and back-end technologies, how do you ensure that the communication between the client-side and server-side is secure, especially when handling sensitive data? Can you discuss the security measures you've implemented in your projects?  

Interviewee: I'm sure the bay the main like this project of differently. It's kind of a very it's a prototype kind of that we worked on which is soon the video prior to me. I had to test it. The main aim was offered was once it gets successful in our College. We kind of have other college. We'll see if other colleges too can make need of use out of it and the very basic level of security that we are doing was it's like the JWT token itself. For example, it was very focused on two main aspects authorized authentication and authorization like authentication is basically a fire if I am if I'm the king I have a castle and I want to different people to come and visit my castle. Authentication is whether this person has been invited to my castle of to my party. Yes or no. That is how I catch these authentication. Okay, once that person has entered my after authentication is an what all he can do in my castle is authorization. So to have that enabled was basically I was using my JWT which itself had 256 bit of encryption and I was for example if person is signing in he has a context statement on the front ending of context state which what we call which returns a JWT token to our to our client site which itself can be like they really comes with a functionality where you can you know, kind of you can you can Average time to live or you can say expiration date, right? It comes with its expiration date. We can set the expression date so we can say that to have a better secure mechanism as for now. It's like I can give for one user login. I can give a maximum of 10 minutes of expression that itself can be, you know, after 10 minutes the P might the person might have to know the authenticate about putting into his password that too can be no bypass if we can like there's a second implementation. It's a furthermore implementation that I can do is refreshing the token and reasoning in the refresh token and I'm sharing it to my client again to make sure the person who's using. My website is the same person who logged in they should not be different. It's like kind of that I'm not not be a very nice idea but definitely have taken on security code this semester and learning various aspects of security. Which out for the like to implement in my projects ahead.",2,"That's a great use of Docker for ensuring consistency across different development environments and for facilitating collaboration. It's clear that you understand the value of containerization in software development. Now, considering your experience with both front-end and back-end technologies, how do you ensure that the communication between the client-side and server-side is secure, especially when handling sensitive data? Can you discuss the security measures you've implemented in your projects?","I'm sure the bay the main like this project of differently. It's kind of a very it's a prototype kind of that we worked on which is soon the video prior to me. I had to test it. The main aim was offered was once it gets successful in our College. We kind of have other college. We'll see if other colleges too can make need of use out of it and the very basic level of security that we are doing was it's like the JWT token itself. For example, it was very focused on two main aspects authorized authentication and authorization like authentication is basically a fire if I am if I'm the king I have a castle and I want to different people to come and visit my castle. Authentication is whether this person has been invited to my castle of to my party. Yes or no. That is how I catch these authentication. Okay, once that person has entered my after authentication is an what all he can do in my castle is authorization. So to have that enabled was basically I was using my JWT which itself had 256 bit of encryption and I was for example if person is signing in he has a context statement on the front ending of context state which what we call which returns a JWT token to our to our client site which itself can be like they really comes with a functionality where you can you know, kind of you can you can Average time to live or you can say expiration date, right? It comes with its expiration date. We can set the expression date so we can say that to have a better secure mechanism as for now. It's like I can give for one user login. I can give a maximum of 10 minutes of expression that itself can be, you know, after 10 minutes the P might the person might have to know the authenticate about putting into his password that too can be no bypass if we can like there's a second implementation. It's a furthermore implementation that I can do is refreshing the token and reasoning in the refresh token and I'm sharing it to my client again to make sure the person who's using. My website is the same person who logged in they should not be different. It's like kind of that I'm not not be a very nice idea but definitely have taken on security code this semester and learning various aspects of security. Which out for the like to implement in my projects ahead.",2,True
4,4,"Interviewer: Given the complexity of parameter sweeps and the need to identify the optimal configuration, how did you manage the computational resources required for such extensive testing? Additionally, can you discuss any specific tools or techniques you used to automate or streamline the process of running and analyzing these parameter sweeps, especially considering the large-scale nature of the EPYC server environment? 


Interviewee: So the resources were managed in such a way that everyone working at AMD in our team was assigned one server to us. So I was assigned a server of my own. So I did not need to share this, system or resources with anyone else. Also, to ensure that the resources are being used optimally, I used to develop code and scripts in the daytime. And when I used to log off during the night, I used to run, my workloads or my parameter sweeps so that the next day when I log in back, I have the data ready with me, so which I can analyze further. And develop my scripts accordingly. Also, to make sure since there are a lot of parameter suites to be done, what I did was I wrote wrapper scripts, which iterate through, every data point in the sweep. And, it generates data accordingly. So with the help of a single run script, or a single wrapper script, I was able to generate all the data which was required for me.",2,"Given the complexity of parameter sweeps and the need to identify the optimal configuration, how did you manage the computational resources required for such extensive testing? Additionally, can you discuss any specific tools or techniques you used to automate or streamline the process of running and analyzing these parameter sweeps, especially considering the large-scale nature of the EPYC server environment?","So the resources were managed in such a way that everyone working at AMD in our team was assigned one server to us. So I was assigned a server of my own. So I did not need to share this, system or resources with anyone else. Also, to ensure that the resources are being used optimally, I used to develop code and scripts in the daytime. And when I used to log off during the night, I used to run, my workloads or my parameter sweeps so that the next day when I log in back, I have the data ready with me, so which I can analyze further. And develop my scripts accordingly. Also, to make sure since there are a lot of parameter suites to be done, what I did was I wrote wrapper scripts, which iterate through, every data point in the sweep. And, it generates data accordingly. So with the help of a single run script, or a single wrapper script, I was able to generate all the data which was required for me.",3,True
5,5,"Interviewer: That's impressive. Dealing with computational resources and data labeling can indeed be challenging. Regarding the DeepStream application you used for model training and deployment, what specific features or functionalities did you find most useful in the context of your project, and were there any limitations you encountered with DeepStream that you had to work around?  

Interviewee: Using deep stream and you will be very enhanced our productivity of bounding box during a model rendering. So and the challenge is we face is a frame rate in the Deep stream. The frame rate is on is compared to other few other tools and Technologies we have right now. The frame rate is very low. So as of our budget are limitations we should use data stream. So we use that in future. We are thinking to upgrade our tools and technology so with we can increase the frame rate of the predictions.",2,"That's impressive. Dealing with computational resources and data labeling can indeed be challenging. Regarding the DeepStream application you used for model training and deployment, what specific features or functionalities did you find most useful in the context of your project, and were there any limitations you encountered with DeepStream that you had to work around?",Using deep stream and you will be very enhanced our productivity of bounding box during a model rendering. So and the challenge is we face is a frame rate in the Deep stream. The frame rate is on is compared to other few other tools and Technologies we have right now. The frame rate is very low. So as of our budget are limitations we should use data stream. So we use that in future. We are thinking to upgrade our tools and technology so with we can increase the frame rate of the predictions.,2,True
6,6,"Interviewer: Using synthetic data for calibration is a smart approach to initially perfect the system. How do you plan to transition from synthetic data to real-world data for calibration, and what are the expected challenges in this transition?  

Interviewee: Yeah, so first and foremost challenge that we are going to face is like knowing the location of the cameras with respect to the real world a center coordinate. So in synthetic data, like the Unreal Engine we all we can just click the camera and we know what is 3D location is but actually but in actual like real words scenario, we cannot measure accurately the distance of the campus from the center of something into 31. So this is the first challenge that we are going to face and we are tend to solve this by using the PNP problems like like we will have certain 20 to 30. 3D points in the ground and also with corresponding points in the image plane. So with the help of these points you will be able to accurately Define the 3D coordinates of the camera with respect to the real world using PNC. So we are assuming we will be able to handle this. But let's see what happens.",2,"Using synthetic data for calibration is a smart approach to initially perfect the system. How do you plan to transition from synthetic data to real-world data for calibration, and what are the expected challenges in this transition?","Yeah, so first and foremost challenge that we are going to face is like knowing the location of the cameras with respect to the real world a center coordinate. So in synthetic data, like the Unreal Engine we all we can just click the camera and we know what is 3D location is but actually but in actual like real words scenario, we cannot measure accurately the distance of the campus from the center of something into 31. So this is the first challenge that we are going to face and we are tend to solve this by using the PNP problems like like we will have certain 20 to 30. 3D points in the ground and also with corresponding points in the image plane. So with the help of these points you will be able to accurately Define the 3D coordinates of the camera with respect to the real world using PNC. So we are assuming we will be able to handle this. But let's see what happens.",3,True
7,7,"Interviewer: Given the challenges with hardware setup and the lack of community support, how did you approach debugging and resolving issues on the QCS 610 board? Additionally, what specific considerations did you have to keep in mind while porting the software to ensure it ran effectively on the embedded platform? 


Interviewee: So the challenge so the way we debug these issues was to look at the logs generated while we are installing the the software and if there are any warnings generated. We also consulted people who have worked on similar hardware before, to get their insights on what might be the issue and how do we resolve them. Coming to, coming to making sure that the software is ported correctly, we again looked at the warnings generated, and we defined very, very basic unit test cases. To make sure that the porting is complete.",2,"Given the challenges with hardware setup and the lack of community support, how did you approach debugging and resolving issues on the QCS 610 board? Additionally, what specific considerations did you have to keep in mind while porting the software to ensure it ran effectively on the embedded platform?","So the challenge so the way we debug these issues was to look at the logs generated while we are installing the the software and if there are any warnings generated. We also consulted people who have worked on similar hardware before, to get their insights on what might be the issue and how do we resolve them. Coming to, coming to making sure that the software is ported correctly, we again looked at the warnings generated, and we defined very, very basic unit test cases. To make sure that the porting is complete.",3,True
8,8,"Interviewer: It seems there might have been a connection issue with your response. Could you please elaborate on the challenges you faced in implementing supervised debiasing in your LLM model, and how you addressed them?  

Interviewee: Basically, the main challenge which I had faced is during the memorized memory efficient training process. So as we the model is a very large one we try to you know. quantize this",1,"It seems there might have been a connection issue with your response. Could you please elaborate on the challenges you faced in implementing supervised debiasing in your LLM model, and how you addressed them?","Basically, the main challenge which I had faced is during the memorized memory efficient training process. So as we the model is a very large one we try to you know. quantize this",2,True
9,9,"Interviewer: Data subsampling is a practical approach to managing large datasets when computational resources are limited. It's good to hear that you focus on maintaining data diversity to preserve the model's capabilities. When you're subsampling, how do you ensure that the reduced dataset still captures the necessary diversity and complexity of the original data? Could you also explain how you validate the model's performance on the full dataset after training on the subsampled data to ensure it generalizes well?  

Interviewee: here when we subsample the data, it's something it could be sometimes a human process where we could actually chose to pick and pluck it out of that huge Corpus of data and choose what we have to do with the training or sometimes we can generate generate some scripts where we get to chose. Or scrape the data from the internet from various sources which in a limited manner so that we could actually pass those data itself having the constraints of the data variety. and then later we while performing validation and all we could we could do that in the Unseen data or so that we could Actually get to know how the language model is actually in this case is performing on the answers. I mean, it's performing its facts. I mean getting it producing its facts or information on the data could have never seen.",2,"Data subsampling is a practical approach to managing large datasets when computational resources are limited. It's good to hear that you focus on maintaining data diversity to preserve the model's capabilities. When you're subsampling, how do you ensure that the reduced dataset still captures the necessary diversity and complexity of the original data? Could you also explain how you validate the model's performance on the full dataset after training on the subsampled data to ensure it generalizes well?","here when we subsample the data, it's something it could be sometimes a human process where we could actually chose to pick and pluck it out of that huge Corpus of data and choose what we have to do with the training or sometimes we can generate generate some scripts where we get to chose. Or scrape the data from the internet from various sources which in a limited manner so that we could actually pass those data itself having the constraints of the data variety. and then later we while performing validation and all we could we could do that in the Unseen data or so that we could Actually get to know how the language model is actually in this case is performing on the answers. I mean, it's performing its facts. I mean getting it producing its facts or information on the data could have never seen.",2,True
10,10,"Interviewer: You've provided a clear distinction between supervised and unsupervised learning. Could you now describe a scenario where you would choose to use ensemble learning, and explain how it might improve the performance of a machine learning model? 

Interviewee: Can you please repeat the question?",1,"You've provided a clear distinction between supervised and unsupervised learning. Could you now describe a scenario where you would choose to use ensemble learning, and explain how it might improve the performance of a machine learning model?",Can you please repeat the question?,3,False
11,11,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,3,False
12,12,"Interviewer: It's clear that you've leveraged the AI capabilities within Google Cloud, particularly the AI Platform and its integration with LLMs, to streamline the process of entity extraction from documents. This has significantly reduced the time and effort required compared to traditional methods. Given your experience with both Python logic and LLMs, how do you approach the trade-off between using pre-trained models like GPT-3 and fine-tuning them for specific tasks, especially in the context of document processing?  

Interviewee: how you",1,"It's clear that you've leveraged the AI capabilities within Google Cloud, particularly the AI Platform and its integration with LLMs, to streamline the process of entity extraction from documents. This has significantly reduced the time and effort required compared to traditional methods. Given your experience with both Python logic and LLMs, how do you approach the trade-off between using pre-trained models like GPT-3 and fine-tuning them for specific tasks, especially in the context of document processing?",how you,3,False
13,13,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,3,False
14,14,"Interviewer: That's a fascinating approach to tackling the challenge of idiomatic expressions in NLP. The use of adapters to differentiate between literal and idiomatic usage, and the cross-linguistic evaluation, provide valuable insights into the language-specific nature of idioms. Given your experience with this research, how would you approach implementing a feature in a production-level NLP system that needs to handle idiomatic expressions accurately? Can you also touch upon how you might leverage large language models to improve the system's understanding of idioms? 

Interviewee: Um, can I go back? I actually explained the project halfway. I was in the middle of explaining the second experiment of it.",1,"That's a fascinating approach to tackling the challenge of idiomatic expressions in NLP. The use of adapters to differentiate between literal and idiomatic usage, and the cross-linguistic evaluation, provide valuable insights into the language-specific nature of idioms. Given your experience with this research, how would you approach implementing a feature in a production-level NLP system that needs to handle idiomatic expressions accurately? Can you also touch upon how you might leverage large language models to improve the system's understanding of idioms?","Um, can I go back? I actually explained the project halfway. I was in the middle of explaining the second experiment of it.",3,False
15,15,"Interviewer: That's a solid architecture using Streamlit and FastAPI for the web interface and model serving. When you pass the user input through the model, how did you ensure the model's predictions were efficient and timely? Did you implement any specific strategies for optimizing the model's inference time, such as using a pre-trained model or leveraging hardware acceleration?  

Interviewee: Yes, so there was a specific model distilled bird. It is quite lightweight and is able to run on the The end it is quite fast. So it generally takes around 8 to 10 seconds or even less than that depending upon the size of the reviews size of the queries that user asked so I mostly use that model for this prediction.",2,"That's a solid architecture using Streamlit and FastAPI for the web interface and model serving. When you pass the user input through the model, how did you ensure the model's predictions were efficient and timely? Did you implement any specific strategies for optimizing the model's inference time, such as using a pre-trained model or leveraging hardware acceleration?","Yes, so there was a specific model distilled bird. It is quite lightweight and is able to run on the The end it is quite fast. So it generally takes around 8 to 10 seconds or even less than that depending upon the size of the reviews size of the queries that user asked so I mostly use that model for this prediction.",3,True
16,16,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.
Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,1,False
17,17,"Interviewer: Understood, your experience with Google Cloud is certainly valuable. Could you elaborate on the specific Google Cloud services you found most beneficial for machine learning projects, and perhaps how you've utilized them in conjunction with large language models?  

Interviewee: Oh, seriously, I can do that. Yeah, that's again a very good question. So. In Google Cloud, we have an AI platform called vertex are platform. That's an integrated AI platform which includes all the machine learning tools. That one would need to, you know, like build models for different projects. So for llm, so let me give highlighted this example. So if you go through my resume, I have worked on two different projects one of them being raced on and another one of them being MB bank. So during Graystone with back then the llm Watson that evolved. So we like we had to spend months working on this project like trying to build python logic to you know to extract the entities accurately. But the MB bank project, which was also some best on similar use case. Like then we have to first use the Google Docs here to extract the entities from the PDF from specially the invoices. And then run the llms on them to extract our required entities. So if you see the use case like all the like both are almost similar use case, but here. our like we we didn't have to spend months of building the python logic in order to tackle different scenarios because our llm was able to handle the OCR output and we just passing a specific instructions to the llm in our prompt like hey, I need this this this entity's in my output in Json format. So the llm was able to find those specific outputs and it was able to provide us with the So, yes, llm has helped quite a lot to in like hell increasing the efficiency of our national Landing. Oh, yeah missile Landing projects.",3,"Understood, your experience with Google Cloud is certainly valuable. Could you elaborate on the specific Google Cloud services you found most beneficial for machine learning projects, and perhaps how you've utilized them in conjunction with large language models?","Oh, seriously, I can do that. Yeah, that's again a very good question. So. In Google Cloud, we have an AI platform called vertex are platform. That's an integrated AI platform which includes all the machine learning tools. That one would need to, you know, like build models for different projects. So for llm, so let me give highlighted this example. So if you go through my resume, I have worked on two different projects one of them being raced on and another one of them being MB bank. So during Graystone with back then the llm Watson that evolved. So we like we had to spend months working on this project like trying to build python logic to you know to extract the entities accurately. But the MB bank project, which was also some best on similar use case. Like then we have to first use the Google Docs here to extract the entities from the PDF from specially the invoices. And then run the llms on them to extract our required entities. So if you see the use case like all the like both are almost similar use case, but here. our like we we didn't have to spend months of building the python logic in order to tackle different scenarios because our llm was able to handle the OCR output and we just passing a specific instructions to the llm in our prompt like hey, I need this this this entity's in my output in Json format. So the llm was able to find those specific outputs and it was able to provide us with the So, yes, llm has helped quite a lot to in like hell increasing the efficiency of our national Landing. Oh, yeah missile Landing projects.",3,True
18,18,"Interviewer: Given your experience with deploying models on mobile devices, how did you optimize your models for TensorFlow Lite, specifically in terms of architecture or layer pruning to reduce model size while maintaining accuracy?  

Interviewee: Yeah, actually our first we had we researched about it. We wanted a model which had this is having very good accuracy and also with very lightweight so we use efficient before model for that. So that model is already is very light model and the Motorcycle was 14 MB. So that was fine for the Android. So I used that and so and also we use the pretend models of fine tune. We use our two layers and then we added two layers into the office network based more Affinity user as a base model and in that we train the final five layers so we can have a very light model because we were not training all the layers of the model of the pretend model of the office net. So we use only last four layers and we added two layers tool then Slayer in that data and we find the model so we got a very lightweight model for that",2,"Given your experience with deploying models on mobile devices, how did you optimize your models for TensorFlow Lite, specifically in terms of architecture or layer pruning to reduce model size while maintaining accuracy?","Yeah, actually our first we had we researched about it. We wanted a model which had this is having very good accuracy and also with very lightweight so we use efficient before model for that. So that model is already is very light model and the Motorcycle was 14 MB. So that was fine for the Android. So I used that and so and also we use the pretend models of fine tune. We use our two layers and then we added two layers into the office network based more Affinity user as a base model and in that we train the final five layers so we can have a very light model because we were not training all the layers of the model of the pretend model of the office net. So we use only last four layers and we added two layers tool then Slayer in that data and we find the model so we got a very lightweight model for that",3,True
19,19,"Interviewer: Logging input and output data, along with token consumption, is essential for performance monitoring. It's good to hear that you've used Streamlit's built-in authentication features. For the username and password implementation, did you use any specific encryption or hashing techniques to store and compare the passwords, and how did you manage the security of the authentication process to prevent common vulnerabilities like brute force attacks or password sniffing?  

Interviewee: Okay, it was a POC demo. So we did not have to for Implement that much level of security on the authentication. But yeah, there was basic encryption in the streamlit authenticator and it was doing the work on the back back side.",2,"Logging input and output data, along with token consumption, is essential for performance monitoring. It's good to hear that you've used Streamlit's built-in authentication features. For the username and password implementation, did you use any specific encryption or hashing techniques to store and compare the passwords, and how did you manage the security of the authentication process to prevent common vulnerabilities like brute force attacks or password sniffing?","Okay, it was a POC demo. So we did not have to for Implement that much level of security on the authentication. But yeah, there was basic encryption in the streamlit authenticator and it was doing the work on the back back side.",2,True
20,20,"Interviewer: Balancing computational requirements with model performance and interpretability is indeed a key consideration. Given your experience with both traditional machine learning algorithms and deep learning models, how do you approach the decision of when to use a traditional ML algorithm versus a deep learning model for a given problem, especially in the context of limited data availability?  


Interviewee: Yeah, so sometimes there are I have worked with normal basic foundational machine learning algorithms and also deep learning algorithms. So in case it depends on the type of data set we are working on in the end because if it depends on the type of data set in the sense, if it's linear, I would definitely go with linear regression and I would test out the accuracy on how I get what accuracy I get and then I would I would figure out my way to support Vector machine decision tree and random forest regressor. And then if it is a more if it is a problem statement where I need more computational power and that's the only place that I would go for deep learning because I don't want to increase the computational power and lost the interpretability for a simple. Problem if it's a more complex problem, I would go with a deep learning because even if we can do it with a foundational machine learning algorithms 2, but probably will of course we will get more accuracy in the Deep learning algorithms. It depends on the data set and the complexity of the problem and if we have limited number of data and the problem is complex. I will go with a deep learning if it's less number of data and the problem is okay. I can do the feature engineering and data augmentation and I can populate the data set and I can still work with the foundational algorithms to reduce the complexity and computational Power.",3,"Balancing computational requirements with model performance and interpretability is indeed a key consideration. Given your experience with both traditional machine learning algorithms and deep learning models, how do you approach the decision of when to use a traditional ML algorithm versus a deep learning model for a given problem, especially in the context of limited data availability?","Yeah, so sometimes there are I have worked with normal basic foundational machine learning algorithms and also deep learning algorithms. So in case it depends on the type of data set we are working on in the end because if it depends on the type of data set in the sense, if it's linear, I would definitely go with linear regression and I would test out the accuracy on how I get what accuracy I get and then I would I would figure out my way to support Vector machine decision tree and random forest regressor. And then if it is a more if it is a problem statement where I need more computational power and that's the only place that I would go for deep learning because I don't want to increase the computational power and lost the interpretability for a simple. Problem if it's a more complex problem, I would go with a deep learning because even if we can do it with a foundational machine learning algorithms 2, but probably will of course we will get more accuracy in the Deep learning algorithms. It depends on the data set and the complexity of the problem and if we have limited number of data and the problem is complex. I will go with a deep learning if it's less number of data and the problem is okay. I can do the feature engineering and data augmentation and I can populate the data set and I can still work with the foundational algorithms to reduce the complexity and computational Power.",3,True
21,21,"Interviewer: It's clear that you have a strong understanding of reinforcement learning and have delved into improving existing algorithms. Could you elaborate on the specific challenges you faced with the continuous action space and how the quantile truncated distribution approach addressed these challenges? Additionally, how did you manage the increased computational resources required for this method?  

Interviewee: How could you please repeat the first part of the question?",1,"It's clear that you have a strong understanding of reinforcement learning and have delved into improving existing algorithms. Could you elaborate on the specific challenges you faced with the continuous action space and how the quantile truncated distribution approach addressed these challenges? Additionally, how did you manage the increased computational resources required for this method?",How could you please repeat the first part of the question?,4,False
22,22,"Interviewer: Great. Could you provide some specific examples of enhancements or expansions you would consider for this tool? 


Interviewee: Sure. Here's some specific enhancement or expansions, like, real time data retrieval, enhanced personalized personalization, multimodal input output, advanced analytical and insights. Integration and external tools, improved context awareness, natural language understanding improvements, interactive tutorials and learning modules, and customizable templates and response collaborative features, advanced programming support, accessibility enhancement. Here's some imply implementing these enhancements would significantly broaden the tool's utility and make it more valuable across a variety of use cases.",2,Great. Could you provide some specific examples of enhancements or expansions you would consider for this tool?,"Sure. Here's some specific enhancement or expansions, like, real time data retrieval, enhanced personalized personalization, multimodal input output, advanced analytical and insights. Integration and external tools, improved context awareness, natural language understanding improvements, interactive tutorials and learning modules, and customizable templates and response collaborative features, advanced programming support, accessibility enhancement. Here's some imply implementing these enhancements would significantly broaden the tool's utility and make it more valuable across a variety of use cases.",3,True
23,23,"Interviewer: Thank you for sharing that. Can you provide more details on how you evaluated the performance of these models in your fraud detection tasks? What metrics did you use? 


Interviewee: Evaluation of models in fraud detection task and the mattresses that I have used are, like, there are some",1,Thank you for sharing that. Can you provide more details on how you evaluated the performance of these models in your fraud detection tasks? What metrics did you use?,"Evaluation of models in fraud detection task and the mattresses that I have used are, like, there are some",3,True
24,24,"Interviewer: Exploring AI-driven solutions for generating interactive content like images and videos to enhance student engagement is a great idea. How do you think AI could be used to tailor educational content to individual student's learning styles and pace, thereby further personalizing the learning experience?  

Interviewee: okay, so for this one and suppose if you Kind of give the previous date of the students as in like their performance. And since all of this is data points, you could technically create a machine learning model that kind of trajectory of these students and according to that. I'm pretty sure you can optimize AI models to teach at a certain Pace that suitable for a better tragically for these students.",2,"Exploring AI-driven solutions for generating interactive content like images and videos to enhance student engagement is a great idea. How do you think AI could be used to tailor educational content to individual student's learning styles and pace, thereby further personalizing the learning experience?","okay, so for this one and suppose if you Kind of give the previous date of the students as in like their performance. And since all of this is data points, you could technically create a machine learning model that kind of trajectory of these students and according to that. I'm pretty sure you can optimize AI models to teach at a certain Pace that suitable for a better tragically for these students.",3,True
25,25,"Interviewer: You've provided a clear distinction between the use cases for SQL and NoSQL databases. Now, considering your experience with OpenCV, could you describe a computer vision project you've worked on and explain how you handled any challenges related to image preprocessing or feature extraction? 

Interviewee: okay, so once we I am currently With the opencv pre-processing techniques. I applied those techniques in my lung X-ray disease classification problem also, like converting and grayscale image to RGB and RGB to grayscale image or so trying to adjust the pixels of an image and applying a filter in which is there in opencv. I've learned quite a few techniques in opencv or I'm learning quite a good I am currently at a research center imarat drdo as a AI ml machine learning intern there. So there I am learning opencv and using that opencv Library. I am learning object detection and object tracking and to Port those algorithms on the robot. So definitely in a in a matter of 15 to 20 days, I'll be learning open CV completely and Know like in the opencv. I have applied like data preprocessing techniques, like data augmentation or some converting a gray scale image to RGB or setting the pixels or getting some important data from from a particular image or reading and image plotting of images. So these basic functionality I know and the advanced functionality. I will be learning in a matter of 15 days at my organization drda where I'm doing a project on object tracking and object detection.",2,"You've provided a clear distinction between the use cases for SQL and NoSQL databases. Now, considering your experience with OpenCV, could you describe a computer vision project you've worked on and explain how you handled any challenges related to image preprocessing or feature extraction?","okay, so once we I am currently With the opencv pre-processing techniques. I applied those techniques in my lung X-ray disease classification problem also, like converting and grayscale image to RGB and RGB to grayscale image or so trying to adjust the pixels of an image and applying a filter in which is there in opencv. I've learned quite a few techniques in opencv or I'm learning quite a good I am currently at a research center imarat drdo as a AI ml machine learning intern there. So there I am learning opencv and using that opencv Library. I am learning object detection and object tracking and to Port those algorithms on the robot. So definitely in a in a matter of 15 to 20 days, I'll be learning open CV completely and Know like in the opencv. I have applied like data preprocessing techniques, like data augmentation or some converting a gray scale image to RGB or setting the pixels or getting some important data from from a particular image or reading and image plotting of images. So these basic functionality I know and the advanced functionality. I will be learning in a matter of 15 days at my organization drda where I'm doing a project on object tracking and object detection.",3,True
26,26,"Interviewer: Good use of HTTPS for data encryption. How about user authentication and authorization within the API? Did you implement any specific methods for that?  

Interviewee: Now I didn't Implement any user authentication. You'll just have a link if you click that link, you'll get to that page and you can upload an image and it will give you its prediction.",1,Good use of HTTPS for data encryption. How about user authentication and authorization within the API? Did you implement any specific methods for that?,"Now I didn't Implement any user authentication. You'll just have a link if you click that link, you'll get to that page and you can upload an image and it will give you its prediction.",2,True
27,27,"Interviewer: It's clear you've delved deep into the intricacies of the Stacking Regressor. Given your experience with this project, how do you approach the selection of models and their hyperparameters in a machine learning project to ensure the best possible accuracy and generalization to unseen data?  

Interviewee: Yes, John, actually as we all know the trial and error method is the first method that we all do in trial and error method. We have checked different regresses like a support Vector machine. And other regressors logistic regression linear regression rhinosaurus equation rigid regression lasso regression elastic net regressor too, but only decisionally request a random forest regressor and linear regression what meant to be together and you as a great prediction score because they are totally reliable to each other and they also have atomicity of their own here the data set we have is a bit of ambiguous. So we first in my project that we are doing I did a data feature selection process here. I use grammar species correlation method for categorical features and Pearson's correlation method for numerical features after doing the correlation things. I have a generated a heat map and according to the heat map. Any correlation that is more than 70% is removed. I mean any data set any data feature with the 70% correlation is remote in that way. I have downsized my In downsized the column from my data set and it helps me it helped me more in a efficient work of my predictors and also efficient predictions of my test data. in starting with lesser a hyper parameters hyperparameters means the parameters that that give more more stress to the prediction. I have choose them in such a way that they always give me a very good predicted values and test data is always give is always the is always the testing predictor. The testing outputs are always equal to the predicted outputs. If you come to the regression to regressor, I have all seen means minimum sample rate that I need to use minimum sample of the leaf notes that I need to use and when I come to the random forestry, I need to check the maximum depth. I need to use and minimum amount and in samples that I can take like how many trees of decision tree you're going to use in the random Forest. And using all this hyperparameter tuning. I have done my project a very well and it is it was a very great approachable and also commanded by my P u sine my guides and coordinators. Thank",2,"It's clear you've delved deep into the intricacies of the Stacking Regressor. Given your experience with this project, how do you approach the selection of models and their hyperparameters in a machine learning project to ensure the best possible accuracy and generalization to unseen data?","Yes, John, actually as we all know the trial and error method is the first method that we all do in trial and error method. We have checked different regresses like a support Vector machine. And other regressors logistic regression linear regression rhinosaurus equation rigid regression lasso regression elastic net regressor too, but only decisionally request a random forest regressor and linear regression what meant to be together and you as a great prediction score because they are totally reliable to each other and they also have atomicity of their own here the data set we have is a bit of ambiguous. So we first in my project that we are doing I did a data feature selection process here. I use grammar species correlation method for categorical features and Pearson's correlation method for numerical features after doing the correlation things. I have a generated a heat map and according to the heat map. Any correlation that is more than 70% is removed. I mean any data set any data feature with the 70% correlation is remote in that way. I have downsized my In downsized the column from my data set and it helps me it helped me more in a efficient work of my predictors and also efficient predictions of my test data. in starting with lesser a hyper parameters hyperparameters means the parameters that that give more more stress to the prediction. I have choose them in such a way that they always give me a very good predicted values and test data is always give is always the is always the testing predictor. The testing outputs are always equal to the predicted outputs. If you come to the regression to regressor, I have all seen means minimum sample rate that I need to use minimum sample of the leaf notes that I need to use and when I come to the random forestry, I need to check the maximum depth. I need to use and minimum amount and in samples that I can take like how many trees of decision tree you're going to use in the random Forest. And using all this hyperparameter tuning. I have done my project a very well and it is it was a very great approachable and also commanded by my P u sine my guides and coordinators. Thank",3,True
28,28,"Interviewer: Leveraging Azure Stream Analytics for real-time data processing is a great choice, and it's clear you've put a lot of thought into feature selection and engineering using a variety of techniques. The use of the AI Control platform for explainability is also commendable. Now, considering your experience with stakeholder management, can you describe a challenging situation where you had to communicate complex machine learning concepts to non-technical stakeholders, and how you ensured that they were on board with the project's objectives and outcomes?  

Interviewee: Okay, so the most important task for a particular project to be successful as stakeholder management so for doing that, let's say if we had developing a complex model, for example, we have developed a model for zero which is the largest client in Japan basically controls 51% of electricity more than 50% 50 or 60% of electricity in Japan. They created software for them out of most of the board members are non-technical and we need to specifically share them a lot of mattresses related to it. So there are some machine learning activities which basically data scientists and machine learning Engineers need to but there are a lot of there are a lot of business matrices. Let's say confusion Matrix. So let's say if they depend upon how efficiently we are able to basically tell that to use basically exploratory returns to basically show them the insight about how the data look like and how it is. Basically once the model is developed based upon it so there are some business mattresses that also needed to be shown to the customers. For example, let's say we can use classification metrics we can use error reporting we can use how much let's say if you are working on a A question answering system. So they are how much a question is answered is more important for that. We can use a lot of metrics like similarity scores and how similar the question and answers are how many question have been answered if we are using recommendation engine then conversion rate is the most important thing which is take voters to know so stakeholder management and machine learning is a crucial thing that need to be presented using power and presentation using our visualization. So there are a lot of things which we can use.",2,"Leveraging Azure Stream Analytics for real-time data processing is a great choice, and it's clear you've put a lot of thought into feature selection and engineering using a variety of techniques. The use of the AI Control platform for explainability is also commendable. Now, considering your experience with stakeholder management, can you describe a challenging situation where you had to communicate complex machine learning concepts to non-technical stakeholders, and how you ensured that they were on board with the project's objectives and outcomes?","Okay, so the most important task for a particular project to be successful as stakeholder management so for doing that, let's say if we had developing a complex model, for example, we have developed a model for zero which is the largest client in Japan basically controls 51% of electricity more than 50% 50 or 60% of electricity in Japan. They created software for them out of most of the board members are non-technical and we need to specifically share them a lot of mattresses related to it. So there are some machine learning activities which basically data scientists and machine learning Engineers need to but there are a lot of there are a lot of business matrices. Let's say confusion Matrix. So let's say if they depend upon how efficiently we are able to basically tell that to use basically exploratory returns to basically show them the insight about how the data look like and how it is. Basically once the model is developed based upon it so there are some business mattresses that also needed to be shown to the customers. For example, let's say we can use classification metrics we can use error reporting we can use how much let's say if you are working on a A question answering system. So they are how much a question is answered is more important for that. We can use a lot of metrics like similarity scores and how similar the question and answers are how many question have been answered if we are using recommendation engine then conversion rate is the most important thing which is take voters to know so stakeholder management and machine learning is a crucial thing that need to be presented using power and presentation using our visualization. So there are a lot of things which we can use.",3,True
29,29,"Interviewer: It's great to hear about your experience with different indexing techniques. Can you explain how you applied hashing or B-tree indexing in a specific assignment to optimize a query, and the measurable improvements in query performance that resulted from your optimization efforts?  


Interviewee: Okay. So",1,"It's great to hear about your experience with different indexing techniques. Can you explain how you applied hashing or B-tree indexing in a specific assignment to optimize a query, and the measurable improvements in query performance that resulted from your optimization efforts?",Okay. So,3,False
30,30,"Interviewer: Your use of Docker and Kubernetes for ensuring high availability is quite strategic. Now, let's delve into the technical skills listed on your resume. Starting with Python, can you explain how you've optimized Python code for performance, particularly in the context of your machine learning and deep learning projects?  

Interviewee: It so like in context of Michelin and deep learning the like we can we can evaluate like where the inference pipeline where it is taking time. So if it is in case in the case of General Loops or those things like we can we can first we can make sure like whether it is code is clean or not for cleanability. So once we have all those things like with then we can go and check with the inference time. The model is taking so in that like we can check like if there is no any other errors from the code side, so then it is like inference time like you can try to we can try to reduce it from the model site. So the apart from that like we need to make sure like Cody's in the courtroom standard format like with all other handling everything. So those those if we handle those things and we have less model interest time. than we will get faster response time from the",2,"Your use of Docker and Kubernetes for ensuring high availability is quite strategic. Now, let's delve into the technical skills listed on your resume. Starting with Python, can you explain how you've optimized Python code for performance, particularly in the context of your machine learning and deep learning projects?","It so like in context of Michelin and deep learning the like we can we can evaluate like where the inference pipeline where it is taking time. So if it is in case in the case of General Loops or those things like we can we can first we can make sure like whether it is code is clean or not for cleanability. So once we have all those things like with then we can go and check with the inference time. The model is taking so in that like we can check like if there is no any other errors from the code side, so then it is like inference time like you can try to we can try to reduce it from the model site. So the apart from that like we need to make sure like Cody's in the courtroom standard format like with all other handling everything. So those those if we handle those things and we have less model interest time. than we will get faster response time from the",2,True
31,31,"Interviewer: Utilizing the Bayesian Information Criterion (BIC) for Gaussian Mixture Model selection is a sophisticated approach. How did you integrate the results of the GARCH model back into the AI system for dynamic adjustment of the generative model, ensuring that the synthetic data's volatility characteristics remained in line with the original dataset?  


Interviewee: So this is where the variational autoencoder came into play. We trained one to generate volatility services. And what happens is in the vae you're able to take a surface and reduce it down to a vector of a specific length. That's much smaller than the actual surface parameterization and by doing some statistical distribution checking on the multivariate vector, you're able to get a distribution value of possible surfaces that you that could occur in the market and now you take your synthetic data that you generated you pass it into the late Vector space and then you can do some statistical Arbitrage checking like any other multivariate data set. So that's how we're able to do that.",2,"Utilizing the Bayesian Information Criterion (BIC) for Gaussian Mixture Model selection is a sophisticated approach. How did you integrate the results of the GARCH model back into the AI system for dynamic adjustment of the generative model, ensuring that the synthetic data's volatility characteristics remained in line with the original dataset?","So this is where the variational autoencoder came into play. We trained one to generate volatility services. And what happens is in the vae you're able to take a surface and reduce it down to a vector of a specific length. That's much smaller than the actual surface parameterization and by doing some statistical distribution checking on the multivariate vector, you're able to get a distribution value of possible surfaces that you that could occur in the market and now you take your synthetic data that you generated you pass it into the late Vector space and then you can do some statistical Arbitrage checking like any other multivariate data set. So that's how we're able to do that.",4,True
32,32,"Interviewer: It's great to hear how you leveraged TypeScript's performance and type safety features for your API work. Given your experience with both Python and TypeScript, how do you ensure that the APIs you develop are secure, especially when handling sensitive data such as crypto prices and exchange information? Can you walk me through the security measures you implemented in your API design?  

Interviewee: Essentially, what are the apis we have used? Those are open source apis. So anybody can access this data. So there is no particular need for ensuring safety or something in the project because everybody can access this like if you can just go to dayama website or you can just go to crypto. I mean coin website but what the efficiency we have achieved here is a user has to go to the website and enter a particular thing and how to sort out where he is, but with AI chatbot, which is actually a thing is automated queries needed and whatever he wants comes at his eyes without the need to call the API again go and now so I prepare responses are Json format. So Json format needs filtering and we have a filtering schema to do the same with the AI chart but that started yeah.",1,"It's great to hear how you leveraged TypeScript's performance and type safety features for your API work. Given your experience with both Python and TypeScript, how do you ensure that the APIs you develop are secure, especially when handling sensitive data such as crypto prices and exchange information? Can you walk me through the security measures you implemented in your API design?","Essentially, what are the apis we have used? Those are open source apis. So anybody can access this data. So there is no particular need for ensuring safety or something in the project because everybody can access this like if you can just go to dayama website or you can just go to crypto. I mean coin website but what the efficiency we have achieved here is a user has to go to the website and enter a particular thing and how to sort out where he is, but with AI chatbot, which is actually a thing is automated queries needed and whatever he wants comes at his eyes without the need to call the API again go and now so I prepare responses are Json format. So Json format needs filtering and we have a filtering schema to do the same with the AI chart but that started yeah.",2,True
33,33,"Interviewer: Certainly, take your time. When you're ready, I'd like to hear your explanation of the difference between TensorFlow constants and variables, and their respective use cases.  

Interviewee: yeah, but",1,"Certainly, take your time. When you're ready, I'd like to hear your explanation of the difference between TensorFlow constants and variables, and their respective use cases.","yeah, but",1,False
34,34,"Interviewer: You've provided a clear explanation of transfer learning and its application in NLP, particularly with ULMFiT. Now, could you describe a scenario where you would choose PyTorch over TensorFlow for a new project and explain your reasoning behind that choice? 

Interviewee: Oh when I faced with the choice where I'll have to implement a new paper research paper that I'm studying or thinking of the new architecture. Maybe I want to add a few more extra blocks to say an efficient Network which Gene net or resonate. I just want to add something else extra to it. I would choose python because It's very simple to add to it. So the you can just code each blocks as is and then you can add the separate ones and all of them are chained together, which means you can form the outputs. You can do lost backwards on the outputs as well. It's attached the chain. So I think whenever I need to add something change something because of the flexibility I mentioned earlier I would go for pytorch and also the main one remaining is the presence of good documentation and good community.",2,"You've provided a clear explanation of transfer learning and its application in NLP, particularly with ULMFiT. Now, could you describe a scenario where you would choose PyTorch over TensorFlow for a new project and explain your reasoning behind that choice?","Oh when I faced with the choice where I'll have to implement a new paper research paper that I'm studying or thinking of the new architecture. Maybe I want to add a few more extra blocks to say an efficient Network which Gene net or resonate. I just want to add something else extra to it. I would choose python because It's very simple to add to it. So the you can just code each blocks as is and then you can add the separate ones and all of them are chained together, which means you can form the outputs. You can do lost backwards on the outputs as well. It's attached the chain. So I think whenever I need to add something change something because of the flexibility I mentioned earlier I would go for pytorch and also the main one remaining is the presence of good documentation and good community.",2,True
35,35,"Interviewer: Correct, skewed data often warrants the use of median for imputation to mitigate the influence of outliers. Now, let's delve into the realm of Computer Vision, which you've also listed as a skill. Can you explain how you've applied transfer learning in a Computer Vision project, including the specific pre-trained models you've utilized and the new layers you've added for fine-tuning?  

Interviewee: Projects, you know in computer vision area like for learning purposes only I did computer vision projects and I have a very little knowledge about it like a very basic knowledge about it.",1,"Correct, skewed data often warrants the use of median for imputation to mitigate the influence of outliers. Now, let's delve into the realm of Computer Vision, which you've also listed as a skill. Can you explain how you've applied transfer learning in a Computer Vision project, including the specific pre-trained models you've utilized and the new layers you've added for fine-tuning?","Projects, you know in computer vision area like for learning purposes only I did computer vision projects and I have a very little knowledge about it like a very basic knowledge about it.",2,True
36,36,"Interviewer: Using Python and Pandas for data manipulation is a great choice. Since you've also mentioned proficiency in JavaScript, can you tell me about a complex problem you've solved using JavaScript, and which libraries or frameworks you used to assist you?  

Interviewee: So basically Java Java Script I used to I used to basically I used to work on JavaScript as as an angular developer. So firstly I feel difficulty to use JavaScript because angular is totally based on type script. So I I learned I learned the basics of JavaScript like Dom manipulation and other loops and other things then I shifted to type script which which have more functionality and more strict strictness then JavaScript. So I chose type script because it provides classes and other other main things. So my proficiency is in angular and I find that difficulty when I work on my e-commerce project, so I found some difficulty in there. So there is latest angular 17 is there and I currently I am working on that.",2,"Using Python and Pandas for data manipulation is a great choice. Since you've also mentioned proficiency in JavaScript, can you tell me about a complex problem you've solved using JavaScript, and which libraries or frameworks you used to assist you?","So basically Java Java Script I used to I used to basically I used to work on JavaScript as as an angular developer. So firstly I feel difficulty to use JavaScript because angular is totally based on type script. So I I learned I learned the basics of JavaScript like Dom manipulation and other loops and other things then I shifted to type script which which have more functionality and more strict strictness then JavaScript. So I chose type script because it provides classes and other other main things. So my proficiency is in angular and I find that difficulty when I work on my e-commerce project, so I found some difficulty in there. So there is latest angular 17 is there and I currently I am working on that.",2,True
37,37,"Interviewer: Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview. 

Interviewee:",1,Thanks for taking the time to interview with us! The time allotted for our interview has come to an end. We will reach out to you with next steps shortly. Please click the end interview button to end the interview.,,1,False
38,38,"Interviewer: It's clear you had a thorough approach to maintaining code quality. Shifting focus to your coursework, you've listed Computer Architecture, Operating Systems, Machine Learning, Deep Learning, and Computer Vision. Can you discuss a project or concept from your Computer Architecture course that you found particularly challenging, and how you overcame this challenge? 


Interviewee: So the part in computer architecture, which was the most challenging for me, was when we were taught caching. Because caching as a concept is very complicated, and there are multiple layers involved to it. Also, since it involves hardware, I had to employ my previous knowledge of digital design to make sure that the concepts which I'm learning there are coherent with what I have learned before in digital design. That's one aspect which I found very confusing and computer architecture. And the solution was to read more books about it and watch more videos to understand exactly how it happens and try to correlate it with my previous knowledge to make sure everything is in at point.",2,"It's clear you had a thorough approach to maintaining code quality. Shifting focus to your coursework, you've listed Computer Architecture, Operating Systems, Machine Learning, Deep Learning, and Computer Vision. Can you discuss a project or concept from your Computer Architecture course that you found particularly challenging, and how you overcame this challenge?","So the part in computer architecture, which was the most challenging for me, was when we were taught caching. Because caching as a concept is very complicated, and there are multiple layers involved to it. Also, since it involves hardware, I had to employ my previous knowledge of digital design to make sure that the concepts which I'm learning there are coherent with what I have learned before in digital design. That's one aspect which I found very confusing and computer architecture. And the solution was to read more books about it and watch more videos to understand exactly how it happens and try to correlate it with my previous knowledge to make sure everything is in at point.",3,True
39,39,"Interviewer: Using Hugging Face's evaluation metrics for AI models is a robust way to assess performance across multiple dimensions. How do you envision further optimizing your system based on the feedback from these metrics, especially in the context of improving model accuracy without significantly increasing computational load?  

Interviewee: Like for this week and able to find tune the large language models, so it will take an input as an a query answer and the context and the prompt. So if you using this we can able to like we can able to generate a question and answer on the contacts. And after that we just give it to Anna a large language model and we can able to find you which will be more appropriate for the particular queue and decision.",1,"Using Hugging Face's evaluation metrics for AI models is a robust way to assess performance across multiple dimensions. How do you envision further optimizing your system based on the feedback from these metrics, especially in the context of improving model accuracy without significantly increasing computational load?","Like for this week and able to find tune the large language models, so it will take an input as an a query answer and the context and the prompt. So if you using this we can able to like we can able to generate a question and answer on the contacts. And after that we just give it to Anna a large language model and we can able to find you which will be more appropriate for the particular queue and decision.",2,True
40,40,"Interviewer: That's a very interesting application of computer vision and text processing. You mentioned using OpenCV for handling the PDFs with mathematical content. Could you tell me more about how you integrated OpenCV with other libraries or tools to achieve the full pipeline from PDF to HTML, especially when dealing with the challenges of mathematical expressions?  

Interviewee: Yeah, so what I am doing like we have lots of PDFs like normal test papers you can think of like SAT test papers or Jee test level. So what I'm doing is we have we have first a model which is a director on the electron to base model, which is useful layout passing. So what I'm doing is we are we are labeling everything on the PDF into different bounding boxes. Like if there is a paragraph then we are we are making a bounding box. For that paragraph and then if there is there a formulas like x square equals to something in between that and I'm using another bonding box all they take then using these labeling technique. I am able to separate text latex and I mean latest so after these we have only only text left in the PDF know and passing this open CV and passing this PDF through an opencv control where we can identify the controls based on the colors. Like if I want to read line by land and I will use Contour which has kernels only one person vertically and Lord like 200 kernels so that Can separate line by line from the text? So so now we have bounding boxes for each line and also bonding boxes for a text like latex and figure so so from this we can we have from this Body Works. We I pass it to the Tesla Model Tesla Model that OCR",2,"That's a very interesting application of computer vision and text processing. You mentioned using OpenCV for handling the PDFs with mathematical content. Could you tell me more about how you integrated OpenCV with other libraries or tools to achieve the full pipeline from PDF to HTML, especially when dealing with the challenges of mathematical expressions?","Yeah, so what I am doing like we have lots of PDFs like normal test papers you can think of like SAT test papers or Jee test level. So what I'm doing is we have we have first a model which is a director on the electron to base model, which is useful layout passing. So what I'm doing is we are we are labeling everything on the PDF into different bounding boxes. Like if there is a paragraph then we are we are making a bounding box. For that paragraph and then if there is there a formulas like x square equals to something in between that and I'm using another bonding box all they take then using these labeling technique. I am able to separate text latex and I mean latest so after these we have only only text left in the PDF know and passing this open CV and passing this PDF through an opencv control where we can identify the controls based on the colors. Like if I want to read line by land and I will use Contour which has kernels only one person vertically and Lord like 200 kernels so that Can separate line by line from the text? So so now we have bounding boxes for each line and also bonding boxes for a text like latex and figure so so from this we can we have from this Body Works. We I pass it to the Tesla Model Tesla Model that OCR",3,True
41,41,"Interviewer: Your approach to feature engineering and handling missing data through the salutation part of the name in the Titanic dataset is a great example of deriving meaningful information from seemingly unrelated features. In the context of handling categorical variables, like the ""Title"" feature you engineered from the name, what are some advanced techniques or methodologies you would consider for encoding or utilizing these categorical variables in a way that enhances the model's predictive capability?  


Interviewee: I have used Rejects and I have used a basic python formatting techniques basic python technique substring to get a substring from the string. That's the basic things from in Python. Since python is a really friendly user-friendly programming language. It helped me so much in deriving those important substrings of the feature that I was working. So basic python helped me do it without putting much of words and it but in the end of the day, it's us who should we who we who should decide what is important and what is not important by analyzing which I think analyzing and getting to know what will give you a meaningful information is more important than the python methods and python syntax. Like logic is more important than the syntax. I hope I answered your question.",1,"Your approach to feature engineering and handling missing data through the salutation part of the name in the Titanic dataset is a great example of deriving meaningful information from seemingly unrelated features. In the context of handling categorical variables, like the ""Title"" feature you engineered from the name, what are some advanced techniques or methodologies you would consider for encoding or utilizing these categorical variables in a way that enhances the model's predictive capability?","I have used Rejects and I have used a basic python formatting techniques basic python technique substring to get a substring from the string. That's the basic things from in Python. Since python is a really friendly user-friendly programming language. It helped me so much in deriving those important substrings of the feature that I was working. So basic python helped me do it without putting much of words and it but in the end of the day, it's us who should we who we who should decide what is important and what is not important by analyzing which I think analyzing and getting to know what will give you a meaningful information is more important than the python methods and python syntax. Like logic is more important than the syntax. I hope I answered your question.",2,True
42,42,"Interviewer: It seems you might need a moment to gather your thoughts. When you're prepared, please share your approach to evaluating the NLP model's performance, including any metrics you used beyond cosine similarity, and your strategies for managing false positives and false negatives. This will give me a better understanding of your problem-solving skills in the context of NLP.  

Interviewee: So exactly so we were using the Precision. Okay, and accuracy is cool. So right now the thing is that accuracy score like is what we can say false more towards the true positives and the false negatives one. So it will just give you the idea of using the true positive and false positive false negatives great. So increasing the accuracy is another task. It is one of the tasks obviously, but we had to take into account the recall also and the procedure so these are the techniques we used to like so just using one metric for this similarity search like cosine similarity in state of Wisconsin similarity accuracy precision and recover also used. I don't remember using F1 score actually. So yeah, these three words are you used?",2,"It seems you might need a moment to gather your thoughts. When you're prepared, please share your approach to evaluating the NLP model's performance, including any metrics you used beyond cosine similarity, and your strategies for managing false positives and false negatives. This will give me a better understanding of your problem-solving skills in the context of NLP.","So exactly so we were using the Precision. Okay, and accuracy is cool. So right now the thing is that accuracy score like is what we can say false more towards the true positives and the false negatives one. So it will just give you the idea of using the true positive and false positive false negatives great. So increasing the accuracy is another task. It is one of the tasks obviously, but we had to take into account the recall also and the procedure so these are the techniques we used to like so just using one metric for this similarity search like cosine similarity in state of Wisconsin similarity accuracy precision and recover also used. I don't remember using F1 score actually. So yeah, these three words are you used?",2,True
43,43,"Interviewer: Handling a memory leak and server issues during a release is a significant challenge. Given the complexity of Google's infrastructure, how did you approach the debugging process to pinpoint the source of the memory leak, and what tools or techniques did you find most effective in resolving such a critical issue?  


Interviewee: So I was the person here I didn't work on that specific bug because Google is a big company and different companies were handled by different, places I mean, different teams which are located in different offices. So I had to reach out to them who worked on that. Section and who are responsible for the memory Memorial League And they called. There was a SRE engineer and some other engineers which were direct point of contact for these kind of issues. So I just, send it to them in order to fix them, and they were usually working on that side.",1,"Handling a memory leak and server issues during a release is a significant challenge. Given the complexity of Google's infrastructure, how did you approach the debugging process to pinpoint the source of the memory leak, and what tools or techniques did you find most effective in resolving such a critical issue?","So I was the person here I didn't work on that specific bug because Google is a big company and different companies were handled by different, places I mean, different teams which are located in different offices. So I had to reach out to them who worked on that. Section and who are responsible for the memory Memorial League And they called. There was a SRE engineer and some other engineers which were direct point of contact for these kind of issues. So I just, send it to them in order to fix them, and they were usually working on that side.",2,True
44,44,"Interviewer: It's impressive how you've tackled the complexities of web scraping, especially with the variety of site structures and the challenges of multi-threading. Using Puppeteer clusters to handle exceptions and maintain a list of URLs for re-scraping is a smart solution for ensuring data integrity. Given your experience with Puppeteer and web scraping, how do you handle the potential legal and ethical issues that can arise from scraping data from websites, and what measures do you take to ensure compliance with terms of service and copyright laws?  

Interviewee: Yeah, whatever the websites we have created. We didn't scrape the without permissions our company got a strategic partnership with those websites and only those things we have created and integrated into our chatbot. And there is nothing that we have done outside of our elements.",1,"It's impressive how you've tackled the complexities of web scraping, especially with the variety of site structures and the challenges of multi-threading. Using Puppeteer clusters to handle exceptions and maintain a list of URLs for re-scraping is a smart solution for ensuring data integrity. Given your experience with Puppeteer and web scraping, how do you handle the potential legal and ethical issues that can arise from scraping data from websites, and what measures do you take to ensure compliance with terms of service and copyright laws?","Yeah, whatever the websites we have created. We didn't scrape the without permissions our company got a strategic partnership with those websites and only those things we have created and integrated into our chatbot. And there is nothing that we have done outside of our elements.",3,True
45,45,"Interviewer: Your approach to fine-tuning with a focus on ethical considerations in healthcare is commendable. The use of DPO for creating a guardrail to prevent the chatbot from providing medical advice is a critical application of AI. It's also interesting to hear about the different fine-tuning techniques, such as continuation and instruction-based fine-tuning, which are tailored to specific domains and user interactions. Given the complexity of these models and the potential for bias, how do you ensure fairness and ethical considerations in your NLP models, especially when dealing with sensitive topics like healthcare?  

Interviewee: Oh, all right. So that's you know and Healthcare you rightly said, you know for a healthcare. We need to have these all biases and we should not let the model go any wrong and that's where DPO comes in. Right? So DPO is such that it tries to reduce the bias the rejected answer and give most priority to the choose an answer. That's how the dpu train where you know, there's a right answer and there's a wrong answer and we try to you know deviate from the wrong answer and come close to the right answer so that we can reduce the biases in our model. That's one way to do it now, you know after doing it also we have these feedback mechanism to our llms what we do this what we do in this feedback mechanism is we try to take the feedback from a user where if you like the if you like the answer is going to give us a thumbs up and show some gratitude so that we understand. Hey the answer looks good, and if You dislikes it we all have this feedback very you can give us detail feedbacks and on the on that feedback. We train the model again such that it tries to, you know, also reduce the bias and and also be faithful to the customer if he's liking it or not. That's how we do it and there are a lot of other techniques and there are a lot of other metrics which we are very we have these McQ kind of a metrics what we do is we we Have a pre-trained set. Let's say we have around about 1000 questions, which we asked to these asked to the model and we already know the you know the answer to it. So it's kind of a super we converted this as a supervised metric such that given the 1000 questions. How many questions is the model is able to answer rightly and if it's not able to answer a few questions, we try to train it again such that it can also include those questions and these 1000 questions which we created on so that we can understand. These are all created. These are all questions on clinical data and medical kind of a domain so that we can understand what's going on and we can try to improve the biases of these elements.",3,"Your approach to fine-tuning with a focus on ethical considerations in healthcare is commendable. The use of DPO for creating a guardrail to prevent the chatbot from providing medical advice is a critical application of AI. It's also interesting to hear about the different fine-tuning techniques, such as continuation and instruction-based fine-tuning, which are tailored to specific domains and user interactions. Given the complexity of these models and the potential for bias, how do you ensure fairness and ethical considerations in your NLP models, especially when dealing with sensitive topics like healthcare?","Oh, all right. So that's you know and Healthcare you rightly said, you know for a healthcare. We need to have these all biases and we should not let the model go any wrong and that's where DPO comes in. Right? So DPO is such that it tries to reduce the bias the rejected answer and give most priority to the choose an answer. That's how the dpu train where you know, there's a right answer and there's a wrong answer and we try to you know deviate from the wrong answer and come close to the right answer so that we can reduce the biases in our model. That's one way to do it now, you know after doing it also we have these feedback mechanism to our llms what we do this what we do in this feedback mechanism is we try to take the feedback from a user where if you like the if you like the answer is going to give us a thumbs up and show some gratitude so that we understand. Hey the answer looks good, and if You dislikes it we all have this feedback very you can give us detail feedbacks and on the on that feedback. We train the model again such that it tries to, you know, also reduce the bias and and also be faithful to the customer if he's liking it or not. That's how we do it and there are a lot of other techniques and there are a lot of other metrics which we are very we have these McQ kind of a metrics what we do is we we Have a pre-trained set. Let's say we have around about 1000 questions, which we asked to these asked to the model and we already know the you know the answer to it. So it's kind of a super we converted this as a supervised metric such that given the 1000 questions. How many questions is the model is able to answer rightly and if it's not able to answer a few questions, we try to train it again such that it can also include those questions and these 1000 questions which we created on so that we can understand. These are all created. These are all questions on clinical data and medical kind of a domain so that we can understand what's going on and we can try to improve the biases of these elements.",3,True
46,46,"Interviewer: Thank you for sharing that detailed explanation. Can you elaborate on your role in developing the AI-powered Mental Health Fitness Tracker at Edunet Foundation? What specific technologies and methodologies did you employ? 


Interviewee: These are included in my project, and the code provides a comprehensive in the mental health digit recognition project. The main target features was a different mental illness",1,Thank you for sharing that detailed explanation. Can you elaborate on your role in developing the AI-powered Mental Health Fitness Tracker at Edunet Foundation? What specific technologies and methodologies did you employ?,"These are included in my project, and the code provides a comprehensive in the mental health digit recognition project. The main target features was a different mental illness",2,True
47,47,"Interviewer: The use of FID scores for evaluating the image quality post-compression is a solid approach. When it comes to the quantization of model weights, could you elaborate on the specific quantization techniques you applied, such as whether you used fixed-point or dynamic quantization, and how you determined the appropriate level of quantization to balance model size reduction with output fidelity?  


Interviewee: So, basically, for, reducing the model size, so the general,",1,"The use of FID scores for evaluating the image quality post-compression is a solid approach. When it comes to the quantization of model weights, could you elaborate on the specific quantization techniques you applied, such as whether you used fixed-point or dynamic quantization, and how you determined the appropriate level of quantization to balance model size reduction with output fidelity?","So, basically, for, reducing the model size, so the general,",3,False
48,48,"Interviewer: Understood, GPT-4 is indeed a powerful tool. It's great to hear that you've employed techniques like Chain of Thought prompting to maintain coherence and prevent hallucinations, especially with numerical data. Now, shifting focus a bit, I noticed you've also worked on a solution for document comparison using Google Cloud Platform. Can you describe the architecture you used for this solution and how you managed to achieve high accuracy in the comparison process?  

Interviewee: Yes, so if you are referring to the greatest and project. So what happened is so grossed out is a real estate company. So being in real estate and they they usually get high number of documents. So they have this three types of specific documents. One of them is upload. One of them is commitment and another one is loan agreement. So across this trade documents, they have to compare some of the entities and make sure that the entities are. Equal in all these three documents if there is any discrepancy that is found then the human in the loop pitch notified. So back, then we didn't have any other resources. So we used. the document OCR of Google Cloud platform and so using that to try to extract the entities in Json format and then we use Python logic in order to Find the Json output and get our specific entities and for tabular data. We were facing some problem. um, because the Google Cloud OCR it sometimes it reads the text row wise while other times it is the text in column wise so In that case it was difficult for us to face the tabular data. So in that case, we utilize the test direct OCR. And that worked pretty well. So yeah that that's that was my contribution in the project. So we were able like I was specifically working on the commitment part and the commitment and document part. So we were able to extract the entities from the PDF similarly the other message learning Engineers. They were like one of them was working on lawn agreement part while the other one was working on the upload document part. So once we got the output from each of these documents then we created a accuracy comparison using logic and anything that went below 95 below 9 like we said it was 90% So anything that went below 90% we like we were highlighting it as a discrepancy. Yeah.",3,"Understood, GPT-4 is indeed a powerful tool. It's great to hear that you've employed techniques like Chain of Thought prompting to maintain coherence and prevent hallucinations, especially with numerical data. Now, shifting focus a bit, I noticed you've also worked on a solution for document comparison using Google Cloud Platform. Can you describe the architecture you used for this solution and how you managed to achieve high accuracy in the comparison process?","Yes, so if you are referring to the greatest and project. So what happened is so grossed out is a real estate company. So being in real estate and they they usually get high number of documents. So they have this three types of specific documents. One of them is upload. One of them is commitment and another one is loan agreement. So across this trade documents, they have to compare some of the entities and make sure that the entities are. Equal in all these three documents if there is any discrepancy that is found then the human in the loop pitch notified. So back, then we didn't have any other resources. So we used. the document OCR of Google Cloud platform and so using that to try to extract the entities in Json format and then we use Python logic in order to Find the Json output and get our specific entities and for tabular data. We were facing some problem. um, because the Google Cloud OCR it sometimes it reads the text row wise while other times it is the text in column wise so In that case it was difficult for us to face the tabular data. So in that case, we utilize the test direct OCR. And that worked pretty well. So yeah that that's that was my contribution in the project. So we were able like I was specifically working on the commitment part and the commitment and document part. So we were able to extract the entities from the PDF similarly the other message learning Engineers. They were like one of them was working on lawn agreement part while the other one was working on the upload document part. So once we got the output from each of these documents then we created a accuracy comparison using logic and anything that went below 95 below 9 like we said it was 90% So anything that went below 90% we like we were highlighting it as a discrepancy. Yeah.",3,True
49,49,"Interviewer: Absolutely, that's a fantastic application of generators in the context of data augmentation for deep learning. Could you explain the concept of ""yield"" in Python and how it is used in the context of defining a generator function? How does the use of ""yield"" differ from ""return"" in a standard function, and what advantages does it offer in terms of memory efficiency and code execution?  

Interviewee: Mmm, I have implemented many a project but I have never come across as some function called yield, but I'm very much excited to know about it.",1,"Absolutely, that's a fantastic application of generators in the context of data augmentation for deep learning. Could you explain the concept of ""yield"" in Python and how it is used in the context of defining a generator function? How does the use of ""yield"" differ from ""return"" in a standard function, and what advantages does it offer in terms of memory efficiency and code execution?","Mmm, I have implemented many a project but I have never come across as some function called yield, but I'm very much excited to know about it.",1,False
