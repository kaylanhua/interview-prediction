,Unnamed: 0.2,Unnamed: 0.1,Unnamed: 0,Input Text,Label,Question,Response,zero_shot,Valid,final_prediction,xgb,length_lr
0,0,0,0,"Interviewer: Achieving state-of-the-art results is impressive. How do you envision further improving the system's capability to handle a wider range of financial questions and document types, considering the limitations of dataset-specific fine-tuning?  


Interviewee: So one of the things that can be done is query expansion that is being used and retrieval augmented generate generation tasks a lot. So what happens is given a query you use another model like a large language model to expand this query identifying topics that can be helpful in answering this question or writing several writing this query in a different way. And using this expanded query to create a vector representation and this helps it to retrieve much better documents and it helps to actually Identify some topics or context that was not even trained on a data set specific fine tuning task and for prompting I think newer models are very very good on numerical reasoning. And the questions that are covered in the finger data set actually are more specific to final question answering or people who are really interested in financial question answering. So if there is a use case or if there is there a few questions that doesn't answer correctly, then we can like enhance the problem, but I think you actually covers a lot of different types of financial questions. Or we can also integrate other data sets that are related to financial question answering and integrate them into the dynamically generated prompts.",4,"Achieving state-of-the-art results is impressive. How do you envision further improving the system's capability to handle a wider range of financial questions and document types, considering the limitations of dataset-specific fine-tuning?","So one of the things that can be done is query expansion that is being used and retrieval augmented generate generation tasks a lot. So what happens is given a query you use another model like a large language model to expand this query identifying topics that can be helpful in answering this question or writing several writing this query in a different way. And using this expanded query to create a vector representation and this helps it to retrieve much better documents and it helps to actually Identify some topics or context that was not even trained on a data set specific fine tuning task and for prompting I think newer models are very very good on numerical reasoning. And the questions that are covered in the finger data set actually are more specific to final question answering or people who are really interested in financial question answering. So if there is a use case or if there is there a few questions that doesn't answer correctly, then we can like enhance the problem, but I think you actually covers a lot of different types of financial questions. Or we can also integrate other data sets that are related to financial question answering and integrate them into the dynamically generated prompts.",4,True,,2,2
1,1,2,2,"Interviewer: Testing the security of a system, especially one involving wireless communication, can be challenging. Could you discuss any specific testing methodologies or tools you used to verify the security and reliability of your wireless traffic light system during development? Additionally, how did the results of these tests influence the design of your system?  

Interviewee: During the development phase of a radio frequency Communication System. We approach testing the security by conducting Radiologists testing and evaluating. We Simon We simulating various synchronizes to identify to identify or not and weakness in the system. We also provide penetration testing to assess a system resting against an organized access to ensure the systems security against an organized access. We implemented several measurements We Trust wrong we request wrong and protocol to protect the transmitter transmitted data. We also implemented Access Control mechanisms such as security organized and other indication process to ensure the only organized can communicate with the system furthermore. We regularly updated and Pat. This is a system software address any known security while while they're both consider regular security edited and assignments to identify and interesting enemy. Potential security is by talking this measures we aim to create a rope a robust and secure radio frequency communication system that provided the confidential integration a liability that transmitted data. Let me know if you have any further questions.",2,"Testing the security of a system, especially one involving wireless communication, can be challenging. Could you discuss any specific testing methodologies or tools you used to verify the security and reliability of your wireless traffic light system during development? Additionally, how did the results of these tests influence the design of your system?",During the development phase of a radio frequency Communication System. We approach testing the security by conducting Radiologists testing and evaluating. We Simon We simulating various synchronizes to identify to identify or not and weakness in the system. We also provide penetration testing to assess a system resting against an organized access to ensure the systems security against an organized access. We implemented several measurements We Trust wrong we request wrong and protocol to protect the transmitter transmitted data. We also implemented Access Control mechanisms such as security organized and other indication process to ensure the only organized can communicate with the system furthermore. We regularly updated and Pat. This is a system software address any known security while while they're both consider regular security edited and assignments to identify and interesting enemy. Potential security is by talking this measures we aim to create a rope a robust and secure radio frequency communication system that provided the confidential integration a liability that transmitted data. Let me know if you have any further questions.,3,True,,2,2
2,2,3,3,"Interviewer: That's a great use of Docker for ensuring consistency across different development environments and for facilitating collaboration. It's clear that you understand the value of containerization in software development. Now, considering your experience with both front-end and back-end technologies, how do you ensure that the communication between the client-side and server-side is secure, especially when handling sensitive data? Can you discuss the security measures you've implemented in your projects?  

Interviewee: I'm sure the bay the main like this project of differently. It's kind of a very it's a prototype kind of that we worked on which is soon the video prior to me. I had to test it. The main aim was offered was once it gets successful in our College. We kind of have other college. We'll see if other colleges too can make need of use out of it and the very basic level of security that we are doing was it's like the JWT token itself. For example, it was very focused on two main aspects authorized authentication and authorization like authentication is basically a fire if I am if I'm the king I have a castle and I want to different people to come and visit my castle. Authentication is whether this person has been invited to my castle of to my party. Yes or no. That is how I catch these authentication. Okay, once that person has entered my after authentication is an what all he can do in my castle is authorization. So to have that enabled was basically I was using my JWT which itself had 256 bit of encryption and I was for example if person is signing in he has a context statement on the front ending of context state which what we call which returns a JWT token to our to our client site which itself can be like they really comes with a functionality where you can you know, kind of you can you can Average time to live or you can say expiration date, right? It comes with its expiration date. We can set the expression date so we can say that to have a better secure mechanism as for now. It's like I can give for one user login. I can give a maximum of 10 minutes of expression that itself can be, you know, after 10 minutes the P might the person might have to know the authenticate about putting into his password that too can be no bypass if we can like there's a second implementation. It's a furthermore implementation that I can do is refreshing the token and reasoning in the refresh token and I'm sharing it to my client again to make sure the person who's using. My website is the same person who logged in they should not be different. It's like kind of that I'm not not be a very nice idea but definitely have taken on security code this semester and learning various aspects of security. Which out for the like to implement in my projects ahead.",2,"That's a great use of Docker for ensuring consistency across different development environments and for facilitating collaboration. It's clear that you understand the value of containerization in software development. Now, considering your experience with both front-end and back-end technologies, how do you ensure that the communication between the client-side and server-side is secure, especially when handling sensitive data? Can you discuss the security measures you've implemented in your projects?","I'm sure the bay the main like this project of differently. It's kind of a very it's a prototype kind of that we worked on which is soon the video prior to me. I had to test it. The main aim was offered was once it gets successful in our College. We kind of have other college. We'll see if other colleges too can make need of use out of it and the very basic level of security that we are doing was it's like the JWT token itself. For example, it was very focused on two main aspects authorized authentication and authorization like authentication is basically a fire if I am if I'm the king I have a castle and I want to different people to come and visit my castle. Authentication is whether this person has been invited to my castle of to my party. Yes or no. That is how I catch these authentication. Okay, once that person has entered my after authentication is an what all he can do in my castle is authorization. So to have that enabled was basically I was using my JWT which itself had 256 bit of encryption and I was for example if person is signing in he has a context statement on the front ending of context state which what we call which returns a JWT token to our to our client site which itself can be like they really comes with a functionality where you can you know, kind of you can you can Average time to live or you can say expiration date, right? It comes with its expiration date. We can set the expression date so we can say that to have a better secure mechanism as for now. It's like I can give for one user login. I can give a maximum of 10 minutes of expression that itself can be, you know, after 10 minutes the P might the person might have to know the authenticate about putting into his password that too can be no bypass if we can like there's a second implementation. It's a furthermore implementation that I can do is refreshing the token and reasoning in the refresh token and I'm sharing it to my client again to make sure the person who's using. My website is the same person who logged in they should not be different. It's like kind of that I'm not not be a very nice idea but definitely have taken on security code this semester and learning various aspects of security. Which out for the like to implement in my projects ahead.",2,True,,2,3
3,3,4,4,"Interviewer: Given the complexity of parameter sweeps and the need to identify the optimal configuration, how did you manage the computational resources required for such extensive testing? Additionally, can you discuss any specific tools or techniques you used to automate or streamline the process of running and analyzing these parameter sweeps, especially considering the large-scale nature of the EPYC server environment? 


Interviewee: So the resources were managed in such a way that everyone working at AMD in our team was assigned one server to us. So I was assigned a server of my own. So I did not need to share this, system or resources with anyone else. Also, to ensure that the resources are being used optimally, I used to develop code and scripts in the daytime. And when I used to log off during the night, I used to run, my workloads or my parameter sweeps so that the next day when I log in back, I have the data ready with me, so which I can analyze further. And develop my scripts accordingly. Also, to make sure since there are a lot of parameter suites to be done, what I did was I wrote wrapper scripts, which iterate through, every data point in the sweep. And, it generates data accordingly. So with the help of a single run script, or a single wrapper script, I was able to generate all the data which was required for me.",2,"Given the complexity of parameter sweeps and the need to identify the optimal configuration, how did you manage the computational resources required for such extensive testing? Additionally, can you discuss any specific tools or techniques you used to automate or streamline the process of running and analyzing these parameter sweeps, especially considering the large-scale nature of the EPYC server environment?","So the resources were managed in such a way that everyone working at AMD in our team was assigned one server to us. So I was assigned a server of my own. So I did not need to share this, system or resources with anyone else. Also, to ensure that the resources are being used optimally, I used to develop code and scripts in the daytime. And when I used to log off during the night, I used to run, my workloads or my parameter sweeps so that the next day when I log in back, I have the data ready with me, so which I can analyze further. And develop my scripts accordingly. Also, to make sure since there are a lot of parameter suites to be done, what I did was I wrote wrapper scripts, which iterate through, every data point in the sweep. And, it generates data accordingly. So with the help of a single run script, or a single wrapper script, I was able to generate all the data which was required for me.",3,True,,2,2
4,4,5,5,"Interviewer: That's impressive. Dealing with computational resources and data labeling can indeed be challenging. Regarding the DeepStream application you used for model training and deployment, what specific features or functionalities did you find most useful in the context of your project, and were there any limitations you encountered with DeepStream that you had to work around?  

Interviewee: Using deep stream and you will be very enhanced our productivity of bounding box during a model rendering. So and the challenge is we face is a frame rate in the Deep stream. The frame rate is on is compared to other few other tools and Technologies we have right now. The frame rate is very low. So as of our budget are limitations we should use data stream. So we use that in future. We are thinking to upgrade our tools and technology so with we can increase the frame rate of the predictions.",2,"That's impressive. Dealing with computational resources and data labeling can indeed be challenging. Regarding the DeepStream application you used for model training and deployment, what specific features or functionalities did you find most useful in the context of your project, and were there any limitations you encountered with DeepStream that you had to work around?",Using deep stream and you will be very enhanced our productivity of bounding box during a model rendering. So and the challenge is we face is a frame rate in the Deep stream. The frame rate is on is compared to other few other tools and Technologies we have right now. The frame rate is very low. So as of our budget are limitations we should use data stream. So we use that in future. We are thinking to upgrade our tools and technology so with we can increase the frame rate of the predictions.,2,True,,2,2
5,5,6,6,"Interviewer: Using synthetic data for calibration is a smart approach to initially perfect the system. How do you plan to transition from synthetic data to real-world data for calibration, and what are the expected challenges in this transition?  

Interviewee: Yeah, so first and foremost challenge that we are going to face is like knowing the location of the cameras with respect to the real world a center coordinate. So in synthetic data, like the Unreal Engine we all we can just click the camera and we know what is 3D location is but actually but in actual like real words scenario, we cannot measure accurately the distance of the campus from the center of something into 31. So this is the first challenge that we are going to face and we are tend to solve this by using the PNP problems like like we will have certain 20 to 30. 3D points in the ground and also with corresponding points in the image plane. So with the help of these points you will be able to accurately Define the 3D coordinates of the camera with respect to the real world using PNC. So we are assuming we will be able to handle this. But let's see what happens.",2,"Using synthetic data for calibration is a smart approach to initially perfect the system. How do you plan to transition from synthetic data to real-world data for calibration, and what are the expected challenges in this transition?","Yeah, so first and foremost challenge that we are going to face is like knowing the location of the cameras with respect to the real world a center coordinate. So in synthetic data, like the Unreal Engine we all we can just click the camera and we know what is 3D location is but actually but in actual like real words scenario, we cannot measure accurately the distance of the campus from the center of something into 31. So this is the first challenge that we are going to face and we are tend to solve this by using the PNP problems like like we will have certain 20 to 30. 3D points in the ground and also with corresponding points in the image plane. So with the help of these points you will be able to accurately Define the 3D coordinates of the camera with respect to the real world using PNC. So we are assuming we will be able to handle this. But let's see what happens.",3,True,,2,2
6,6,7,7,"Interviewer: Given the challenges with hardware setup and the lack of community support, how did you approach debugging and resolving issues on the QCS 610 board? Additionally, what specific considerations did you have to keep in mind while porting the software to ensure it ran effectively on the embedded platform? 


Interviewee: So the challenge so the way we debug these issues was to look at the logs generated while we are installing the the software and if there are any warnings generated. We also consulted people who have worked on similar hardware before, to get their insights on what might be the issue and how do we resolve them. Coming to, coming to making sure that the software is ported correctly, we again looked at the warnings generated, and we defined very, very basic unit test cases. To make sure that the porting is complete.",2,"Given the challenges with hardware setup and the lack of community support, how did you approach debugging and resolving issues on the QCS 610 board? Additionally, what specific considerations did you have to keep in mind while porting the software to ensure it ran effectively on the embedded platform?","So the challenge so the way we debug these issues was to look at the logs generated while we are installing the the software and if there are any warnings generated. We also consulted people who have worked on similar hardware before, to get their insights on what might be the issue and how do we resolve them. Coming to, coming to making sure that the software is ported correctly, we again looked at the warnings generated, and we defined very, very basic unit test cases. To make sure that the porting is complete.",3,True,,1,2
7,7,8,8,"Interviewer: It seems there might have been a connection issue with your response. Could you please elaborate on the challenges you faced in implementing supervised debiasing in your LLM model, and how you addressed them?  

Interviewee: Basically, the main challenge which I had faced is during the memorized memory efficient training process. So as we the model is a very large one we try to you know. quantize this",1,"It seems there might have been a connection issue with your response. Could you please elaborate on the challenges you faced in implementing supervised debiasing in your LLM model, and how you addressed them?","Basically, the main challenge which I had faced is during the memorized memory efficient training process. So as we the model is a very large one we try to you know. quantize this",2,True,,1,1
8,8,9,9,"Interviewer: Data subsampling is a practical approach to managing large datasets when computational resources are limited. It's good to hear that you focus on maintaining data diversity to preserve the model's capabilities. When you're subsampling, how do you ensure that the reduced dataset still captures the necessary diversity and complexity of the original data? Could you also explain how you validate the model's performance on the full dataset after training on the subsampled data to ensure it generalizes well?  

Interviewee: here when we subsample the data, it's something it could be sometimes a human process where we could actually chose to pick and pluck it out of that huge Corpus of data and choose what we have to do with the training or sometimes we can generate generate some scripts where we get to chose. Or scrape the data from the internet from various sources which in a limited manner so that we could actually pass those data itself having the constraints of the data variety. and then later we while performing validation and all we could we could do that in the Unseen data or so that we could Actually get to know how the language model is actually in this case is performing on the answers. I mean, it's performing its facts. I mean getting it producing its facts or information on the data could have never seen.",2,"Data subsampling is a practical approach to managing large datasets when computational resources are limited. It's good to hear that you focus on maintaining data diversity to preserve the model's capabilities. When you're subsampling, how do you ensure that the reduced dataset still captures the necessary diversity and complexity of the original data? Could you also explain how you validate the model's performance on the full dataset after training on the subsampled data to ensure it generalizes well?","here when we subsample the data, it's something it could be sometimes a human process where we could actually chose to pick and pluck it out of that huge Corpus of data and choose what we have to do with the training or sometimes we can generate generate some scripts where we get to chose. Or scrape the data from the internet from various sources which in a limited manner so that we could actually pass those data itself having the constraints of the data variety. and then later we while performing validation and all we could we could do that in the Unseen data or so that we could Actually get to know how the language model is actually in this case is performing on the answers. I mean, it's performing its facts. I mean getting it producing its facts or information on the data could have never seen.",2,True,,2,2
