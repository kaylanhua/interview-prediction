,Unnamed: 0.2,Unnamed: 0.1,Unnamed: 0,Input Text,Label,Question,Response,Prediction,Valid
0,0,0,0,"Interviewer: Your plan to implement smart extraction of phrases and the idea of a dashboard with user-selectable keywords for sentiment analysis is quite innovative. It shows a good understanding of the user's needs and how to present complex data in a user-friendly manner. As you work on these features, how do you intend to measure the effectiveness of the smart phrase extraction, and what metrics or user feedback will you use to evaluate the dashboard's usefulness to the end-users?  

Interviewee: That's a very good question. So for dashboard what I did was I had in my complete if we had some we had a team for business intelligence. They used to work in on creating dashboards and Tableau as well as power bi so I had sit with them and ask them like if I give you just data of reviews, what do you want to see? I had talked with multiple people like what they want to see in a just in the reviews in kpis. Let's say I am developing a KP what they want to see what they wanted to know apart from it. I've been working for a client who was also working on reviews and I got some ideas over there. Like let's say showing them the how the product has evolved over here. So let's say we have a Samsung a series phone. So how the A52 a53 a 54 so how it has evolved over years. What do people are talking about it? When it was launched people were completely about camera. The next iteration people they didn't complain about cameras. They were complaining about something else or let's say, oh they had perfected the phone something like this. So they wanna show a line chart for this and then showing them top keywords to people are using so let's see people are using the word. battery a lot since it is a since it's a very good phone for have that has a amazing battery life of six or seven hours and people are talking about battery. I can just show them a chart or bar graph that showed that battery has a most expensive word similarly to English sentences. They told me something else like display or brightness. So brightens is very low for the phone. So people are talking Negatively about this brightness so we can have a bar chart that shows that brightness is on the top of negative words apart from the other. other complaints that people have and yeah, so that's it.",3,"Your plan to implement smart extraction of phrases and the idea of a dashboard with user-selectable keywords for sentiment analysis is quite innovative. It shows a good understanding of the user's needs and how to present complex data in a user-friendly manner. As you work on these features, how do you intend to measure the effectiveness of the smart phrase extraction, and what metrics or user feedback will you use to evaluate the dashboard's usefulness to the end-users?","That's a very good question. So for dashboard what I did was I had in my complete if we had some we had a team for business intelligence. They used to work in on creating dashboards and Tableau as well as power bi so I had sit with them and ask them like if I give you just data of reviews, what do you want to see? I had talked with multiple people like what they want to see in a just in the reviews in kpis. Let's say I am developing a KP what they want to see what they wanted to know apart from it. I've been working for a client who was also working on reviews and I got some ideas over there. Like let's say showing them the how the product has evolved over here. So let's say we have a Samsung a series phone. So how the A52 a53 a 54 so how it has evolved over years. What do people are talking about it? When it was launched people were completely about camera. The next iteration people they didn't complain about cameras. They were complaining about something else or let's say, oh they had perfected the phone something like this. So they wanna show a line chart for this and then showing them top keywords to people are using so let's see people are using the word. battery a lot since it is a since it's a very good phone for have that has a amazing battery life of six or seven hours and people are talking about battery. I can just show them a chart or bar graph that showed that battery has a most expensive word similarly to English sentences. They told me something else like display or brightness. So brightens is very low for the phone. So people are talking Negatively about this brightness so we can have a bar chart that shows that brightness is on the top of negative words apart from the other. other complaints that people have and yeah, so that's it.",3,True
2,2,2,2,"Interviewer: That's a good approach to content filtering and ensuring user privacy. Using regular expressions to sanitize user input is a common practice. Now, considering your experience with web scraping, can you describe a challenging scenario you encountered while scraping data and how you overcame it? What tools or techniques did you find most effective for handling the challenges in web scraping?  

Interviewee: In web scraping, I have faced multiple challenges. I will describe each one in detail earlier first. We are used to scrape any very few sites like not to sites and scraping them is they are just static Pages like basic HTML if you load the website, you'll get the HTML you can just scrape it from there. But with the with the number of parties that are growing with the chatbot and the company we need to scrape multiple websites and each website comes in different formats. So I'm asked static websites some come as an infinite scroll upside and some commas like toggle ones like you have to use some feature like sidebar or something to scroll between the news articles. So yeah, what have done at this time was I used to study how these website is frame. Like what is the essential HTML structure that is followed and how is the class structure given to each component in the website and if there is any common class, how can you differentiate? Particular content I want from other ones which share this common class like using other properties or Keys available in the HTML tag and some kind of that. I had landed through debugging the site and doing some test runs and in finance clothing have done that with the puppeteer. Failure. We used access for simple scripting which doesn't work. Then we shifted to Puppeteer and when we shifted to Puppeteer when we automated this whole scraping and cleaning and everything process, we Face difficulties because of multi-threading what happened what used to happen was there used to be the scraper used to work on some websites. And if any website is loading slow, it used to exit after 30 seconds and it will not address it again. So then I resist about what are the other ways we can do then I got to know about Puppeteer clusters, which is essentially you open fights in a chrome instance and do balance scraping which increases the speed of scraping and also that also has a feature of sending back to the cube suppose if any site is a or any subside in a website, which we are supposed to scrape. He's a not loading fast. It could be due to network reason or it could be due to the server problem at the website holder itself What we have done was if any exception has happened during the scraping. We used to recent back into the cluster. So this class thank you maintains a list of URLs. It needs to scrape and when something is filled it is added back to the scraping happens until this all these URLs are scraped safely and embedded into the database and that is one difficult. I saw the property clusters and a we used to scrape only static sites, and then I converted into scrolling sites and INF. school websites, these are all I just done with Puppeteer and various extensions it has",4,"That's a good approach to content filtering and ensuring user privacy. Using regular expressions to sanitize user input is a common practice. Now, considering your experience with web scraping, can you describe a challenging scenario you encountered while scraping data and how you overcame it? What tools or techniques did you find most effective for handling the challenges in web scraping?","In web scraping, I have faced multiple challenges. I will describe each one in detail earlier first. We are used to scrape any very few sites like not to sites and scraping them is they are just static Pages like basic HTML if you load the website, you'll get the HTML you can just scrape it from there. But with the with the number of parties that are growing with the chatbot and the company we need to scrape multiple websites and each website comes in different formats. So I'm asked static websites some come as an infinite scroll upside and some commas like toggle ones like you have to use some feature like sidebar or something to scroll between the news articles. So yeah, what have done at this time was I used to study how these website is frame. Like what is the essential HTML structure that is followed and how is the class structure given to each component in the website and if there is any common class, how can you differentiate? Particular content I want from other ones which share this common class like using other properties or Keys available in the HTML tag and some kind of that. I had landed through debugging the site and doing some test runs and in finance clothing have done that with the puppeteer. Failure. We used access for simple scripting which doesn't work. Then we shifted to Puppeteer and when we shifted to Puppeteer when we automated this whole scraping and cleaning and everything process, we Face difficulties because of multi-threading what happened what used to happen was there used to be the scraper used to work on some websites. And if any website is loading slow, it used to exit after 30 seconds and it will not address it again. So then I resist about what are the other ways we can do then I got to know about Puppeteer clusters, which is essentially you open fights in a chrome instance and do balance scraping which increases the speed of scraping and also that also has a feature of sending back to the cube suppose if any site is a or any subside in a website, which we are supposed to scrape. He's a not loading fast. It could be due to network reason or it could be due to the server problem at the website holder itself What we have done was if any exception has happened during the scraping. We used to recent back into the cluster. So this class thank you maintains a list of URLs. It needs to scrape and when something is filled it is added back to the scraping happens until this all these URLs are scraped safely and embedded into the database and that is one difficult. I saw the property clusters and a we used to scrape only static sites, and then I converted into scrolling sites and INF. school websites, these are all I just done with Puppeteer and various extensions it has",3,True
8,8,8,8,"Interviewer: I appreciate the detailed explanation of how you've adapted reinforcement learning algorithms to address overestimation bias and leverage distributional learning. Could you now explain the concept of transfer learning in the context of deep learning and provide an example of how you've applied it in your work? 

Interviewee: A transfer learning is where we use the existing domain knowledge of a given neural network or any machine learning algorithm, right? Whatever parameters we have extracted and we applied to a new domain. So for example domain domain Knowledge from extracting. Parts of speech by but right you can use the same domain knowledge for a different application. for example If you can nowadays last language models essentially predict the next word in the sequence the sequence right? So essentially you can use the same. Weights of these networks and fine tune them for different tasks. The different tasks can be questioned answered. It can be generating essays. It can be. You know, very fine. The authenticity of a particular paragraph, right? So this is what transfer learning essentially is you transfer the knowledge obtained in a given domain and then you apply it in a different domain.",3,I appreciate the detailed explanation of how you've adapted reinforcement learning algorithms to address overestimation bias and leverage distributional learning. Could you now explain the concept of transfer learning in the context of deep learning and provide an example of how you've applied it in your work?,"A transfer learning is where we use the existing domain knowledge of a given neural network or any machine learning algorithm, right? Whatever parameters we have extracted and we applied to a new domain. So for example domain domain Knowledge from extracting. Parts of speech by but right you can use the same domain knowledge for a different application. for example If you can nowadays last language models essentially predict the next word in the sequence the sequence right? So essentially you can use the same. Weights of these networks and fine tune them for different tasks. The different tasks can be questioned answered. It can be generating essays. It can be. You know, very fine. The authenticity of a particular paragraph, right? So this is what transfer learning essentially is you transfer the knowledge obtained in a given domain and then you apply it in a different domain.",3,True
9,9,9,9,"Interviewer: It sounds like you had a robust approach to training the model, especially with the focus on intent recognition and entity extraction. Given the complexity of language models, how did you manage the trade-off between model size and inference speed, and which specific LLM architecture did you find most suitable for this application? Additionally, could you elaborate on the performance metrics you used to evaluate the chatbot's success in understanding and responding to user queries?  

Interviewee: All right. So rasa and LP architecture is known to be high low latency and high performance architecture. That's why it became a choice a good choice for the particular implementation. It is highly used in online settings and hence is known for low latency use cases dealing with low latency use cases. Secondly. It's not actually an llm based model. It is more on the NLP side and neural network model that is not a large language model, but it works really well on the intended entity. Use use cases like named entity text extraction and intent classifiers in terms of metric that we had to incorporate. There was a there were different sorts of metrics that we use to evaluate the success of our model or the performance in general it is it was something like the out of the top 10 recommendation how many recommendations were getting clicks how many recommendation turned out to be useful apart from that? We also had other metric like other metric like taking into account. Whether the recommendations were ordered in the manner that was most useful to the most useful to the users. Therein. We incorporated the normalized discounted cumulative gain metric wherein we could judge the ranking of the recommendation that we were providing.",3,"It sounds like you had a robust approach to training the model, especially with the focus on intent recognition and entity extraction. Given the complexity of language models, how did you manage the trade-off between model size and inference speed, and which specific LLM architecture did you find most suitable for this application? Additionally, could you elaborate on the performance metrics you used to evaluate the chatbot's success in understanding and responding to user queries?","All right. So rasa and LP architecture is known to be high low latency and high performance architecture. That's why it became a choice a good choice for the particular implementation. It is highly used in online settings and hence is known for low latency use cases dealing with low latency use cases. Secondly. It's not actually an llm based model. It is more on the NLP side and neural network model that is not a large language model, but it works really well on the intended entity. Use use cases like named entity text extraction and intent classifiers in terms of metric that we had to incorporate. There was a there were different sorts of metrics that we use to evaluate the success of our model or the performance in general it is it was something like the out of the top 10 recommendation how many recommendations were getting clicks how many recommendation turned out to be useful apart from that? We also had other metric like other metric like taking into account. Whether the recommendations were ordered in the manner that was most useful to the most useful to the users. Therein. We incorporated the normalized discounted cumulative gain metric wherein we could judge the ranking of the recommendation that we were providing.",3,True
14,14,14,14,"Interviewer: Using bcrypt for password hashing and JWT for token-based authentication is a robust approach to security. It's clear you understand the importance of protecting sensitive user information. Now, considering your experience with machine learning and AI models, how do you approach the deployment of these models in a production environment, and what considerations do you take into account to ensure they perform well and are scalable?  

Interviewee: sure, so for the production of machine learning and AI models the most important thing is the backend that I use for that for machine learning the backend Frameworks exist are flask. Or Django and there are other platforms also, so deploying machine learning and a models in production requirement requires careful planning and consideration to ensure they perform well and are scalable model development before deployment. I thoroughly developed and evaluate the machine learning modeling sure. It meets the desired performance metrics in generalization capability capabilities. So after I design the model architecture with them with scalability in mind considering factors such as Computational efficiency memory usage and parallelization capabilities techniques like model parallelism distributed training and batch processing are employed to handle large volumes of data and scale the model to meet the production demands. I configure the deployment infrastructure including Cloud platforms like AWS as your or on premises services to support their deployment and execution of the machine learning model.",3,"Using bcrypt for password hashing and JWT for token-based authentication is a robust approach to security. It's clear you understand the importance of protecting sensitive user information. Now, considering your experience with machine learning and AI models, how do you approach the deployment of these models in a production environment, and what considerations do you take into account to ensure they perform well and are scalable?","sure, so for the production of machine learning and AI models the most important thing is the backend that I use for that for machine learning the backend Frameworks exist are flask. Or Django and there are other platforms also, so deploying machine learning and a models in production requirement requires careful planning and consideration to ensure they perform well and are scalable model development before deployment. I thoroughly developed and evaluate the machine learning modeling sure. It meets the desired performance metrics in generalization capability capabilities. So after I design the model architecture with them with scalability in mind considering factors such as Computational efficiency memory usage and parallelization capabilities techniques like model parallelism distributed training and batch processing are employed to handle large volumes of data and scale the model to meet the production demands. I configure the deployment infrastructure including Cloud platforms like AWS as your or on premises services to support their deployment and execution of the machine learning model.",3,True
16,16,16,16,"Interviewer: The use of DPO to mitigate bias and the feedback mechanism for continuous learning are excellent strategies for ensuring fairness and user satisfaction. Leveraging user feedback to retrain the model is a key component of maintaining model performance and ethical standards. Additionally, using a set of predefined questions for a supervised metric like MCQs is a solid approach to evaluate the model's understanding of the domain. To further explore your technical skills, can you explain how you would use GCP (Google Cloud Platform) in conjunction with these models, particularly for training and deployment in a production environment?  

Interviewee: Right. So gcp is Google Cloud platform and that this platform gives us a lot of models in build on it. So we have these models like Mistral Mistral Moe mixture of experts and lot of other models which are already inbuilt deployed on this so that to make use of these models is pretty easy for us. There is all in a you know, in a overall project. There are a lot of things to you know handle like there is scaling models scaling when to you know, use a lot of gpus when to decrease the GPU. It depends on the number of people are there so that all these kinds of things are already taken care by the gcp so that we can we can try to focus on a lot of other important stuff the Technical Machine learning stuff. So gcp does all these things like we have these vertex AI which has these all inbuilt morals in on it and we try To use them and also on top of it I guess gcp is a partnered with BLM BLM is a library which hosts these model much more better. It has these techniques of fast attention, which can make the model more faster and it can it can improve the inference time. So using gcp what we did is we have these all model techniques and all these optimization all these you know, Load balancing techniques and all is already included in it. And on top of that in the gcp platform. We can also have these databases also we can map to our model. So there are different databases. We are using in Google one is a blob storage where we can store all the CSV PDFs and all in a pocket such that and there are a lot of other storages like we can have logs we can have all these logs if there is an issue. We log all those issues and Trace them whenever we need them. We are using the gcp platform to do these logs and we also also save all the model answers so that we can you know Trace back that day, you know, we have improved the model and and we can compare our previous Model results and our new model results and check if we are improving and there are a lot of other metrics also like observation tools which gcp builds us like observation like, you know the inference Time the you know the load time how many if there is a any downtime for these models and there are a lot of other metrics and observation tools which gcp has using all these things all together and gcp gives us one environment where all these three all these things can come together and gcp also provides these interfaces where we can do a development environment. We can do the testing and we can do the production environment so that If you don't, you know mess up the production we are doing all the development in the development environment. And whenever we have something new we can push it to the testing and that you know, we can do all the testing and the testing environment and when every test case is passes we can push it to the production environment and we can do this in a cicd pipeline where continuous continuous integration and continuous deployment. Is there once we pushed in our get gcp will understand. Hey, you are pushed it's going to go automatically go into the testing environment test it out and automatically fall as clear as it can, you know do all these production all these take things take a lot of time. But if we include gcp in inside the picture these things, you know easily we can equip this thing's much more faster and better and it also provides these kubernetes cluster it also have these Docker registry where we can you know, push all the Dockers. And it also has all these kind of everything it has observability logs the in the kubernetes scaling and storage database is and all these together comes in to the gcp. So I have worked on a lot of things in gcp.",3,"The use of DPO to mitigate bias and the feedback mechanism for continuous learning are excellent strategies for ensuring fairness and user satisfaction. Leveraging user feedback to retrain the model is a key component of maintaining model performance and ethical standards. Additionally, using a set of predefined questions for a supervised metric like MCQs is a solid approach to evaluate the model's understanding of the domain. To further explore your technical skills, can you explain how you would use GCP (Google Cloud Platform) in conjunction with these models, particularly for training and deployment in a production environment?","Right. So gcp is Google Cloud platform and that this platform gives us a lot of models in build on it. So we have these models like Mistral Mistral Moe mixture of experts and lot of other models which are already inbuilt deployed on this so that to make use of these models is pretty easy for us. There is all in a you know, in a overall project. There are a lot of things to you know handle like there is scaling models scaling when to you know, use a lot of gpus when to decrease the GPU. It depends on the number of people are there so that all these kinds of things are already taken care by the gcp so that we can we can try to focus on a lot of other important stuff the Technical Machine learning stuff. So gcp does all these things like we have these vertex AI which has these all inbuilt morals in on it and we try To use them and also on top of it I guess gcp is a partnered with BLM BLM is a library which hosts these model much more better. It has these techniques of fast attention, which can make the model more faster and it can it can improve the inference time. So using gcp what we did is we have these all model techniques and all these optimization all these you know, Load balancing techniques and all is already included in it. And on top of that in the gcp platform. We can also have these databases also we can map to our model. So there are different databases. We are using in Google one is a blob storage where we can store all the CSV PDFs and all in a pocket such that and there are a lot of other storages like we can have logs we can have all these logs if there is an issue. We log all those issues and Trace them whenever we need them. We are using the gcp platform to do these logs and we also also save all the model answers so that we can you know Trace back that day, you know, we have improved the model and and we can compare our previous Model results and our new model results and check if we are improving and there are a lot of other metrics also like observation tools which gcp builds us like observation like, you know the inference Time the you know the load time how many if there is a any downtime for these models and there are a lot of other metrics and observation tools which gcp has using all these things all together and gcp gives us one environment where all these three all these things can come together and gcp also provides these interfaces where we can do a development environment. We can do the testing and we can do the production environment so that If you don't, you know mess up the production we are doing all the development in the development environment. And whenever we have something new we can push it to the testing and that you know, we can do all the testing and the testing environment and when every test case is passes we can push it to the production environment and we can do this in a cicd pipeline where continuous continuous integration and continuous deployment. Is there once we pushed in our get gcp will understand. Hey, you are pushed it's going to go automatically go into the testing environment test it out and automatically fall as clear as it can, you know do all these production all these take things take a lot of time. But if we include gcp in inside the picture these things, you know easily we can equip this thing's much more faster and better and it also provides these kubernetes cluster it also have these Docker registry where we can you know, push all the Dockers. And it also has all these kind of everything it has observability logs the in the kubernetes scaling and storage database is and all these together comes in to the gcp. So I have worked on a lot of things in gcp.",3,True
29,29,29,29,"Interviewer: I appreciate your clear explanation of transfer learning in deep learning. Now, could you describe a situation where you had to use Docker in your workflow and explain the benefits it provided for your project? 

Interviewee: Yeah, sure. So the internship that I did at Owl hill we had to use Docker there because basically in the main project that I had where I needed to optimize a pipeline using tensor RT. We had to work on different machines and different environments. So for training and all you used a server that was there, but for actual deployment we needed to use Edge devices. We used all different devices and just to be sure that everything is compatible and there are no problems because of one system. I had to use Docker there and like create my own environment in the docker with the docker image and Docker container and do everything in that itself. So that is where Docker was really useful and it had not just me but the whole team to understand the project and to work on it as well.",3,"I appreciate your clear explanation of transfer learning in deep learning. Now, could you describe a situation where you had to use Docker in your workflow and explain the benefits it provided for your project?","Yeah, sure. So the internship that I did at Owl hill we had to use Docker there because basically in the main project that I had where I needed to optimize a pipeline using tensor RT. We had to work on different machines and different environments. So for training and all you used a server that was there, but for actual deployment we needed to use Edge devices. We used all different devices and just to be sure that everything is compatible and there are no problems because of one system. I had to use Docker there and like create my own environment in the docker with the docker image and Docker container and do everything in that itself. So that is where Docker was really useful and it had not just me but the whole team to understand the project and to work on it as well.",3,True
40,40,40,40,"Interviewer: It's interesting to hear how you tailored the solution to balance performance and computational cost by using a decision tree and a simple LSTM. Given that you had to iterate on the LSTM's performance, how did you measure the accuracy of the generated code, and what kind of evaluation metrics did you use to ensure that the translation was as close to the original pseudocode as possible?  

Interviewee: But actually the main metrics that we used in this particular part was the blue Matrix. Which kind of tells you how close this sentence is to the other sentence and also how in order it is to the other one. So this metrics was the main thing that we used to figure out what was the output and so on. The Box slight problem which we had which we fully did not tackle was the fact that a particular line in a court can be written in different ways. Right in a Solo code. For example say python. You have set X to 5 set of I can be x equals to find Python and all so can be written as x equals to 5. So making sure that you know, it's an intense so on so we are not able to exactly capture this particular variation and that's something you would like to do in the future.",3,"It's interesting to hear how you tailored the solution to balance performance and computational cost by using a decision tree and a simple LSTM. Given that you had to iterate on the LSTM's performance, how did you measure the accuracy of the generated code, and what kind of evaluation metrics did you use to ensure that the translation was as close to the original pseudocode as possible?","But actually the main metrics that we used in this particular part was the blue Matrix. Which kind of tells you how close this sentence is to the other sentence and also how in order it is to the other one. So this metrics was the main thing that we used to figure out what was the output and so on. The Box slight problem which we had which we fully did not tackle was the fact that a particular line in a court can be written in different ways. Right in a Solo code. For example say python. You have set X to 5 set of I can be x equals to find Python and all so can be written as x equals to 5. So making sure that you know, it's an intense so on so we are not able to exactly capture this particular variation and that's something you would like to do in the future.",3,True
43,43,43,43,"Interviewer: You've highlighted the importance of data quality and model selection for specific contexts, which is crucial for successful machine learning projects. Could you now explain the concept of attention mechanisms in the context of natural language processing and describe a scenario where you found them particularly beneficial? 

Interviewee: Sure. So attention mechanism is a method which ensures that sentences can be used as training data in a parallel form attention mechanisms are useful to ensure that for each single token that is passed. It captures the maximum context of that particular particular token based on all the surrounding tokens. That is it exists with and this ensures that it is strange. The model is trained. By with a higher contextual input and learns to actually learn different tasks on the front of processing attention mechanisms. Make it possible for a model to parallelize the training. In in terms of use cases that I have dealt with I worked on finding the embeddings of keywords and URLs. I feel the attack existing attention mechanism that Burton corporates within its Transformers Transformer based model. It shows that it is able to capture the context of the keywords as well as the URLs that URL data that we are providing by creating meaningful embeddings that we present the keyword in a manner that it's able to perform. Well on the pairwise classification tasks that we fine-tuned it on along with that when it comes to training large language of fine tuning the large language models on huge amount of data like the one we had which which used about 20 million records. It became possible to parallelize these process in a short amount of time within five days. We were able to train a fine tune fine tune a model on a single GPU because of the fast paralyzation process that attention mechanism come with so this is how it has been really useful for attention really useful tool to use useful mechanism as a part of the model architecture.",3,"You've highlighted the importance of data quality and model selection for specific contexts, which is crucial for successful machine learning projects. Could you now explain the concept of attention mechanisms in the context of natural language processing and describe a scenario where you found them particularly beneficial?",Sure. So attention mechanism is a method which ensures that sentences can be used as training data in a parallel form attention mechanisms are useful to ensure that for each single token that is passed. It captures the maximum context of that particular particular token based on all the surrounding tokens. That is it exists with and this ensures that it is strange. The model is trained. By with a higher contextual input and learns to actually learn different tasks on the front of processing attention mechanisms. Make it possible for a model to parallelize the training. In in terms of use cases that I have dealt with I worked on finding the embeddings of keywords and URLs. I feel the attack existing attention mechanism that Burton corporates within its Transformers Transformer based model. It shows that it is able to capture the context of the keywords as well as the URLs that URL data that we are providing by creating meaningful embeddings that we present the keyword in a manner that it's able to perform. Well on the pairwise classification tasks that we fine-tuned it on along with that when it comes to training large language of fine tuning the large language models on huge amount of data like the one we had which which used about 20 million records. It became possible to parallelize these process in a short amount of time within five days. We were able to train a fine tune fine tune a model on a single GPU because of the fast paralyzation process that attention mechanism come with so this is how it has been really useful for attention really useful tool to use useful mechanism as a part of the model architecture.,3,True
48,48,48,48,"Interviewer: Moving on to a different aspect of your projects, you mentioned developing a Deep Learning-based handwritten text recognition system with TensorFlow. Can you discuss the architecture of the model you used for this handwritten text recognition system, specifically how you chose the layers and their configurations?  

Interviewee: Yeah, sure. So handy and text recognition system was basically made choosing tensorflow. So we employed a deep learning architecture tailored to effectively process and recognize handwritten text that will convert the you know, handwritten text to digital text light any person would write anything and its own way and then a model will detect that and converted to digital text. Yeah. So yeah majorly we have to train the model on different styles to we have just trained it for a certain style as of now. So the architecture primarily consisted of CNN's followed by rnns with this CTC laws. Here's an overview of the so I will give you an overview of the architecture. So CNN's like we used five CNN layers three. Two are in layers and one city Slayer. So cnns are well suited for extracting relevant features from image data making them ideal for Henderson text recognition tasks. We utilized you know, multiple convolutional layers where with varying filters sizes and depths to capture hierarchical features at different scale. Max cooling layers were Incorporated to this down sample, the feature Maps then rnns are effective at capturing sequential dependencies in data, which is crucial for recognizing the sequential nature of handwritten text. We employed long short term memory or gated recurrent unit layers to process the sequential output from the CNN layers. These recurrent layers have been encoding the temporal context of the input sequence enabling the model to recognize the sequential structure of handwritten text. Then the CDC losses commonly used in sequence to sequence stars, like speech and handwritten recognition with the alignment between input and output sequences not want to one. It allows the model to align input sequences handed and text images with output sequences that are transcription levels without the need for a line train data. Minecart rating CTC loss on model can learn to decode variable and sequences effectively handling the variability in hand detained Styles and lens. The choice of layer configuration was Guided by empirical experimentation and Architectural best practices. So the overall the architecture are designed to left the strength of cnns and rnns for Effective feature extraction and sequential modeling with careful consideration given player configurations and hypermint reviewing to achieve the best performance on hand in text recognition talks. Yeah.",4,"Moving on to a different aspect of your projects, you mentioned developing a Deep Learning-based handwritten text recognition system with TensorFlow. Can you discuss the architecture of the model you used for this handwritten text recognition system, specifically how you chose the layers and their configurations?","Yeah, sure. So handy and text recognition system was basically made choosing tensorflow. So we employed a deep learning architecture tailored to effectively process and recognize handwritten text that will convert the you know, handwritten text to digital text light any person would write anything and its own way and then a model will detect that and converted to digital text. Yeah. So yeah majorly we have to train the model on different styles to we have just trained it for a certain style as of now. So the architecture primarily consisted of CNN's followed by rnns with this CTC laws. Here's an overview of the so I will give you an overview of the architecture. So CNN's like we used five CNN layers three. Two are in layers and one city Slayer. So cnns are well suited for extracting relevant features from image data making them ideal for Henderson text recognition tasks. We utilized you know, multiple convolutional layers where with varying filters sizes and depths to capture hierarchical features at different scale. Max cooling layers were Incorporated to this down sample, the feature Maps then rnns are effective at capturing sequential dependencies in data, which is crucial for recognizing the sequential nature of handwritten text. We employed long short term memory or gated recurrent unit layers to process the sequential output from the CNN layers. These recurrent layers have been encoding the temporal context of the input sequence enabling the model to recognize the sequential structure of handwritten text. Then the CDC losses commonly used in sequence to sequence stars, like speech and handwritten recognition with the alignment between input and output sequences not want to one. It allows the model to align input sequences handed and text images with output sequences that are transcription levels without the need for a line train data. Minecart rating CTC loss on model can learn to decode variable and sequences effectively handling the variability in hand detained Styles and lens. The choice of layer configuration was Guided by empirical experimentation and Architectural best practices. So the overall the architecture are designed to left the strength of cnns and rnns for Effective feature extraction and sequential modeling with careful consideration given player configurations and hypermint reviewing to achieve the best performance on hand in text recognition talks. Yeah.",4,True
49,49,49,49,"Interviewer: Working with a small dataset certainly helps manage computational complexity. Shifting focus to your Computer Vision projects, you've worked on 3D object localization using triangulation methods. Can you explain the specific triangulation method you employed and why you chose it over other localization techniques?  

Interviewee: Yeah, so basically for a triangulation part but the most important thing is not the triangulation algorithm first of all, we need to like calibrate the camera perfectly then only the triangulation part will work so the first part of the project is to find the intrinsic and extrinsic parameters of the camera intrinsic parameters include the focal length in the X and Y direction of the camera and a center of center of projection of the camera and same and extension parameters include the orientation and location of the camera with respect to the world. To which we need to find with the respect of which we need to find the location of the object in 3D World. So first of all, I used a sense method with the help of checkered chess boards. I calibrated the camera using X Factor and for the extensive matter now extensive parameters I used. PNP me weather and then it was like simple geometrical it was after. Getting the intrinsic and actually parameters. It is very easy to triangulate like I use the stereo camera vision like I the Pixel values in the left corner left camera and I get the pixel values in the right camera of the image and with the help of simple triangulation like through geometrical method. I calculate it. so you say",3,"Working with a small dataset certainly helps manage computational complexity. Shifting focus to your Computer Vision projects, you've worked on 3D object localization using triangulation methods. Can you explain the specific triangulation method you employed and why you chose it over other localization techniques?","Yeah, so basically for a triangulation part but the most important thing is not the triangulation algorithm first of all, we need to like calibrate the camera perfectly then only the triangulation part will work so the first part of the project is to find the intrinsic and extrinsic parameters of the camera intrinsic parameters include the focal length in the X and Y direction of the camera and a center of center of projection of the camera and same and extension parameters include the orientation and location of the camera with respect to the world. To which we need to find with the respect of which we need to find the location of the object in 3D World. So first of all, I used a sense method with the help of checkered chess boards. I calibrated the camera using X Factor and for the extensive matter now extensive parameters I used. PNP me weather and then it was like simple geometrical it was after. Getting the intrinsic and actually parameters. It is very easy to triangulate like I use the stereo camera vision like I the Pixel values in the left corner left camera and I get the pixel values in the right camera of the image and with the help of simple triangulation like through geometrical method. I calculate it. so you say",3,True
60,60,60,60,"Interviewer: Given your experience with optimizing models for deployment, how do you envision deploying this handwritten text recognition system in a real-world scenario, especially in terms of ensuring its performance in real-time or near real-time applications?  

Interviewee: So 11 text recognition was basically Made For Real World scenario, like it involves separate considerations to ensure its performance in real-time or near real-time applications. So before deployment the model May undergo for the optimization to enhance its efficiency and Speed without compromising accuracy. Techniques such as quantization model pruning and compression can reduce the model size and computational requirements. Making it more suitable for deployment on the source contain devices or in real time applications. Then the choice of Hardware played a crucial role in achieving real-time performance depending on the deployment scenarios specialized Hardware sliders, like gpu's tensor Processing Unit tpus or dedicated for inference chips. virtualized to isolate model in France and ensure low latency response times Then optimizing the inference pipeline is essential for minimizing latency and maximizing throughput. Techniques such as batch inference and asynchronous processing and model catching can help expediate inference times allowing the system to process handwritten text inputs in real time or near real time. then like by implementing like these strategies and leveraging optimization Stellar to the deployment environment. The hand didn't extract condition system and Achieve real-time on near real-time performance in real world scenarios, meaning the demand of applications where rapid processing like great recognition is Paramount. Yeah.",4,"Given your experience with optimizing models for deployment, how do you envision deploying this handwritten text recognition system in a real-world scenario, especially in terms of ensuring its performance in real-time or near real-time applications?","So 11 text recognition was basically Made For Real World scenario, like it involves separate considerations to ensure its performance in real-time or near real-time applications. So before deployment the model May undergo for the optimization to enhance its efficiency and Speed without compromising accuracy. Techniques such as quantization model pruning and compression can reduce the model size and computational requirements. Making it more suitable for deployment on the source contain devices or in real time applications. Then the choice of Hardware played a crucial role in achieving real-time performance depending on the deployment scenarios specialized Hardware sliders, like gpu's tensor Processing Unit tpus or dedicated for inference chips. virtualized to isolate model in France and ensure low latency response times Then optimizing the inference pipeline is essential for minimizing latency and maximizing throughput. Techniques such as batch inference and asynchronous processing and model catching can help expediate inference times allowing the system to process handwritten text inputs in real time or near real time. then like by implementing like these strategies and leveraging optimization Stellar to the deployment environment. The hand didn't extract condition system and Achieve real-time on near real-time performance in real world scenarios, meaning the demand of applications where rapid processing like great recognition is Paramount. Yeah.",3,True
63,63,63,63,"Interviewer: Given the reliance on baseline scores and the statistical approach to data analysis, how did you handle scenarios where the observed performance deviated significantly from the baseline, especially in terms of identifying the root cause of such deviations? Additionally, can you discuss any specific instances where your understanding of computer architecture directly influenced the interpretation of performance data, leading to actionable insights or optimizations? 


Interviewee: So, like I said, we had defined baseline scores and the baseline scores were matched with the work which was previously done upon by the same team in the same field. So that helped us in, ensuring that the baseline scores which we are currently getting are coherent with the, with what we were expect expecting. Now coming to significant deviations, during my sweeps, we, tried to debug. We also generated a lot of log files And the way to go about it was to analyze the log files and make sure that, there are no warnings generated or any line which was, skipped in the run script. Which might be the cause of this. Also, coming back to how the knowledge of computer architecture was helped in under in debugging this particular any particular instance. So like I said, we observed that l 3 cache was giving us negligible sensitivity to the workload, large language model workload. Meaning, if I tried if I tried to halve the l three access to my, code, code for print, it did not impact those scores, which we were expecting. But then on further analysis, it was noticed that all the l three accesses were l three misses. And there was, the utilization of DDR, was a high was a lot. It was very high, which, implied that all, my l three, hit, l three accesses are all l three messes. And hence, the large language model, workload depends heavily on memory accesses directly from the RAM. Hence, that helped us in, knowing that, okay. What I'm looking at is coherent with what we are supposed to observe.",4,"Given the reliance on baseline scores and the statistical approach to data analysis, how did you handle scenarios where the observed performance deviated significantly from the baseline, especially in terms of identifying the root cause of such deviations? Additionally, can you discuss any specific instances where your understanding of computer architecture directly influenced the interpretation of performance data, leading to actionable insights or optimizations?","So, like I said, we had defined baseline scores and the baseline scores were matched with the work which was previously done upon by the same team in the same field. So that helped us in, ensuring that the baseline scores which we are currently getting are coherent with the, with what we were expect expecting. Now coming to significant deviations, during my sweeps, we, tried to debug. We also generated a lot of log files And the way to go about it was to analyze the log files and make sure that, there are no warnings generated or any line which was, skipped in the run script. Which might be the cause of this. Also, coming back to how the knowledge of computer architecture was helped in under in debugging this particular any particular instance. So like I said, we observed that l 3 cache was giving us negligible sensitivity to the workload, large language model workload. Meaning, if I tried if I tried to halve the l three access to my, code, code for print, it did not impact those scores, which we were expecting. But then on further analysis, it was noticed that all the l three accesses were l three misses. And there was, the utilization of DDR, was a high was a lot. It was very high, which, implied that all, my l three, hit, l three accesses are all l three messes. And hence, the large language model, workload depends heavily on memory accesses directly from the RAM. Hence, that helped us in, knowing that, okay. What I'm looking at is coherent with what we are supposed to observe.",4,True
68,68,68,68,"Interviewer: It seems there might have been a slight interruption. You were about to explain how you addressed a specific challenge in training the CNN for pose detection. Could you please continue with that, particularly focusing on how you tackled overfitting and ensured the model's robustness to varying conditions?  

Interviewee: Sure. So the first task was to gather a data set which was capable enough to train a convolutional neural network because a convolutional neural networks requires a very large data set and to perform a good accuracy and to detect the complex patterns. So one of the challenges that I encountered while training the CNN for post estimation and other was over fitting especially when dealing with this large amount of data set and this Huge complex model. So one of the technique that I used was Dropout clear. I Incorporated Dropout layers into the CNN architecture during training Dropout randomly deactivate a certain percentage of neurons in each training iteration focusing. In the network to learn redundant representations and preventing it from relying to heavily on any particular set of features.",3,"It seems there might have been a slight interruption. You were about to explain how you addressed a specific challenge in training the CNN for pose detection. Could you please continue with that, particularly focusing on how you tackled overfitting and ensured the model's robustness to varying conditions?",Sure. So the first task was to gather a data set which was capable enough to train a convolutional neural network because a convolutional neural networks requires a very large data set and to perform a good accuracy and to detect the complex patterns. So one of the challenges that I encountered while training the CNN for post estimation and other was over fitting especially when dealing with this large amount of data set and this Huge complex model. So one of the technique that I used was Dropout clear. I Incorporated Dropout layers into the CNN architecture during training Dropout randomly deactivate a certain percentage of neurons in each training iteration focusing. In the network to learn redundant representations and preventing it from relying to heavily on any particular set of features.,3,True
76,76,76,76,"Interviewer: That's a comprehensive overview of your approach. Given the focus on experimentation and model comparison, how did you design the system to ensure reproducibility and scalability, especially considering the potential variations in data sources and model architectures?  


Interviewee: Okay. So the thing is, we don't have a very large team which uses the inference. So we don't have to deal with scalability at this particular point in time. But for experimentation, like I said, we were tracking all sort of hyperparameters, and, all the things that we do when, all the things that we supply to the ML pipeline. Using MLflow. So those were also logged as an artifact to key chain corresponding experiment. And regarding the data sources, so we have one centralized data source where, we store all the orthomosaics of the solar power plants. And we use the core API. Basically, our core API is an internal API that we have to fetch the corresponding ortho mosaics for any particular projects. We use that, to fetch the and get their corresponding, q seed or labeled labels, you could say, bounding boxes, and use those to prepare their dataset. So, and for preprocessing that, ortho, we turn it to grayscale and use a Volley filter. So Volley filter is that you use to make the contrast constant across the whole art.",3,"That's a comprehensive overview of your approach. Given the focus on experimentation and model comparison, how did you design the system to ensure reproducibility and scalability, especially considering the potential variations in data sources and model architectures?","Okay. So the thing is, we don't have a very large team which uses the inference. So we don't have to deal with scalability at this particular point in time. But for experimentation, like I said, we were tracking all sort of hyperparameters, and, all the things that we do when, all the things that we supply to the ML pipeline. Using MLflow. So those were also logged as an artifact to key chain corresponding experiment. And regarding the data sources, so we have one centralized data source where, we store all the orthomosaics of the solar power plants. And we use the core API. Basically, our core API is an internal API that we have to fetch the corresponding ortho mosaics for any particular projects. We use that, to fetch the and get their corresponding, q seed or labeled labels, you could say, bounding boxes, and use those to prepare their dataset. So, and for preprocessing that, ortho, we turn it to grayscale and use a Volley filter. So Volley filter is that you use to make the contrast constant across the whole art.",3,True
77,77,77,77,"Interviewer: Using Truffle for testing and MythX for security checks is a solid approach. Can you elaborate on how you integrated these tools into your development workflow, and perhaps share how you addressed any vulnerabilities that were identified during the static and dynamic analysis?  

Interviewee: So basically when we did a static analysis with mythics, so first radical is first, I need to mention that I use my fix and also I use filter so we found a certain vulnerabilities like so that are overflow underflow those kind of vulnerabilities were fixed using a new wind. So even using that you and some Library so we also have the math like math library of the 30 open simply Library. So using that we actually prevented the Overflow and underflow kind of situation for interiors. So also we did a manual identification of the potential security issues paying so we did this by checking the arithmetic operations and external calls of the particular functions. If there are function calls, then we are actually checking whether that function call could lead to other kind of exposure of the security breach. So those those things come from the statically in case of dynamic analysis. I I wrote the test tips so already I mentioned that I used mocha and chai as a description. So those tips were kind enough to judge the security or else the What to say also the use case checks that were done to to ensure that the court or the solidity smart contract at the time writing should ensure the proper if you execution. So those were part of some things also Dynamic analysis. I used in So at the first step of mine. I didn't I didn't directly implemented my smart contract to a blockchain or even not even taste Network. I implemented it. I did intimidation in the local blockchain. That is the ganache so ganache tool I used for the local deployment. And once I am very much assured that there won't be a much of loss then only I moved to the move to towards. So this where two men things that I did in in the broad line of static analysis and dynamic analysis. Yeah.",3,"Using Truffle for testing and MythX for security checks is a solid approach. Can you elaborate on how you integrated these tools into your development workflow, and perhaps share how you addressed any vulnerabilities that were identified during the static and dynamic analysis?","So basically when we did a static analysis with mythics, so first radical is first, I need to mention that I use my fix and also I use filter so we found a certain vulnerabilities like so that are overflow underflow those kind of vulnerabilities were fixed using a new wind. So even using that you and some Library so we also have the math like math library of the 30 open simply Library. So using that we actually prevented the Overflow and underflow kind of situation for interiors. So also we did a manual identification of the potential security issues paying so we did this by checking the arithmetic operations and external calls of the particular functions. If there are function calls, then we are actually checking whether that function call could lead to other kind of exposure of the security breach. So those those things come from the statically in case of dynamic analysis. I I wrote the test tips so already I mentioned that I used mocha and chai as a description. So those tips were kind enough to judge the security or else the What to say also the use case checks that were done to to ensure that the court or the solidity smart contract at the time writing should ensure the proper if you execution. So those were part of some things also Dynamic analysis. I used in So at the first step of mine. I didn't I didn't directly implemented my smart contract to a blockchain or even not even taste Network. I implemented it. I did intimidation in the local blockchain. That is the ganache so ganache tool I used for the local deployment. And once I am very much assured that there won't be a much of loss then only I moved to the move to towards. So this where two men things that I did in in the broad line of static analysis and dynamic analysis. Yeah.",2,True
84,84,84,84,"Interviewer: Your approach to anomaly detection using ARIMA and a three-sigma threshold is a classic yet effective method, especially given the constraints of data availability and the need to minimize false alarms. For the FDC data, clustering is a powerful technique for identifying patterns and anomalies. Could you elaborate on the specific clustering algorithms you found most effective for FDC data, and how you validated the clusters to ensure they accurately represented normal and anomalous behavior?  

Interviewee: Okay, so that is a very interesting question. And actually that is what made the task the most challenging for us now initially. Well, now the thing is one of the challenges of using the time series data is if you try to like initially I would use the hierarchical clustering method to Cluster based on the time series data, but that would take up too much amount of time. And because the production is a very fast process so it became like necessary that we can do it efficiently and like in a very short period of time so that's why what I did is at first I will use the PCA the principal component analysis method to do the dimensionality reduction on the time series data and bring it down to a 2d space. And now that we have it in the 2D space then I will use the clustering method. So here initially I started with the came in clustering method, but the problem with the came in clustering is it is like a distance Place method. So all the Clusters that will be formed is will always be in a circle. but in this particular case when you project it to the lower Dimension does not always mean that that they should be in the circular form like all the Clusters so it became more important to to take into consideration the the density as well as like the distribution of each of the like points. And so as far as the distribution based clustering is concerned so I use the gaussian mixture model for the clustering and yeah, so and then like what I will do is like for example during the training period I will have the Clusters and then I will note down like which pattern will belong to which clusters like by that I mean that even though the clustering is done on the two-dimensional space, but the pattern will then be Taken for the time series data and to take the pattern from the time series data. I will use the Battery Center method. So like Barry Center is similar to the center, but Center is for the two-dimensional or the three-dimensional data, but the Battery Center will be the center of the time series data now, I will Store The Battery Center in a library kind of thing. And that is the training part. And now then I will again do the clustering on the Testing period of the monitoring period and now I will try to see now after the Clusters has been done and the patterns are have been generated based on the time series data. I will try to check with the library like whether this patterns are already present inside the library or they are the new patterns now if they are already present inside the library and like because we already know that all the patterns inside the library is supposed to be from the budo if it's because no alarm was raised. So in that way we will try to like there were two ways either inside the Library if there is some pattern and the user have already informed us that this pattern is a bad pattern and if we get the same pattern in the monitoring period then we can directly say that these are bad patterns, but if a new pattern will come up which is not there inside our library and we don't know anything about it then all so we will try to inform the user about the new Pattern from the clustering that came up and like that will be kind of a Softail on to the user and they will check and verify and will fit back to us whether that pattern is a actual bad pattern or not. But because like clustering is a unsupervised method. So the important task here is to detect new patterns, and and we did that quite successful. Yeah.",4,"Your approach to anomaly detection using ARIMA and a three-sigma threshold is a classic yet effective method, especially given the constraints of data availability and the need to minimize false alarms. For the FDC data, clustering is a powerful technique for identifying patterns and anomalies. Could you elaborate on the specific clustering algorithms you found most effective for FDC data, and how you validated the clusters to ensure they accurately represented normal and anomalous behavior?","Okay, so that is a very interesting question. And actually that is what made the task the most challenging for us now initially. Well, now the thing is one of the challenges of using the time series data is if you try to like initially I would use the hierarchical clustering method to Cluster based on the time series data, but that would take up too much amount of time. And because the production is a very fast process so it became like necessary that we can do it efficiently and like in a very short period of time so that's why what I did is at first I will use the PCA the principal component analysis method to do the dimensionality reduction on the time series data and bring it down to a 2d space. And now that we have it in the 2D space then I will use the clustering method. So here initially I started with the came in clustering method, but the problem with the came in clustering is it is like a distance Place method. So all the Clusters that will be formed is will always be in a circle. but in this particular case when you project it to the lower Dimension does not always mean that that they should be in the circular form like all the Clusters so it became more important to to take into consideration the the density as well as like the distribution of each of the like points. And so as far as the distribution based clustering is concerned so I use the gaussian mixture model for the clustering and yeah, so and then like what I will do is like for example during the training period I will have the Clusters and then I will note down like which pattern will belong to which clusters like by that I mean that even though the clustering is done on the two-dimensional space, but the pattern will then be Taken for the time series data and to take the pattern from the time series data. I will use the Battery Center method. So like Barry Center is similar to the center, but Center is for the two-dimensional or the three-dimensional data, but the Battery Center will be the center of the time series data now, I will Store The Battery Center in a library kind of thing. And that is the training part. And now then I will again do the clustering on the Testing period of the monitoring period and now I will try to see now after the Clusters has been done and the patterns are have been generated based on the time series data. I will try to check with the library like whether this patterns are already present inside the library or they are the new patterns now if they are already present inside the library and like because we already know that all the patterns inside the library is supposed to be from the budo if it's because no alarm was raised. So in that way we will try to like there were two ways either inside the Library if there is some pattern and the user have already informed us that this pattern is a bad pattern and if we get the same pattern in the monitoring period then we can directly say that these are bad patterns, but if a new pattern will come up which is not there inside our library and we don't know anything about it then all so we will try to inform the user about the new Pattern from the clustering that came up and like that will be kind of a Softail on to the user and they will check and verify and will fit back to us whether that pattern is a actual bad pattern or not. But because like clustering is a unsupervised method. So the important task here is to detect new patterns, and and we did that quite successful. Yeah.",3,True
87,87,87,87,"Interviewer: It sounds like you've implemented a straightforward tokenization strategy for your project, leveraging both default and custom tokenizers to handle the specific needs of your data. Given this experience, could you explain the concept of transfer learning in the context of natural language processing and how it might be applied to your pseudocode to code generation task? 

Interviewee: short transfer learning is a very common process not just when LP but also for A computer vision as well, but transfer learning has gained its popularity in llms right now. So the point of transfer learning is when you have a particular deep learning model you train it on lots of data set the right. For example, let's take the ULM fit paper, which I know very summary because briefly because that was very pivotal. So this particular paper just took a large language model to some extent Transformers and training it on Wikipedia, right? So I just change the prediction next for prediction and sometimes even fill in the blacks prediction, right? So the idea is just to train the model on a large Corpus of data set. What this does is it generalize a model on a data set and for NLP Publishers, at least make sure that the model. Knows a lot of things about the language like English. It was a few string together sentences. And so give them this when you take this particular model and then you find your network to a particular task you bringing the experience in the weights of the model from previous huge task and then making sure it's focused on us. For example, we take the first one that is what a trained on Wikipedia data set. He was data set for next what prediction and we take these weights and we change the head of the classification of the transformer change it to something else. Say maybe a sentiment prediction very simple. This is subset of this because that's a entire language model that's predicting next to it. But this one. It just uses a few sentences and so on. To protect what the sentiment is now where this previous knowledge. It becomes very easy and the training converges very easy simpler in this particular product. So what we usually do is we freeze a few weeks not all the layers maybe Top Gear layers and then we find unit and this basically uses previous information and then fine tunes and better. So this makes the train faster and the train converges better.",4,"It sounds like you've implemented a straightforward tokenization strategy for your project, leveraging both default and custom tokenizers to handle the specific needs of your data. Given this experience, could you explain the concept of transfer learning in the context of natural language processing and how it might be applied to your pseudocode to code generation task?","short transfer learning is a very common process not just when LP but also for A computer vision as well, but transfer learning has gained its popularity in llms right now. So the point of transfer learning is when you have a particular deep learning model you train it on lots of data set the right. For example, let's take the ULM fit paper, which I know very summary because briefly because that was very pivotal. So this particular paper just took a large language model to some extent Transformers and training it on Wikipedia, right? So I just change the prediction next for prediction and sometimes even fill in the blacks prediction, right? So the idea is just to train the model on a large Corpus of data set. What this does is it generalize a model on a data set and for NLP Publishers, at least make sure that the model. Knows a lot of things about the language like English. It was a few string together sentences. And so give them this when you take this particular model and then you find your network to a particular task you bringing the experience in the weights of the model from previous huge task and then making sure it's focused on us. For example, we take the first one that is what a trained on Wikipedia data set. He was data set for next what prediction and we take these weights and we change the head of the classification of the transformer change it to something else. Say maybe a sentiment prediction very simple. This is subset of this because that's a entire language model that's predicting next to it. But this one. It just uses a few sentences and so on. To protect what the sentiment is now where this previous knowledge. It becomes very easy and the training converges very easy simpler in this particular product. So what we usually do is we freeze a few weeks not all the layers maybe Top Gear layers and then we find unit and this basically uses previous information and then fine tunes and better. So this makes the train faster and the train converges better.",3,True
91,91,91,91,"Interviewer: It's clear that you value the flexibility and control that PyTorch offers, especially with the additional libraries like fastai and PyTorch Lightning for faster experimentation. Given your experience with these libraries, how do you approach model deployment? Do you continue to use PyTorch in production, or do you have a different strategy for deploying your machine learning models?  

Interviewee: Yes, I continue to use pytorch and production. Recently, I've been experimenting with the converting them combining them using Onyx or some other formats, but generally I use pytorch. So what I do is I will pop a Docker container with a fast API. Framework in it and I end up assuring the endpoints and once I'm done with that, I just deploy the docker container. Well, eventually I figured out that there was a lot of problem especially when we go with large models that's probably less and so on so I had ended up implementing messaging cues and sometimes in my startup because the number of users when freezing and we had to make sure that it was always available. I was expected with kubernetes at this point.",4,"It's clear that you value the flexibility and control that PyTorch offers, especially with the additional libraries like fastai and PyTorch Lightning for faster experimentation. Given your experience with these libraries, how do you approach model deployment? Do you continue to use PyTorch in production, or do you have a different strategy for deploying your machine learning models?","Yes, I continue to use pytorch and production. Recently, I've been experimenting with the converting them combining them using Onyx or some other formats, but generally I use pytorch. So what I do is I will pop a Docker container with a fast API. Framework in it and I end up assuring the endpoints and once I'm done with that, I just deploy the docker container. Well, eventually I figured out that there was a lot of problem especially when we go with large models that's probably less and so on so I had ended up implementing messaging cues and sometimes in my startup because the number of users when freezing and we had to make sure that it was always available. I was expected with kubernetes at this point.",4,True
95,95,95,95,"Interviewer: What potential trade-offs or compromises might you need to make during the implementation of this real-time object detection system? 


Interviewee: So first, I'll I'll work with a very small dummy model for object detection using a same model to debug. That trade off could be, like, if I use very good model version like YOLO v YOLO v 10, big large version, that accuracy will be very high. But the model will be of too large size and using it on phones or some embedded device would be very difficult, and the latency will be very high. Such hardware support might not be available. So with little decrease in accuracy, we can achieve good, speed and, still have a manageable still have a except sufficient accuracies with that. Like, instead of using more of heavy models with high accuracy, I will try to use some, lightweight models like small version instead of using large version of YOLO. And try to check if the those models are giving sufficient accurate for the production environment. And then I'll compromise I will compromise, high accuracy with a slightly low accuracy, but for faster speed.",3,What potential trade-offs or compromises might you need to make during the implementation of this real-time object detection system?,"So first, I'll I'll work with a very small dummy model for object detection using a same model to debug. That trade off could be, like, if I use very good model version like YOLO v YOLO v 10, big large version, that accuracy will be very high. But the model will be of too large size and using it on phones or some embedded device would be very difficult, and the latency will be very high. Such hardware support might not be available. So with little decrease in accuracy, we can achieve good, speed and, still have a manageable still have a except sufficient accuracies with that. Like, instead of using more of heavy models with high accuracy, I will try to use some, lightweight models like small version instead of using large version of YOLO. And try to check if the those models are giving sufficient accurate for the production environment. And then I'll compromise I will compromise, high accuracy with a slightly low accuracy, but for faster speed.",4,True
98,98,98,98,"Interviewer: Great! I see from your resume that you've worked extensively with Large Language Models (LLMs) and have even developed a pipeline to convert topics into sales decks. Could you walk me through the key components of this pipeline and how you ensured a 98.9% efficiency boost?  

Interviewee: Oh, yes. So this was started as an initiative back then we didn't even know if this would work. So problem was like I have I have many friends in the sales team. So they always come to me like hey, we have to spend two to three hours in preparing sales deck. So that's when I tried to figure something out and then I tried to pitch it to my co-founder Asif. Like if there's anything we can try and that's when it tried to go with the GPT model. Of the open AI so what happened was so we went back then we didn't even know if they should work. So. We integrated like we started giving random Pro like small prompts direct prompts to the deputy model back. Then we were using the free version gt3.5. So we started with random prompts sword from which were more direct and we tried to get some output using the open a API key. So it back then the problem was like we didn't get any good results like which we can directly include in the sales deck. so then we decided to go with the one sort of few sort approaches like giving some huge cases that are already solved solved in quantify and like using to generate responses based on these examples. So that's when we try to use a few sort. But again like we were using just think like we were putting everything in just one prompt like we were stacking everything only in one prop. So that's when we got the like we first issue up the token limit output like this is certain limitation on the total number of tokens that you can pass through the model. So we test that challenge and in order to take all that we decided to like there are separate sections. There are data data data sets like what are the data? Like what is the current state of the project? What is the expected future instead like design instead of the project but the technical expertise required to complete the project like and what would be the data required completely to complete the project who are the stakeholders like we have this different sections which which are to be populated in the cell state. So right so we what we did was we tried to use a separate prompts for each section like for data section. We had a specific from for skill section we had specific from So similarly we try to do it but again like while trying to do it we faced with the problem of the like since we were using the free version of the openai API, so there's what there's a quota limit and that got exited. So then we had to buy the premium version of the openai API and that's when we got access to the GPD form model. So right now what you're doing is we are integrating GPT form model and all so with the help of apps script like we are automating the task updating the sales deck. So previously the task. Which was manually done like it was taking around 3 hours. So we are able to do it or wrap it under one minute one to two minutes. So I believe like that's a 98% efficiency goes so that's how we We we are able we were able to help ourselves team. Yeah.",3,Great! I see from your resume that you've worked extensively with Large Language Models (LLMs) and have even developed a pipeline to convert topics into sales decks. Could you walk me through the key components of this pipeline and how you ensured a 98.9% efficiency boost?,"Oh, yes. So this was started as an initiative back then we didn't even know if this would work. So problem was like I have I have many friends in the sales team. So they always come to me like hey, we have to spend two to three hours in preparing sales deck. So that's when I tried to figure something out and then I tried to pitch it to my co-founder Asif. Like if there's anything we can try and that's when it tried to go with the GPT model. Of the open AI so what happened was so we went back then we didn't even know if they should work. So. We integrated like we started giving random Pro like small prompts direct prompts to the deputy model back. Then we were using the free version gt3.5. So we started with random prompts sword from which were more direct and we tried to get some output using the open a API key. So it back then the problem was like we didn't get any good results like which we can directly include in the sales deck. so then we decided to go with the one sort of few sort approaches like giving some huge cases that are already solved solved in quantify and like using to generate responses based on these examples. So that's when we try to use a few sort. But again like we were using just think like we were putting everything in just one prompt like we were stacking everything only in one prop. So that's when we got the like we first issue up the token limit output like this is certain limitation on the total number of tokens that you can pass through the model. So we test that challenge and in order to take all that we decided to like there are separate sections. There are data data data sets like what are the data? Like what is the current state of the project? What is the expected future instead like design instead of the project but the technical expertise required to complete the project like and what would be the data required completely to complete the project who are the stakeholders like we have this different sections which which are to be populated in the cell state. So right so we what we did was we tried to use a separate prompts for each section like for data section. We had a specific from for skill section we had specific from So similarly we try to do it but again like while trying to do it we faced with the problem of the like since we were using the free version of the openai API, so there's what there's a quota limit and that got exited. So then we had to buy the premium version of the openai API and that's when we got access to the GPD form model. So right now what you're doing is we are integrating GPT form model and all so with the help of apps script like we are automating the task updating the sales deck. So previously the task. Which was manually done like it was taking around 3 hours. So we are able to do it or wrap it under one minute one to two minutes. So I believe like that's a 98% efficiency goes so that's how we We we are able we were able to help ourselves team. Yeah.",3,True
103,103,103,103,"Interviewer: That's a comprehensive explanation, thank you. Now, considering your experience with NLP and the development of a chatbot for real estate agents, could you walk me through how you approached the challenge of training and fine-tuning the LLM model for this specific domain, and what kind of performance metrics you found most valuable in evaluating the chatbot's effectiveness?  

Interviewee: Right. So in this scenario, we incorporated a rasa NLP based model, which was an intent and entity based model. It had a two-part solution for fine tuning Incorporated to or two sections. The first was to train the model for the intent recognition when we had just a handful of intents that is to identify. Whether a query was a listing based query whether the query required solution for real estate agents or whether open houses or mortgage related questions. These were the questions that we needed intent recognition for so it became fairly easy to provide examples of intense belonging to each of these in each of these classes each of these labels and along with that for for the entities that we wanted to extract. We had already composed already had an extensive database wherein we recorded all sorts of Different entities that were relevant in different queries like locations language is that language is that real estate agents spoke or specific? Zip codes are neighborhoods where the listings were located in the kind of community that the particular address was a part of Etc. So all of these different parts different entities became the training data for us and Using these data we find the rasa and you along with the rasa and will be libraries itself. We utilize these this data to find you in the model and just better support our use case which was the compass spot. So the bot operated in this manner wherein it could identify messages on the agency the post that agents made in a LinkedIn Lake Network, which was called Asian feed and it answered or recommended solutions to their queries as comments another section. Another front where the compost bought was very handy was on in real estate agent agent base slack groups wherein it could identify the entrance of the messages of the queries that uses posted and then answer these queries with a bunch of recommendations by identifying the intense of these queries and then providing some a list of top 10 recommendations for each of these queries.",4,"That's a comprehensive explanation, thank you. Now, considering your experience with NLP and the development of a chatbot for real estate agents, could you walk me through how you approached the challenge of training and fine-tuning the LLM model for this specific domain, and what kind of performance metrics you found most valuable in evaluating the chatbot's effectiveness?","Right. So in this scenario, we incorporated a rasa NLP based model, which was an intent and entity based model. It had a two-part solution for fine tuning Incorporated to or two sections. The first was to train the model for the intent recognition when we had just a handful of intents that is to identify. Whether a query was a listing based query whether the query required solution for real estate agents or whether open houses or mortgage related questions. These were the questions that we needed intent recognition for so it became fairly easy to provide examples of intense belonging to each of these in each of these classes each of these labels and along with that for for the entities that we wanted to extract. We had already composed already had an extensive database wherein we recorded all sorts of Different entities that were relevant in different queries like locations language is that language is that real estate agents spoke or specific? Zip codes are neighborhoods where the listings were located in the kind of community that the particular address was a part of Etc. So all of these different parts different entities became the training data for us and Using these data we find the rasa and you along with the rasa and will be libraries itself. We utilize these this data to find you in the model and just better support our use case which was the compass spot. So the bot operated in this manner wherein it could identify messages on the agency the post that agents made in a LinkedIn Lake Network, which was called Asian feed and it answered or recommended solutions to their queries as comments another section. Another front where the compost bought was very handy was on in real estate agent agent base slack groups wherein it could identify the entrance of the messages of the queries that uses posted and then answer these queries with a bunch of recommendations by identifying the intense of these queries and then providing some a list of top 10 recommendations for each of these queries.",3,True
104,104,104,104,"Interviewer: That's quite an impressive journey from experimenting with GPT-3.5 to leveraging GPT-3 and AppScript for automation. It's clear you've tackled some significant challenges. Given your experience with GPT-3, can you discuss any specific techniques or fine-tuning you applied to the model to ensure the generated content was not only coherent but also tailored to the sales context?  

Interviewee: Oh, yes. So this is small clarification here. So we we didn't we were not using the GPD 3 we got we went with the GPT 4 model. Who is the latest model of openai? So yes, we did a very like the prompt engineering part was the most difficult part. I would say like we used to find tune the prompts again and again, so that that was on up the important expect of the project like we try to tailor it as for our use case like also like while giving the example prompt so we met here like the model should not have you seen it so that's so we like if there are any sections like let's say Coast saving or Revenue section. So those includes numbers and in order to make ensure that the model doesn't hallucinate so we try to give it step by step loading. We use Chain of Thought prompting all so you you several different from techniques in order to get a tailored response from the model and popularity now sells deck.",3,"That's quite an impressive journey from experimenting with GPT-3.5 to leveraging GPT-3 and AppScript for automation. It's clear you've tackled some significant challenges. Given your experience with GPT-3, can you discuss any specific techniques or fine-tuning you applied to the model to ensure the generated content was not only coherent but also tailored to the sales context?","Oh, yes. So this is small clarification here. So we we didn't we were not using the GPD 3 we got we went with the GPT 4 model. Who is the latest model of openai? So yes, we did a very like the prompt engineering part was the most difficult part. I would say like we used to find tune the prompts again and again, so that that was on up the important expect of the project like we try to tailor it as for our use case like also like while giving the example prompt so we met here like the model should not have you seen it so that's so we like if there are any sections like let's say Coast saving or Revenue section. So those includes numbers and in order to make ensure that the model doesn't hallucinate so we try to give it step by step loading. We use Chain of Thought prompting all so you you several different from techniques in order to get a tailored response from the model and popularity now sells deck.",2,True
121,121,121,121,"Interviewer: Your focus on regulatory compliance and the simplicity of explaining the model's behavior is commendable. Shifting gears to the technical skills listed on your resume, let's delve into Python and its applications in AI. Can you explain how you've utilized Python, specifically in conjunction with libraries like NumPy and Pandas, to optimize data handling and manipulation in your AI projects? What are some key considerations for efficient data processing in Python, especially when dealing with large financial datasets?  


Interviewee: Um, so one of the challenges that um kind of and occur when you were working with python is a memory allocation. So when you take Panda dataframes and you try to concatenate them or do any operations the memory footprint tends to explode because python is not a memory managed. properly measure memory managed language So effectively, yes, you can use numpy and pandas but often times when related to manipulating data on large scales. We would actually employ C plus plus and call it underneath. So I would write C plus plus programs which would manipulate the specific data entries where I needed to do some kind of transformation or concatenation. So in summary, yes, we employed the use of numpy and pandas but that is only for data manipulation where the memory footprint was. Okay, when memory footprint gets really really large we use C plus plus with openmp and we are able to load data in Via something called protobuf.",3,"Your focus on regulatory compliance and the simplicity of explaining the model's behavior is commendable. Shifting gears to the technical skills listed on your resume, let's delve into Python and its applications in AI. Can you explain how you've utilized Python, specifically in conjunction with libraries like NumPy and Pandas, to optimize data handling and manipulation in your AI projects? What are some key considerations for efficient data processing in Python, especially when dealing with large financial datasets?","Um, so one of the challenges that um kind of and occur when you were working with python is a memory allocation. So when you take Panda dataframes and you try to concatenate them or do any operations the memory footprint tends to explode because python is not a memory managed. properly measure memory managed language So effectively, yes, you can use numpy and pandas but often times when related to manipulating data on large scales. We would actually employ C plus plus and call it underneath. So I would write C plus plus programs which would manipulate the specific data entries where I needed to do some kind of transformation or concatenation. So in summary, yes, we employed the use of numpy and pandas but that is only for data manipulation where the memory footprint was. Okay, when memory footprint gets really really large we use C plus plus with openmp and we are able to load data in Via something called protobuf.",3,True
122,122,122,122,"Interviewer: It's interesting to hear about the shift in focus from purely technical aspects to a more holistic view of project success. How has this perspective influenced your approach to software development, specifically in terms of code optimization and feature prioritization?  

Interviewee: Yeah code optimization is very important because when you have been working, you know particular code base, you are not the only one who is working on the code base. There are several other factors and so in most of the companies what happens is that the person who writes a code, he just sits it into build. I mean, he gives it for building the building takes around 2015 or 20 minutes and he or she spends the rest of the time leaving the code of the others the other peers that they get so this is very important because it helps to optimize the court that rather have written and also ensure that no pretty much a code is being pushed into the repo so that the future developers were going to come into the organization and work did not face any shoes and one of the one of the example I like to take for example, super holder while building this copilot. One of the issues that we fixed is the database throughput the database throughput is very limited if you see mongodb and other databases like we have used their Azure Cosmos DBS, however, the truth is very limited as a result of which. We just cannot I mean we just cannot append a lot of data to the database or faced a lot of data. We database due to the restricted throughput so what we did integrate we have to integrate the Apache of career. So as to handle this sort of issues. The throughput of Apache of Christmas greater than the databases that we generally use. So we use that and after that after handling the I mean high frequency things then what we did we did a bulk entry into the database. So that is how we approached a problem and did some optimization because whenever we are building the project individually this doesn't have a lot of users into our projects look kind of projects. So lot of the issues doesn't get but whenever we are working for a company or organization, we see that a lot of new shoes come because with the overload of users we have to reduce the redundancy. We have to increase the scalability. We have to keep the latency. So all of this things come into Factor,",3,"It's interesting to hear about the shift in focus from purely technical aspects to a more holistic view of project success. How has this perspective influenced your approach to software development, specifically in terms of code optimization and feature prioritization?","Yeah code optimization is very important because when you have been working, you know particular code base, you are not the only one who is working on the code base. There are several other factors and so in most of the companies what happens is that the person who writes a code, he just sits it into build. I mean, he gives it for building the building takes around 2015 or 20 minutes and he or she spends the rest of the time leaving the code of the others the other peers that they get so this is very important because it helps to optimize the court that rather have written and also ensure that no pretty much a code is being pushed into the repo so that the future developers were going to come into the organization and work did not face any shoes and one of the one of the example I like to take for example, super holder while building this copilot. One of the issues that we fixed is the database throughput the database throughput is very limited if you see mongodb and other databases like we have used their Azure Cosmos DBS, however, the truth is very limited as a result of which. We just cannot I mean we just cannot append a lot of data to the database or faced a lot of data. We database due to the restricted throughput so what we did integrate we have to integrate the Apache of career. So as to handle this sort of issues. The throughput of Apache of Christmas greater than the databases that we generally use. So we use that and after that after handling the I mean high frequency things then what we did we did a bulk entry into the database. So that is how we approached a problem and did some optimization because whenever we are building the project individually this doesn't have a lot of users into our projects look kind of projects. So lot of the issues doesn't get but whenever we are working for a company or organization, we see that a lot of new shoes come because with the overload of users we have to reduce the redundancy. We have to increase the scalability. We have to keep the latency. So all of this things come into Factor,",3,True
130,130,130,130,"Interviewer: That's a good overview of CI/CD with GitHub Actions. Leveraging these tools for automation is key in modern development practices. Since you've worked with GitHub Actions, can you describe a specific workflow you've set up for a machine learning project? How did you handle version control of your models, and what strategies did you use to manage the deployment of new model versions while ensuring the system's reliability?  

Interviewee: Yeah, it's a good question. So I did try to implement the Deep learning project through I mean it was a Chase Chase Cancer image classification. It was a symbol a simple project. But I mainly my objective was to have hands on on how get action works. So. um We did try to incorporate DVC which is a data Version Control and also the ml flow as you know, having a technique for a model versioning and model tracking and model comparisons to maintaining on a one config file through which we can you know experiment overall. Parameters or whatever parameters that we set and we bring out the best solution that we can so these were the practices I followed.",3,"That's a good overview of CI/CD with GitHub Actions. Leveraging these tools for automation is key in modern development practices. Since you've worked with GitHub Actions, can you describe a specific workflow you've set up for a machine learning project? How did you handle version control of your models, and what strategies did you use to manage the deployment of new model versions while ensuring the system's reliability?","Yeah, it's a good question. So I did try to implement the Deep learning project through I mean it was a Chase Chase Cancer image classification. It was a symbol a simple project. But I mainly my objective was to have hands on on how get action works. So. um We did try to incorporate DVC which is a data Version Control and also the ml flow as you know, having a technique for a model versioning and model tracking and model comparisons to maintaining on a one config file through which we can you know experiment overall. Parameters or whatever parameters that we set and we bring out the best solution that we can so these were the practices I followed.",3,True
154,154,154,154,"Interviewer: You've given a clear explanation of how you evaluate model performance and ensure generalization. Could you now describe a situation where you would choose to use ensemble learning, and explain how it might improve the performance of a machine learning model? 

Interviewee: up Certainly ansible learning is the powerful technique where multiple machine learning models are combined to improve overall performance one situation where Ensemble learning can be particularly effective is when dealing with high variance or unstable models such as decision trees or neural networks as an example scenario in simple learning could be beneficial. Let's say we are working on a classification problem where you go to predict whether give an email is Spam or not spam. We have a data set with the mix of text features such as word frequencies email and information. In this case. We would create an ensemble of different cans classifiers each train on a subset of the data or reusing different algorithms. For instance. We could combine decision trees and forth gradient boosting machine and support machine into an example that we used in a project loan eligibility prediction. By combining these diverse models the symbol can leverage the distance of each individual model while mitigating the weaknesses, for example, reducing improving overstress enhancing performance handling imbalance data. That's all.",3,"You've given a clear explanation of how you evaluate model performance and ensure generalization. Could you now describe a situation where you would choose to use ensemble learning, and explain how it might improve the performance of a machine learning model?","up Certainly ansible learning is the powerful technique where multiple machine learning models are combined to improve overall performance one situation where Ensemble learning can be particularly effective is when dealing with high variance or unstable models such as decision trees or neural networks as an example scenario in simple learning could be beneficial. Let's say we are working on a classification problem where you go to predict whether give an email is Spam or not spam. We have a data set with the mix of text features such as word frequencies email and information. In this case. We would create an ensemble of different cans classifiers each train on a subset of the data or reusing different algorithms. For instance. We could combine decision trees and forth gradient boosting machine and support machine into an example that we used in a project loan eligibility prediction. By combining these diverse models the symbol can leverage the distance of each individual model while mitigating the weaknesses, for example, reducing improving overstress enhancing performance handling imbalance data. That's all.",3,True
163,163,163,163,"Interviewer: Certainly. Could you explain the concept of 'dropout' in neural networks and how it contributes to the generalization of a model? 

Interviewee: Drop what is a regularization technique using neural networks to prevent overfitting and improve the generalization of the model in Dropout during training randomly selected neurons are ignored or dropped out with a certain probability typically between 0.2 and 0.5. This means that the contributions to the activation of Downstream neurons are temporarily removed on the forward pass and the weights are not updated during back propagation. And Dropout contributes through the generalization of model reducing overweight encouraging robustness acting as model averaging overall drop over to the powerful legalization technique that helps better to unseen data by reducing over creating encouraging robustness and effectively programming model averaging during inference.",4,Certainly. Could you explain the concept of 'dropout' in neural networks and how it contributes to the generalization of a model?,Drop what is a regularization technique using neural networks to prevent overfitting and improve the generalization of the model in Dropout during training randomly selected neurons are ignored or dropped out with a certain probability typically between 0.2 and 0.5. This means that the contributions to the activation of Downstream neurons are temporarily removed on the forward pass and the weights are not updated during back propagation. And Dropout contributes through the generalization of model reducing overweight encouraging robustness acting as model averaging overall drop over to the powerful legalization technique that helps better to unseen data by reducing over creating encouraging robustness and effectively programming model averaging during inference.,4,True
164,164,164,164,"Interviewer: Given your experience with optimizing the Flask app and the use of Kafka for asynchronous processing, what specific challenges did you encounter with multiprocessing, and how did you address them to ensure the system's stability and performance?  

Interviewee: Okay. Um, so like there can be it's a very wide. topic actually and it is something that we all had to explore because firstly we would have so we have a service for Extracting our data from contracts, right? So contracts would be in textual format files which have been scanned. They will be in the form of PDFs or they can be in the format of docx. Right? So the extractions are done by a separate service, which was not part of our team. Right and the problem was that they were also scaling from their end. Right? And now what happens is when I'm running my code async code and I'm utilizing their ABA is in inside my code and I'm using I'm as well using multiplication, right? So it's like it's like I'm calling them versus function and inside that there is another function that is calling a process for process for function. Right? So it was recursively spawning, right? They were followers of that their We could see that we were recursively spawning a lot of processes and we have to we had to look into it why we are creating that many processes and that was one of the very important thing that we noticed after that other one is of maintaining a global state of model. Right? So once the models are loaded into our flask program, we would want all our Kafka topics. We want all our Kafka consumers to be having a separate state where they are maintaining their each of them are entering their model state, right? So for example, if I'm having a Kafka topic of let's say language detection, I wouldn't want Just to the space that topic is having for language detection. I wouldn't want to share it with let's call it language translation, right? So that separation of concern is there and that thing is pretty it is it is easier to do it in let's say threading but That is not something that works pretty well in Python, right? We have Gil, right? So we are using we are working the processes and then we are serving the topics. So that was issue. Another one was the issue of instrumentation of apis because since we are using Kafka and it is a message messaging based system and it is not an API based system and it is a sink. So instrumentation was also very tough for me because I had to prove it to my I have to show it to my leaders right that whatever steps that I have taken. They will they are conclude. There is some conclusion. There is some data behind it. Right? I wanted to connect some data and I wanted to collect some stats and that was all very tough to do because of the async nature of Kafka.",4,"Given your experience with optimizing the Flask app and the use of Kafka for asynchronous processing, what specific challenges did you encounter with multiprocessing, and how did you address them to ensure the system's stability and performance?","Okay. Um, so like there can be it's a very wide. topic actually and it is something that we all had to explore because firstly we would have so we have a service for Extracting our data from contracts, right? So contracts would be in textual format files which have been scanned. They will be in the form of PDFs or they can be in the format of docx. Right? So the extractions are done by a separate service, which was not part of our team. Right and the problem was that they were also scaling from their end. Right? And now what happens is when I'm running my code async code and I'm utilizing their ABA is in inside my code and I'm using I'm as well using multiplication, right? So it's like it's like I'm calling them versus function and inside that there is another function that is calling a process for process for function. Right? So it was recursively spawning, right? They were followers of that their We could see that we were recursively spawning a lot of processes and we have to we had to look into it why we are creating that many processes and that was one of the very important thing that we noticed after that other one is of maintaining a global state of model. Right? So once the models are loaded into our flask program, we would want all our Kafka topics. We want all our Kafka consumers to be having a separate state where they are maintaining their each of them are entering their model state, right? So for example, if I'm having a Kafka topic of let's say language detection, I wouldn't want Just to the space that topic is having for language detection. I wouldn't want to share it with let's call it language translation, right? So that separation of concern is there and that thing is pretty it is it is easier to do it in let's say threading but That is not something that works pretty well in Python, right? We have Gil, right? So we are using we are working the processes and then we are serving the topics. So that was issue. Another one was the issue of instrumentation of apis because since we are using Kafka and it is a message messaging based system and it is not an API based system and it is a sink. So instrumentation was also very tough for me because I had to prove it to my I have to show it to my leaders right that whatever steps that I have taken. They will they are conclude. There is some conclusion. There is some data behind it. Right? I wanted to connect some data and I wanted to collect some stats and that was all very tough to do because of the async nature of Kafka.",3,True
165,165,165,165,"Interviewer: That sounds like a comprehensive project, combining data science techniques with web scraping and API integration. Given your experience with NLP for sentiment analysis, can you discuss the specific libraries or tools you used in Python for NLP, and how you ensured the accuracy of the sentiment predictions?  

Interviewee: Yes, so the specific tools that I had used for that project which involved the NLP that is natural language processing wire as follows first algorithm that I had used was the RNN through RNN what had done was that? I applied the TF IDF vectorizer, which actually helped me in converting the textual information into vectors and because obviously the algorithm and the model is recognizes vectors nicely not the textual part information so that the IDF vectorizer helped me in converting that textual into vectors so due to those vectors I was able to like the model was able to recognize which commands are positive and which comments are negative or neutral then as the vectors gave the output as 0 and 1 and also 0.5 so If through that only like if a vector if the output of the vector is a zero, it means that the particular rating of the movie is negative and one means the rating of that movie is positive. So that was with the help of that. It helps me and through the cross-validation process and Hyper parameter tuning. What are what I did was that I had trained the parameters through that hyper but amateur during and at the end. With the help of cross-validation products measures and the I predicted the accuracy of that model including the accuracy prediction rate, which was which was very good as like as I remember it was around 90.7% So yeah, it was a very good rating so through these tools I had done. So and also I had used that pickle library in order to pickle the data for the process of pickling and unpicking that is carrying the data from the actual data resource and converting into the form that is actually understandable by the model and you use that for the further steps. So yeah, these were the basic tools that had used",3,"That sounds like a comprehensive project, combining data science techniques with web scraping and API integration. Given your experience with NLP for sentiment analysis, can you discuss the specific libraries or tools you used in Python for NLP, and how you ensured the accuracy of the sentiment predictions?","Yes, so the specific tools that I had used for that project which involved the NLP that is natural language processing wire as follows first algorithm that I had used was the RNN through RNN what had done was that? I applied the TF IDF vectorizer, which actually helped me in converting the textual information into vectors and because obviously the algorithm and the model is recognizes vectors nicely not the textual part information so that the IDF vectorizer helped me in converting that textual into vectors so due to those vectors I was able to like the model was able to recognize which commands are positive and which comments are negative or neutral then as the vectors gave the output as 0 and 1 and also 0.5 so If through that only like if a vector if the output of the vector is a zero, it means that the particular rating of the movie is negative and one means the rating of that movie is positive. So that was with the help of that. It helps me and through the cross-validation process and Hyper parameter tuning. What are what I did was that I had trained the parameters through that hyper but amateur during and at the end. With the help of cross-validation products measures and the I predicted the accuracy of that model including the accuracy prediction rate, which was which was very good as like as I remember it was around 90.7% So yeah, it was a very good rating so through these tools I had done. So and also I had used that pickle library in order to pickle the data for the process of pickling and unpicking that is carrying the data from the actual data resource and converting into the form that is actually understandable by the model and you use that for the further steps. So yeah, these were the basic tools that had used",3,True
172,172,172,172,"Interviewer: Your approach to data augmentation and feature extraction is quite innovative, especially the use of attention mechanisms and the concept of counter-based learning. It's clear you're keeping up with the latest research. Shifting focus to your NLP experience, you mentioned building an NLP model to find conditional statements in legal documents at Miziynet. Could you walk me through the process you followed to develop this model and how you ensured its accuracy and reliability?  

Interviewee: Exactly. So first of all, what we were trying to do is that we had to we had the legal documents which contained the legalities from Singapore and USA. So we had to find the conditional statements from these documents. Let's say if a person does a murder he will have to go trial for this and that period so we have to find we had to find such statements. So first of all, we had to do a data annotation task in which we have to First of all take out these sentences and then we applied bird to learn the sentences embeddings and the word emitting so that we could be find the most similar statements. That is the task of what we can say send similarities search or those sentences, which will have the most similarity with the sentences. We will be giving those out so that will become finding the conditionalities after that. We found out that they were the models which were working pretty good. They're like distal boat and Roberta. So we use those that is a we will we were going with what we can simply in models as they are better than the traditional approaches like bag of words or what we can say what to make right? And so we employed these techniques to find the conditional statements after data annotation and then modeling and then inferencing obviously to extract the what we can say information. From these Pages. We used various python libraries to extract the text information and we were getting unnecessary information. So we then employed OCR techniques in the images images of these pages. So, yeah, so that's what we did.",3,"Your approach to data augmentation and feature extraction is quite innovative, especially the use of attention mechanisms and the concept of counter-based learning. It's clear you're keeping up with the latest research. Shifting focus to your NLP experience, you mentioned building an NLP model to find conditional statements in legal documents at Miziynet. Could you walk me through the process you followed to develop this model and how you ensured its accuracy and reliability?","Exactly. So first of all, what we were trying to do is that we had to we had the legal documents which contained the legalities from Singapore and USA. So we had to find the conditional statements from these documents. Let's say if a person does a murder he will have to go trial for this and that period so we have to find we had to find such statements. So first of all, we had to do a data annotation task in which we have to First of all take out these sentences and then we applied bird to learn the sentences embeddings and the word emitting so that we could be find the most similar statements. That is the task of what we can say send similarities search or those sentences, which will have the most similarity with the sentences. We will be giving those out so that will become finding the conditionalities after that. We found out that they were the models which were working pretty good. They're like distal boat and Roberta. So we use those that is a we will we were going with what we can simply in models as they are better than the traditional approaches like bag of words or what we can say what to make right? And so we employed these techniques to find the conditional statements after data annotation and then modeling and then inferencing obviously to extract the what we can say information. From these Pages. We used various python libraries to extract the text information and we were getting unnecessary information. So we then employed OCR techniques in the images images of these pages. So, yeah, so that's what we did.",3,True
177,177,177,177,"Interviewer: Great! I see from your resume that you've been focusing on optimizing language detection and translation services in your current role. Can you walk me through the specific techniques or models you've found most effective in this context, and why you chose them over others?  

Interviewee: Okay. So in our large monolith service, one of the entry point was language detection and another entry point was language translation. And of course they were both related to each other depending upon the language that we are detecting we would have to compile it into English language because some of our models that were used for other business activities. They were only trained on English language. So that was the use case of both the language detection and translation. Services and regarding the optimization. But so when I saw the initial code that was in Python, so the operation was worked. So the operation part was done in two phases. First was the code part or you could say the development part and the second one was. The machine learning part so first I'll describe the code part in the core Part. I noticed that in our development phases or whatever. They used in the development the code the python code written was not clean and there were some rules which were employed for detecting language and they had CLD the name of the library. I'm quoting here citing here. They were using CLD. So basically they were using Bayesian methods for language what the library that they were using they based on Bayesian methods and it was slow and it was slow for Asian languages and it was inaccurate for Forex Auto sentences that were very small in size. And in our case there were chants that we would get a lot of data that would be good as a small sentences. so yeah, that's the like that's how I went about with the language detection part regarding the models as I told initial initial. I would I wouldn't say model but the way they were using the libraries it used. Live base. And after that what I told was either we can go with the the word level embeddings or sentence level embeddings and we can use the classification over that so I tested couple of word embeddings. Like for example, what do I can fast text and then I also evaluated couple of bird base architecture where we would be using Anna classification layer to classify the language that is off. But the problem here is of course that we don't want our language detection model to be very big, right? So the fastest that was language now on to the language translation part initially, they had a GPU based models andand easy So yeah, that's about it regarding these two.",3,"Great! I see from your resume that you've been focusing on optimizing language detection and translation services in your current role. Can you walk me through the specific techniques or models you've found most effective in this context, and why you chose them over others?","Okay. So in our large monolith service, one of the entry point was language detection and another entry point was language translation. And of course they were both related to each other depending upon the language that we are detecting we would have to compile it into English language because some of our models that were used for other business activities. They were only trained on English language. So that was the use case of both the language detection and translation. Services and regarding the optimization. But so when I saw the initial code that was in Python, so the operation was worked. So the operation part was done in two phases. First was the code part or you could say the development part and the second one was. The machine learning part so first I'll describe the code part in the core Part. I noticed that in our development phases or whatever. They used in the development the code the python code written was not clean and there were some rules which were employed for detecting language and they had CLD the name of the library. I'm quoting here citing here. They were using CLD. So basically they were using Bayesian methods for language what the library that they were using they based on Bayesian methods and it was slow and it was slow for Asian languages and it was inaccurate for Forex Auto sentences that were very small in size. And in our case there were chants that we would get a lot of data that would be good as a small sentences. so yeah, that's the like that's how I went about with the language detection part regarding the models as I told initial initial. I would I wouldn't say model but the way they were using the libraries it used. Live base. And after that what I told was either we can go with the the word level embeddings or sentence level embeddings and we can use the classification over that so I tested couple of word embeddings. Like for example, what do I can fast text and then I also evaluated couple of bird base architecture where we would be using Anna classification layer to classify the language that is off. But the problem here is of course that we don't want our language detection model to be very big, right? So the fastest that was language now on to the language translation part initially, they had a GPU based models andand easy So yeah, that's about it regarding these two.",3,True
188,188,188,188,"Interviewer: Great! I see from your resume that you've worked on a financial question answering system at JP Morgan. Can you elaborate on the specific challenges you faced while developing this system, and how you addressed them?  


Interviewee: Okay. So one of the mid so basically Financial question answering system is like you have a lot of annual reports and you have some questions on let's say the finances of the top 500 companies. So what S&P 500 companies so what that essentially that ask is you have the question you first you have to retrieve relevant reports then retriever limit when pages from those reports and then answer the question. So one of the most daunting task is was related to retrieving the relevant contacts for answering the question. And another task was how do you refine numerical reasoning using large language models? So for retrieval task, I trained a binary classifier using Advanced method of negative sampling which was placed on clustering and for the prompting will you enhancing numerical reasoning through prompting? I actually created came up with a new methodology which used questions from the training set. As an in-context learning as in context learning examples and also created Chain of Thought problems using GPS 35. So for this training questions, we had all the reasoning steps the final answer which helped in model to understand what kind of calculations it needed to A to answer the question. Yeah, that's it, I guess.",4,"Great! I see from your resume that you've worked on a financial question answering system at JP Morgan. Can you elaborate on the specific challenges you faced while developing this system, and how you addressed them?","Okay. So one of the mid so basically Financial question answering system is like you have a lot of annual reports and you have some questions on let's say the finances of the top 500 companies. So what S&P 500 companies so what that essentially that ask is you have the question you first you have to retrieve relevant reports then retriever limit when pages from those reports and then answer the question. So one of the most daunting task is was related to retrieving the relevant contacts for answering the question. And another task was how do you refine numerical reasoning using large language models? So for retrieval task, I trained a binary classifier using Advanced method of negative sampling which was placed on clustering and for the prompting will you enhancing numerical reasoning through prompting? I actually created came up with a new methodology which used questions from the training set. As an in-context learning as in context learning examples and also created Chain of Thought problems using GPS 35. So for this training questions, we had all the reasoning steps the final answer which helped in model to understand what kind of calculations it needed to A to answer the question. Yeah, that's it, I guess.",3,True
195,195,195,195,"Interviewer: Those are some insightful challenges you've identified. Let's discuss the potential impact or benefits of successfully solving this problem. How do you think implementing this machine learning pipeline will benefit the company and its operations? 


Interviewee: Okay. So, few things. So the thing is initially, we had a team of 30 QC people, which used to manually mark, all the issues in these orthomosacs, which was really time consuming. So once we have built this pipeline and we were kind of able to quickly experiment with model. So model experimentation was time was also reduced by a single day. Because we had to experiment with the different hyperparameters, which we had to initially change and see how it is going to fit. Now we have single configuration file where we cannot directly change the parameters and pass it to the pipeline. Next was, that, the work or the manual work by the team of this, QC people. Was reduced by a lot because now the model is used to detect, all the issues in the solar power plants. And then, they only need to recheck it. So the model accuracy was around 70 to 18%, which kind of helped the team to deliver the projects to the client much faster. So we kind of have cut the project delivery time by 40 to 50% depending on the size of the auto or size of the power power plant site. So faster delivery and faster experimentation. That these are the 2 things that we kind of accomplished for, by developing this pipeline. So faster experimentation is for us, ML team, and this faster delivery of the project process is, kind of, something that we were able to accomplish across organization. So when we are able to faster deliver, so we are also able to complete more projects in less time.",4,Those are some insightful challenges you've identified. Let's discuss the potential impact or benefits of successfully solving this problem. How do you think implementing this machine learning pipeline will benefit the company and its operations?,"Okay. So, few things. So the thing is initially, we had a team of 30 QC people, which used to manually mark, all the issues in these orthomosacs, which was really time consuming. So once we have built this pipeline and we were kind of able to quickly experiment with model. So model experimentation was time was also reduced by a single day. Because we had to experiment with the different hyperparameters, which we had to initially change and see how it is going to fit. Now we have single configuration file where we cannot directly change the parameters and pass it to the pipeline. Next was, that, the work or the manual work by the team of this, QC people. Was reduced by a lot because now the model is used to detect, all the issues in the solar power plants. And then, they only need to recheck it. So the model accuracy was around 70 to 18%, which kind of helped the team to deliver the projects to the client much faster. So we kind of have cut the project delivery time by 40 to 50% depending on the size of the auto or size of the power power plant site. So faster delivery and faster experimentation. That these are the 2 things that we kind of accomplished for, by developing this pipeline. So faster experimentation is for us, ML team, and this faster delivery of the project process is, kind of, something that we were able to accomplish across organization. So when we are able to faster deliver, so we are also able to complete more projects in less time.",3,True
197,197,197,197,"Interviewer: I appreciate the detailed explanation of your approach to creating a plug-and-play microservices architecture and how you leveraged AWS for deployment. Given your experience with AWS, can you discuss a specific AWS service or feature that you found particularly beneficial for ensuring the resilience of your microservices, such as auto-scaling, load balancing, or any other service that played a key role in your architecture?  

Interviewee: Definitely. Um, like my the platform had my task was again my task force basically to have a platform which can cater to the needs of thousands of people. Now, this is more of a you know, when you're kind of building up I can if you want to scale anything I can put a vertical scale or you can go horizontally. I mean I have my what do you call my code running on a particular machine and more and more users come in I can do one thing that I can have a bigger computer. I can have a bigger CPU, but that won't actually solve a lot of my problems, right the next solution to that is a very horizontal approach. I have one machine listening to what kind of listening to all the needs of all the traffic requests. If I have more machines can actually give me a more wide range of I can give me an ability to get more of this traffic towards me and but the one of the best feature that I like of AWS that it offers you a plan which it advertises as scale as you grow in which if we are getting our call. We are getting traffic higher than a normal higher than a normal threshold. It will kind of give you it'll kind of build you another server for that and it can load balance all the traffic according to it. Will it will distribute all its traffic to do those different servers according to this load balancing. Definitely it gives you a different charts for how much more the service you are using but that is the best hand like about it AWS is that it really helps you scale as you go like it really? Observe to its word and the features like the ease of how it's done is really great with that being said my application can be scaled from you know, let it be a thousand ten thousand fifty thousand. It can actually keep on getting scale as I keep on growing. Yeah.",3,"I appreciate the detailed explanation of your approach to creating a plug-and-play microservices architecture and how you leveraged AWS for deployment. Given your experience with AWS, can you discuss a specific AWS service or feature that you found particularly beneficial for ensuring the resilience of your microservices, such as auto-scaling, load balancing, or any other service that played a key role in your architecture?","Definitely. Um, like my the platform had my task was again my task force basically to have a platform which can cater to the needs of thousands of people. Now, this is more of a you know, when you're kind of building up I can if you want to scale anything I can put a vertical scale or you can go horizontally. I mean I have my what do you call my code running on a particular machine and more and more users come in I can do one thing that I can have a bigger computer. I can have a bigger CPU, but that won't actually solve a lot of my problems, right the next solution to that is a very horizontal approach. I have one machine listening to what kind of listening to all the needs of all the traffic requests. If I have more machines can actually give me a more wide range of I can give me an ability to get more of this traffic towards me and but the one of the best feature that I like of AWS that it offers you a plan which it advertises as scale as you grow in which if we are getting our call. We are getting traffic higher than a normal higher than a normal threshold. It will kind of give you it'll kind of build you another server for that and it can load balance all the traffic according to it. Will it will distribute all its traffic to do those different servers according to this load balancing. Definitely it gives you a different charts for how much more the service you are using but that is the best hand like about it AWS is that it really helps you scale as you go like it really? Observe to its word and the features like the ease of how it's done is really great with that being said my application can be scaled from you know, let it be a thousand ten thousand fifty thousand. It can actually keep on getting scale as I keep on growing. Yeah.",3,True
200,200,200,200,"Interviewer: It's clear you've thoughtfully integrated supervised and unsupervised learning methods. Shifting focus a bit, could you describe the architecture of the API you designed for the on-premise data privacy product at bizAmica Software? Specifically, how did you ensure data privacy and security while maintaining the API's functionality and performance?  

Interviewee: Sure. So I was tasked to deploy and design a model training API, which is hosted on AWS ec2 platform. So the model training API was hosted using Python and flask. So flask gives us an API invariant. The customer can issue a model training command API and internally the model will fits. So first model will start the API will start and AWS ec2 instance of within GPU access. Once the AWS instantly started internally through a database a call will be forwarded to and new instance saying to download and get all the data together which is required for model training. Once the data is downloaded and accessible to AWS S3 bucket. We start the model training when the model training is in progress. Let's say when he is completed internally that progress will be pulled to database and from the database through the front end user can see the training progress. Once the progress is completed. That is the model is completely trained at 100% we Save the model data on an AWS ec2 bucket. We clear all the data which is present on the new AWS ec2 instance with GPU. We close and shut down the AWS ec2 instance of the GPU and we handle it. So this results in a cost benefit as The user can actually annotate the data on an ec2 instance without using GPU which cost list but only the training will happen on GPU which results in cost benefit. Also. This is backend click configured by AWS S3 bucket and database applications to get the pooling of the progress results and whether the model is completed or not, which indirectly helps in data privacy as well.",3,"It's clear you've thoughtfully integrated supervised and unsupervised learning methods. Shifting focus a bit, could you describe the architecture of the API you designed for the on-premise data privacy product at bizAmica Software? Specifically, how did you ensure data privacy and security while maintaining the API's functionality and performance?","Sure. So I was tasked to deploy and design a model training API, which is hosted on AWS ec2 platform. So the model training API was hosted using Python and flask. So flask gives us an API invariant. The customer can issue a model training command API and internally the model will fits. So first model will start the API will start and AWS ec2 instance of within GPU access. Once the AWS instantly started internally through a database a call will be forwarded to and new instance saying to download and get all the data together which is required for model training. Once the data is downloaded and accessible to AWS S3 bucket. We start the model training when the model training is in progress. Let's say when he is completed internally that progress will be pulled to database and from the database through the front end user can see the training progress. Once the progress is completed. That is the model is completely trained at 100% we Save the model data on an AWS ec2 bucket. We clear all the data which is present on the new AWS ec2 instance with GPU. We close and shut down the AWS ec2 instance of the GPU and we handle it. So this results in a cost benefit as The user can actually annotate the data on an ec2 instance without using GPU which cost list but only the training will happen on GPU which results in cost benefit. Also. This is backend click configured by AWS S3 bucket and database applications to get the pooling of the progress results and whether the model is completed or not, which indirectly helps in data privacy as well.",3,True
206,206,206,206,"Interviewer: Great! I see from your resume that you've developed an ML model to detect E. coli with an impressive accuracy. Can you walk me through the process you followed to achieve such high accuracy, and which specific machine learning algorithms or techniques you found most effective in this project?  

Interviewee: Sure. So the objective of this project was to develop a machine learning model that can accurately classify DNA sequences as either promoter or non-non promoter regions based on their nucleotide compositions identifying promoter regions is fundamentally understanding Gene regulation and expression patterns. In order to perform feature extraction in my project. I used one hot encoding in that I converted the DNA sequences into numerical features suitable for machine learning models. And then I applied the machine learning model which was support Vector machine with a linear kernel to learn decision boundary between promoter and on premature sequences and finally to assess the performance of the svm model. I used the metrics which were training accuracy and testing accuracy along with confusion Matrix and classification report.",3,"Great! I see from your resume that you've developed an ML model to detect E. coli with an impressive accuracy. Can you walk me through the process you followed to achieve such high accuracy, and which specific machine learning algorithms or techniques you found most effective in this project?",Sure. So the objective of this project was to develop a machine learning model that can accurately classify DNA sequences as either promoter or non-non promoter regions based on their nucleotide compositions identifying promoter regions is fundamentally understanding Gene regulation and expression patterns. In order to perform feature extraction in my project. I used one hot encoding in that I converted the DNA sequences into numerical features suitable for machine learning models. And then I applied the machine learning model which was support Vector machine with a linear kernel to learn decision boundary between promoter and on premature sequences and finally to assess the performance of the svm model. I used the metrics which were training accuracy and testing accuracy along with confusion Matrix and classification report.,3,True
212,212,212,212,"Interviewer: That's a solid approach. Can you dive into the technical details of how you analyzed and characterized these workloads? Specifically, what metrics or parameters did you find most useful in differentiating between the workloads, and how did you extract and analyze this data? 


Interviewee: So we had to define our own parameters and, metrics, catering to the specific workload. For instance, if I'm doing a large language model characterization, the parameters which I defined were time taken, for the entire workload to be run. Time taken for the first token to be generated, as well as the number of tokens generated in one second. Those were the metrics I defined for my large language model workload. Also, for the MongoDB workload which I worked on, the parameters were the number of reads and writes which happened per second, depending on which subcategory of workload am I studying. All these studies were done in Excel, so the data was in, gathered using a Python script. The Python script created a CSV file from which I took the data, and I put it in a Excel sheet where I did analysis based on how how are my metrics changing when I change certain parameters.",3,"That's a solid approach. Can you dive into the technical details of how you analyzed and characterized these workloads? Specifically, what metrics or parameters did you find most useful in differentiating between the workloads, and how did you extract and analyze this data?","So we had to define our own parameters and, metrics, catering to the specific workload. For instance, if I'm doing a large language model characterization, the parameters which I defined were time taken, for the entire workload to be run. Time taken for the first token to be generated, as well as the number of tokens generated in one second. Those were the metrics I defined for my large language model workload. Also, for the MongoDB workload which I worked on, the parameters were the number of reads and writes which happened per second, depending on which subcategory of workload am I studying. All these studies were done in Excel, so the data was in, gathered using a Python script. The Python script created a CSV file from which I took the data, and I put it in a Excel sheet where I did analysis based on how how are my metrics changing when I change certain parameters.",4,True
213,213,213,213,"Interviewer: Let's dive into the technical skills listed on your resume. Starting with Data Structures and Algorithms, can you explain how you would optimize a basic algorithm like Bubble Sort for a large dataset, and what the resulting Big O notation would be for your optimized solution?  

Interviewee: Okay. So as you read my, you know to like data sector to optimize the basic algorithm like a bubble short for a large data set. I would Implement an optimization technique known as early termination of flying. So like I will tell you how it works like during each part during each pass through the array. I would introduce a flag variable to track whether this webs were made. If no swaps occur during a pass. It indicates that the array is already sorted and we can terminate the Sorting process early. So after it creating through the entire array and Performing comparisons, if a swap is made I said the flag variable to true no swaps are made you think about I said the flag variable to false after completing a full pass through the array if of leg remains false, it indicates that no swaps were made and there is sorted and this point I terminate the Sorting process. Then like so the average case time complexity of bubble sort with early termination remains wins. Where n is the number of elements in the air. However, the best case time complexity improves to when in scenarios where the array is already sorted as a determination prevents unnecessary iteration. Of course, we have to test that to solar. So by implementing early termination in bubble sort. I can optimize the algorithm to achieve better performance for large data sets, especially in scenarios where the air is partially sorted or nearly sorted this optimization reduces the number of comparisons and swaps required leading to improved efficiency. Yeah.",3,"Let's dive into the technical skills listed on your resume. Starting with Data Structures and Algorithms, can you explain how you would optimize a basic algorithm like Bubble Sort for a large dataset, and what the resulting Big O notation would be for your optimized solution?","Okay. So as you read my, you know to like data sector to optimize the basic algorithm like a bubble short for a large data set. I would Implement an optimization technique known as early termination of flying. So like I will tell you how it works like during each part during each pass through the array. I would introduce a flag variable to track whether this webs were made. If no swaps occur during a pass. It indicates that the array is already sorted and we can terminate the Sorting process early. So after it creating through the entire array and Performing comparisons, if a swap is made I said the flag variable to true no swaps are made you think about I said the flag variable to false after completing a full pass through the array if of leg remains false, it indicates that no swaps were made and there is sorted and this point I terminate the Sorting process. Then like so the average case time complexity of bubble sort with early termination remains wins. Where n is the number of elements in the air. However, the best case time complexity improves to when in scenarios where the array is already sorted as a determination prevents unnecessary iteration. Of course, we have to test that to solar. So by implementing early termination in bubble sort. I can optimize the algorithm to achieve better performance for large data sets, especially in scenarios where the air is partially sorted or nearly sorted this optimization reduces the number of comparisons and swaps required leading to improved efficiency. Yeah.",2,True
217,217,217,217,"Interviewer: You've given a comprehensive overview of how you optimize models and handle various techniques in machine learning. Could you now explain the concept of 'overfitting' and discuss some strategies to prevent it in machine learning models? 

Interviewee: yes, so over fitting is basically when you in general words, we can say that when you get a higher highest accuracy in your training data, but you get very low accuracy in your testing data, right so basically overfitting occurs when there is a high variance in the model and less bias, so there the overfitting occurs and model tries to fit every like data point in its generalization whereas underfitting like Is when the there is a high bias and low variance. Okay. So model the just stretches and generalize line and do not care about various outliers and classifieds now to prevent some the reasons for overfitting our Highway variants and low buyers. The model is too much complex and its not ready to use on a simpler data the size of the training data can be also big these are the reasons for overfitting also to reduce the overfitting we can increase the training data reduce the model complexity if we are using suppose a neural network and we can decrease the layer so decrease the layers so that the models complexity is reduced then all right early stopping during the train. Phase like have an eye over the loss over the training period as soon as loss begins to increase the stop the training then also there are methods like Ridge regularization and lasso regularization. And yeah again, as I said that using the Dropout for neural networks to tackle overfeeding are some kinds of techniques we can generally use to come out of overheating. or reduce",4,You've given a comprehensive overview of how you optimize models and handle various techniques in machine learning. Could you now explain the concept of 'overfitting' and discuss some strategies to prevent it in machine learning models?,"yes, so over fitting is basically when you in general words, we can say that when you get a higher highest accuracy in your training data, but you get very low accuracy in your testing data, right so basically overfitting occurs when there is a high variance in the model and less bias, so there the overfitting occurs and model tries to fit every like data point in its generalization whereas underfitting like Is when the there is a high bias and low variance. Okay. So model the just stretches and generalize line and do not care about various outliers and classifieds now to prevent some the reasons for overfitting our Highway variants and low buyers. The model is too much complex and its not ready to use on a simpler data the size of the training data can be also big these are the reasons for overfitting also to reduce the overfitting we can increase the training data reduce the model complexity if we are using suppose a neural network and we can decrease the layer so decrease the layers so that the models complexity is reduced then all right early stopping during the train. Phase like have an eye over the loss over the training period as soon as loss begins to increase the stop the training then also there are methods like Ridge regularization and lasso regularization. And yeah again, as I said that using the Dropout for neural networks to tackle overfeeding are some kinds of techniques we can generally use to come out of overheating. or reduce",3,True
222,222,222,222,"Interviewer: It sounds like you took a very methodical approach to ensure the compatibility of the model outputs. Given your experience with computer vision and machine learning, how do you stay current with the latest advancements in these fields, and could you share how you've applied a recent advancement or technique in a project or problem you've worked on?  

Interviewee: So I was working on graph convolution networks, and I have seen some blocks and some research papers on Latin space things. So to be like you I used to read the recent research papers published in reputed conferences. So I used to read the abstract and I tell if it is relevant to my work is to download that and go through that paper and if that sounds interesting and if those things are somewhat relevant to my work, then I will try to replicate that paper by searching for the code or by directly emailing the author for the code so that I can use his work and take something for work. So when I was working on graph convolution networks for hyperspectral image classification, I was getting a decent accuracy, but not very good accuracy. Then they have seen a paper where they have used Latin space. They have just encoded decoder to project. A hyperspectral image and Latin space and after the Latin space project using the laden Express projected data, they have performed convolution based classification. So I have took this thing and included in my upcoming including in my upcoming projects where I have I will take an auto project hyper spectral image into Latin space to reduce the dimension, then the light and space projected data will use that data for graph. Revolution and get an classification and we can also reconstruct the hyperspectral image from Latin space projected data. And we can train that model.",4,"It sounds like you took a very methodical approach to ensure the compatibility of the model outputs. Given your experience with computer vision and machine learning, how do you stay current with the latest advancements in these fields, and could you share how you've applied a recent advancement or technique in a project or problem you've worked on?","So I was working on graph convolution networks, and I have seen some blocks and some research papers on Latin space things. So to be like you I used to read the recent research papers published in reputed conferences. So I used to read the abstract and I tell if it is relevant to my work is to download that and go through that paper and if that sounds interesting and if those things are somewhat relevant to my work, then I will try to replicate that paper by searching for the code or by directly emailing the author for the code so that I can use his work and take something for work. So when I was working on graph convolution networks for hyperspectral image classification, I was getting a decent accuracy, but not very good accuracy. Then they have seen a paper where they have used Latin space. They have just encoded decoder to project. A hyperspectral image and Latin space and after the Latin space project using the laden Express projected data, they have performed convolution based classification. So I have took this thing and included in my upcoming including in my upcoming projects where I have I will take an auto project hyper spectral image into Latin space to reduce the dimension, then the light and space projected data will use that data for graph. Revolution and get an classification and we can also reconstruct the hyperspectral image from Latin space projected data. And we can train that model.",3,True
240,240,240,240,"Interviewer: Great! I see from your resume that you've led a team on search optimization and recommender systems at Grazitti Interactive. Can you elaborate on the specific techniques or algorithms you've found most effective in this leadership role?  

Interviewee: Oh, yeah. So first of all for the search of we were using elasticsearch as the base search engine, but it has its own limitations. So we move to more, you know, semantics based approach, which is Vector search. So we use different deep learning models like expert which is all mini based LM the like at that time. It was at the first at the ranking of empty net. So we use that and it has an image of 386 and we use that as a form making the vectors of the text. And we integrated the semantic search. But and now we have reached like we have our own Benchmark at around meditation. So or search was that around 62 65% at that time, but when we included semantic search and we went we went through it. So the benchmarking Spike up to around 85% So right now we are working on these things and one more thing that we are now embedding it to it is LTR. It's basically learned to rank. So to basically take the user feedback taking the user clicks on and Above and try to assign ways to the article when a query is sourced. So that's how the ranking is now being improved. Another thing we are. Focusing right now is using LMS. So we are basically doing some poses on query expansion. We are using now knowledge graphs for because we have new feature of to forged for the generative answer and we have integrated the gpt's so now we are building a semantic waste step drag applications. We are doing places on it. We have you used embedding with face index. We are now exploring the knowledge graphic which basically a graphic we see So yeah, so we are doing a this. right",4,Great! I see from your resume that you've led a team on search optimization and recommender systems at Grazitti Interactive. Can you elaborate on the specific techniques or algorithms you've found most effective in this leadership role?,"Oh, yeah. So first of all for the search of we were using elasticsearch as the base search engine, but it has its own limitations. So we move to more, you know, semantics based approach, which is Vector search. So we use different deep learning models like expert which is all mini based LM the like at that time. It was at the first at the ranking of empty net. So we use that and it has an image of 386 and we use that as a form making the vectors of the text. And we integrated the semantic search. But and now we have reached like we have our own Benchmark at around meditation. So or search was that around 62 65% at that time, but when we included semantic search and we went we went through it. So the benchmarking Spike up to around 85% So right now we are working on these things and one more thing that we are now embedding it to it is LTR. It's basically learned to rank. So to basically take the user feedback taking the user clicks on and Above and try to assign ways to the article when a query is sourced. So that's how the ranking is now being improved. Another thing we are. Focusing right now is using LMS. So we are basically doing some poses on query expansion. We are using now knowledge graphs for because we have new feature of to forged for the generative answer and we have integrated the gpt's so now we are building a semantic waste step drag applications. We are doing places on it. We have you used embedding with face index. We are now exploring the knowledge graphic which basically a graphic we see So yeah, so we are doing a this. right",3,True
241,241,241,241,"Interviewer: That's a very interesting application of computer vision for pattern recognition. Given the complexity of chip designs and the need for high accuracy, could you elaborate on the specific computer vision techniques or algorithms you used to achieve this pattern matching? And how did you ensure the robustness of your solution to handle variations in the patterns?  

Interviewee: Yes, so that is true. And for that initially how I will start is because the chip is very the original design of the chip is very it's very large and now like because there may be multiple repeating patterns. What I will try to do is at first I will try to do the fuzzy matching. So I will take each of the patterns from the M1 and try to look at like I will do the convolution across the image and then I will try to find the fuzzy locations. Now, it may happen that while fuzzy matching is happening then each of the pattern will have multiple similar matching coordinates. So from there to have the exact coordinates, I would use the intersection of the unions and then from those intersection of the unions. I will know and also like in with intersection of the unions, I will use that ssim so as I am is the index that helped me to realize that how close those two images are and now this images are actually not the physical drawing they are the pattern density. Maps now because of this there may be some there may also be some data loss associated with it. Now, we cannot do anything for those places where the data is lost but for all the other places we found we would help to create give them the exact matching using the IU ssim and the fuzzy matching with the with the convolution method. And and and and and the idea is that that filter for example, like similar to what we do in the CNN methods here in state of filter. We will take the pattern itself and then we will move through the whole image scan through the whole image to get the fuzzy locations initially. And then we will get the exact locations by IOU with SSI. Yeah.",4,"That's a very interesting application of computer vision for pattern recognition. Given the complexity of chip designs and the need for high accuracy, could you elaborate on the specific computer vision techniques or algorithms you used to achieve this pattern matching? And how did you ensure the robustness of your solution to handle variations in the patterns?","Yes, so that is true. And for that initially how I will start is because the chip is very the original design of the chip is very it's very large and now like because there may be multiple repeating patterns. What I will try to do is at first I will try to do the fuzzy matching. So I will take each of the patterns from the M1 and try to look at like I will do the convolution across the image and then I will try to find the fuzzy locations. Now, it may happen that while fuzzy matching is happening then each of the pattern will have multiple similar matching coordinates. So from there to have the exact coordinates, I would use the intersection of the unions and then from those intersection of the unions. I will know and also like in with intersection of the unions, I will use that ssim so as I am is the index that helped me to realize that how close those two images are and now this images are actually not the physical drawing they are the pattern density. Maps now because of this there may be some there may also be some data loss associated with it. Now, we cannot do anything for those places where the data is lost but for all the other places we found we would help to create give them the exact matching using the IU ssim and the fuzzy matching with the with the convolution method. And and and and and the idea is that that filter for example, like similar to what we do in the CNN methods here in state of filter. We will take the pattern itself and then we will move through the whole image scan through the whole image to get the fuzzy locations initially. And then we will get the exact locations by IOU with SSI. Yeah.",3,True
252,252,252,252,"Interviewer: Great! Let's dive into the case study. Imagine you are working for a company that manages a large network of solar farms. Your task is to develop an end-to-end machine learning pipeline to detect and predict maintenance issues in solar panels, leveraging your expertise in machine learning and cloud technologies. The goal is to reduce manual inspection efforts and improve the efficiency of the maintenance process. To start, can you propose a high-level solution architecture or approach to tackle this problem? 


Interviewee: Okay. So the first thing okay. It's kind of exactly problem statement that we are trying to solve. So the first thing is we have a dedicated field team, which basically, flies the drone and get all the images. Once we get all the images of the solar power plant, we particularly use thermal images. Once we get all thermal images of the solar power plant, we take that up. We process them. So, basically, processing means we use photogrammetry to generate the complete view of this, solar power plant. And once we have the complete view of the solar power plant, we based on our historical data that we had we initially had and we had already manually processed it for this, detecting the solar issues, like, we will have bounding boxes corresponding to each ortho mosaic. What we'll do is we take this, orthos, and we take the their corresponding, detections. Bounding boxes, you can say. And then we create a chunk of all these, this complete auto, we take create a chunk of this and create an image dataset with corresponding bounding boxes. Once we have those bounding boxes, what we'll do is we select, more, different models. So model selection for object detection, we can use different models. Like, we can use R CNN, faster R CNN. And, then we have new single shot detection models like YOLO and so we can use those things. So which are the latest is YOLO where it's that's why we proceeded to the yellow beard. And, after that, once we get the dataset prepared and we have preprocessed images like like I had said. We don't want a lot of noise that's why we converted it to grayscale images. And we have also used Wallace filter to make sure the contrast is in. And, oh, yeah. One more thing. So when we do fly or drawn itself, we need something named as, GSD. So GSD is the distance, global sampling distance. So what it means is that, how much meters a single pixel represents. So we have normalized that GSD also so that, model find its e find it easier to map those pixels, and there is not too much of a a difference among the autos even though the capturing process capturing of image process was different or something, you could say, and not the at the same GST. Okay. So once we have trained the model, what we can do is, we can evaluate the model based on whatever previous methods that we had for detection. Marking the solar issues. And, we can compare its performance with our model. And once we see that the the, of model performs better than what we previously had, We can simply put that model into production.",5,"Great! Let's dive into the case study. Imagine you are working for a company that manages a large network of solar farms. Your task is to develop an end-to-end machine learning pipeline to detect and predict maintenance issues in solar panels, leveraging your expertise in machine learning and cloud technologies. The goal is to reduce manual inspection efforts and improve the efficiency of the maintenance process. To start, can you propose a high-level solution architecture or approach to tackle this problem?","Okay. So the first thing okay. It's kind of exactly problem statement that we are trying to solve. So the first thing is we have a dedicated field team, which basically, flies the drone and get all the images. Once we get all the images of the solar power plant, we particularly use thermal images. Once we get all thermal images of the solar power plant, we take that up. We process them. So, basically, processing means we use photogrammetry to generate the complete view of this, solar power plant. And once we have the complete view of the solar power plant, we based on our historical data that we had we initially had and we had already manually processed it for this, detecting the solar issues, like, we will have bounding boxes corresponding to each ortho mosaic. What we'll do is we take this, orthos, and we take the their corresponding, detections. Bounding boxes, you can say. And then we create a chunk of all these, this complete auto, we take create a chunk of this and create an image dataset with corresponding bounding boxes. Once we have those bounding boxes, what we'll do is we select, more, different models. So model selection for object detection, we can use different models. Like, we can use R CNN, faster R CNN. And, then we have new single shot detection models like YOLO and so we can use those things. So which are the latest is YOLO where it's that's why we proceeded to the yellow beard. And, after that, once we get the dataset prepared and we have preprocessed images like like I had said. We don't want a lot of noise that's why we converted it to grayscale images. And we have also used Wallace filter to make sure the contrast is in. And, oh, yeah. One more thing. So when we do fly or drawn itself, we need something named as, GSD. So GSD is the distance, global sampling distance. So what it means is that, how much meters a single pixel represents. So we have normalized that GSD also so that, model find its e find it easier to map those pixels, and there is not too much of a a difference among the autos even though the capturing process capturing of image process was different or something, you could say, and not the at the same GST. Okay. So once we have trained the model, what we can do is, we can evaluate the model based on whatever previous methods that we had for detection. Marking the solar issues. And, we can compare its performance with our model. And once we see that the the, of model performs better than what we previously had, We can simply put that model into production.",3,True
268,268,268,268,"Interviewer: Understood, it's clear that your experience with C++ has been more focused on competitive programming where performance is key. Given your background in competitive programming, how do you approach writing efficient algorithms in C++? Can you give an example of a time when you had to optimize a solution for a particularly challenging problem, and what techniques or data structures you employed to achieve that?  

Interviewee: Uh, yeah, so there are lots of times when and when there is a question in computer programming where the time limit is only one second and the N value is expected and value is like 10 to the power 6. So which expects us to write up optimal code in when I'm complexity but some but the most easy easy course like the boot Force One which comes to our mind which cost two Loops or one Loop one Loop and one binary search approach. So most of the times what we do is like in easier problems, these things can be tackled with using map algorithms like map helps us reduce. Oh and square it and completely problems to Owen and also binary research helps us to improve that like n Square time to unlock in. And they are different different data structures like priority queue. So in if we use that then the time complexity can be reduced. Like there was one one problem where we have to we have to Optimize the transactions between lots of land like a friend. It takes money from BCD and there are lots of transaction between all of them and we have to optimize the minimum number of transaction to settle all the bills between each of the friends. So if we write a Brute Force approach, it will require one square time complexity. So it will not pass in only one second. So if we can what have done is I have optimized and used to Priority use for that one of them including all the friends which have a positive value like they are they are old and the other you contain all of the persons who people and then I keep popping that the highest value from both of them and settling then and like if one one host 100 dollar and the second person is old minus 120 dollars and 100 dollars will be settled and that was 120 dollar bill have printed Dollar in deficit and it will again Go into the priority. So by this approach. I reduce the algorithm from n Square to end login. First you was in login.",3,"Understood, it's clear that your experience with C++ has been more focused on competitive programming where performance is key. Given your background in competitive programming, how do you approach writing efficient algorithms in C++? Can you give an example of a time when you had to optimize a solution for a particularly challenging problem, and what techniques or data structures you employed to achieve that?","Uh, yeah, so there are lots of times when and when there is a question in computer programming where the time limit is only one second and the N value is expected and value is like 10 to the power 6. So which expects us to write up optimal code in when I'm complexity but some but the most easy easy course like the boot Force One which comes to our mind which cost two Loops or one Loop one Loop and one binary search approach. So most of the times what we do is like in easier problems, these things can be tackled with using map algorithms like map helps us reduce. Oh and square it and completely problems to Owen and also binary research helps us to improve that like n Square time to unlock in. And they are different different data structures like priority queue. So in if we use that then the time complexity can be reduced. Like there was one one problem where we have to we have to Optimize the transactions between lots of land like a friend. It takes money from BCD and there are lots of transaction between all of them and we have to optimize the minimum number of transaction to settle all the bills between each of the friends. So if we write a Brute Force approach, it will require one square time complexity. So it will not pass in only one second. So if we can what have done is I have optimized and used to Priority use for that one of them including all the friends which have a positive value like they are they are old and the other you contain all of the persons who people and then I keep popping that the highest value from both of them and settling then and like if one one host 100 dollar and the second person is old minus 120 dollars and 100 dollars will be settled and that was 120 dollar bill have printed Dollar in deficit and it will again Go into the priority. So by this approach. I reduce the algorithm from n Square to end login. First you was in login.",2,True
272,272,272,272,"Interviewer: It's understandable that in a proof of concept stage, there are often higher priority tasks that take precedence. Having the flexibility to fine-tune the model further or provide user options for interpretation is a good strategy for future development. Given your experience with this POC, how do you plan to approach the next stage of development to address these ambiguous cases and potentially improve the user experience with the sentiment analysis results?  

Interviewee: So I had few plans for this specific project. So one of them was to create a specific keywords, or to fetch the specific keywords from from this send reviews. So let's say we have a keyword like I love the battery of this phone. So it will take the battery from this specific keyword from this specific review. And since it is positive sentiment we can just take that as a battery and the user is saying it in a positive way. Now they are some issues with this. So let's say is he I don't like I do not like battery of it. So the most important keyword over here will be battery but it is a negative word. I never negative sentence. I do not like so over here. We need to rather than keep your selecting one specific keyword. We will use in a combination of words or phrases. We call it a smart question phrases. So that is one of the things that I have yet to implement on it and I was just thinking on it. apart from it The objective is to create a web application that has the dashboard the way is a user can see so let's say I just click on they will be option that it will give it to me like battery. I click on battery. It will give me the reviews which has positive as well as negative reviews. It will list me all the reviews that has positive sentiment as well as negative sentiment containing the important words battery. It is not a simple what is a",3,"It's understandable that in a proof of concept stage, there are often higher priority tasks that take precedence. Having the flexibility to fine-tune the model further or provide user options for interpretation is a good strategy for future development. Given your experience with this POC, how do you plan to approach the next stage of development to address these ambiguous cases and potentially improve the user experience with the sentiment analysis results?","So I had few plans for this specific project. So one of them was to create a specific keywords, or to fetch the specific keywords from from this send reviews. So let's say we have a keyword like I love the battery of this phone. So it will take the battery from this specific keyword from this specific review. And since it is positive sentiment we can just take that as a battery and the user is saying it in a positive way. Now they are some issues with this. So let's say is he I don't like I do not like battery of it. So the most important keyword over here will be battery but it is a negative word. I never negative sentence. I do not like so over here. We need to rather than keep your selecting one specific keyword. We will use in a combination of words or phrases. We call it a smart question phrases. So that is one of the things that I have yet to implement on it and I was just thinking on it. apart from it The objective is to create a web application that has the dashboard the way is a user can see so let's say I just click on they will be option that it will give it to me like battery. I click on battery. It will give me the reviews which has positive as well as negative reviews. It will list me all the reviews that has positive sentiment as well as negative sentiment containing the important words battery. It is not a simple what is a",3,True
279,279,279,279,"Interviewer: I appreciate your explanation of transfer learning. Could you now describe a scenario where you would choose to use a convolutional neural network (CNN) over a traditional feedforward neural network, and explain the reasoning behind this choice? 

Interviewee: Yeah, sure. So most common example will be when we are dealing with images. So what basically convolutional neural network does? Which is which is different from a regular neural network is the convolution itself. So basically you have filters of different sizes and you convert these filters with your image and then you suppose you have 20 cross 20 Mage and you have some other filter and after converting you get suppose a five cross five image. So what basically this convolution does is it reduces the amount of complexity of your image, but it still has the data in it because of the convolution you are not just ignoring the cells. And you can you use multiple filters to extract multiple features and then you can apply a new related to this. So this way you still have the information that is needed. But you need a little bit less computation instead of directly feeding the whole image to the neural network. So yeah in images specifically correlational",3,"I appreciate your explanation of transfer learning. Could you now describe a scenario where you would choose to use a convolutional neural network (CNN) over a traditional feedforward neural network, and explain the reasoning behind this choice?","Yeah, sure. So most common example will be when we are dealing with images. So what basically convolutional neural network does? Which is which is different from a regular neural network is the convolution itself. So basically you have filters of different sizes and you convert these filters with your image and then you suppose you have 20 cross 20 Mage and you have some other filter and after converting you get suppose a five cross five image. So what basically this convolution does is it reduces the amount of complexity of your image, but it still has the data in it because of the convolution you are not just ignoring the cells. And you can you use multiple filters to extract multiple features and then you can apply a new related to this. So this way you still have the information that is needed. But you need a little bit less computation instead of directly feeding the whole image to the neural network. So yeah in images specifically correlational",3,True
282,282,282,282,"Interviewer: Thank you for sharing those experiences. It's clear that you've tackled complex problems across a range of domains. Given your extensive work in computer vision, can you discuss a specific instance where you had to optimize a model for better performance, perhaps in terms of speed or accuracy, and what strategies you employed to achieve that?  

Interviewee: Okay. Okay. So in that particular scenario, so we'll take about the sizzle project in which I have applied computer vision. So there was a challenge like okay if someone is shooting someone and there is a gun and in that particular those games, so we annotate those frames. So there were annotators who were annotating. I was involved in moreover the model development and Improvement of due to the hyper parameters and other things so the framework was a tensorflow and the model we were using was resonate 18 reset 50. We tried with different model Inception version 3. Also we have tried that time so majorly the model was revolving around resnet 18 doesn't 50 and Inception version 3. So in that particular scenario, there was a challenge like there are multiple there are not multiple frames of one single in instance because the inside instance of shooting someone suppose. I'm taking just an example suppose if someone is shooting someone in that particular scenario, there will be not multiple frames you will get to annotate so in that particular, If there is a scenario that if you don't have data if much data to train a model that I in that particular scenario, I have applied the argumentation techniques documentation technique was a little bit constrained. So we have applied like in the contract rightness rotation zooming and other prospective to give an idea. Okay, this particular instance will look something like this and this particular instance will look something like this. So I applied different documentation technique. And there is a Dropout layer as well in that that particular training model in which diffuses which basically multiple neurons to learn from each other so that the model should not be biased towards any of the particular class. Basically if you can say because in object or that is a classification plus detection, right? So so that should not be biased so I I have cartoons some of the hyper parameters as well like those hyper parameter first. I applied some argumentation techniques which improve my accuracy from originally the accuracy was like 72 to 72% when I have applied documentation techniques logically, like different different scenarios. There are in build argumentation technique. So in Python and tensorflow now Keras is also part of tensorflow. So in that particular scenario, I have applied documentation technique then a regularization technique in which Dropout layer was a scenario where I have used a Dropout layer. and in the final stage at the time of detection, right? So for optimizing that thing I have also like change some of the hyper parameters inside the tensorflow model which gave me an accuracy of around like 189 to 90% of accuracy and they were some few other things like adding some of the more images for that particular instance. Our accuracy at that particular season was 97.98 % accurate of a model was because we were Add argument and we are keeping our and making our data set very stronger over the times like slowly slowly over the time. We have multiple data set and from those data sets if the data set has been improved and because we are focusing again three to four games right there were not much of the incense is to be stored at our basically annotators. So from the increasing the data set also it helped me a lot and from the hyper parameter tuning like in Yolo. There is a have a parameter tuning to for the bounding boxes, which is generated as cross as right in the how you know works is like as process we divide the basically frames and the bounding box center point if the object lies in that in that particular scenario that bounding box is responsible to basically detect that particular object from from the image. So that particular thing also I have applied so hyper parameter tuning regularization technique like argumentation and Dropout layer help me to achieve almost like 95 to 97% of accuracy in season which was a good accuracy and over the time we we can figure out to increase more as well. But yeah, this was the whole particular thing, which I have done in any of this was the same thing, but there was bunch of 100 returns more annotators as comparison to season 2, we have much of the data and obviously the brand logos and other things should not will not change over the time like it will not change drastically right like suddenly.Over instances are very easy to pick up and why because as comparison to gaming because in gaming sometimes it happens like the instance will be a little bit changed and there will be a challenge if something is like little bit more change than it will be a challenge. But in in terms of logo detection it is not that much difficult and we calculate the pose estimation as well. And we tell in nanometer. We also tell the clients that okay, you can place your logo from here to here so that your brand visibility will be increasing because if you think in when a batsman is playing right in that particular position it he is in a like bend down position in which this particular logo will not be that much visible if it is in the chest and if it is it will be in the stomach then it will be more visible when a batsman is playing and if some player is playing at that particular point which particular section of the boundary in I'm taking up example of cricket in which particular section like situation is always hitting in this particular. Direction so what will happen is like for the other and that was a real-time detection in which God we suggested is true like at the end in the fencing there are brandings. Right? So if Rohit Sharma is having this this particular flow of playing in this particular direction, so we suggest Brands. Okay, you pay something more and real in the real time whenever there is a boundary you will get your visibility at that particular time. So we give that particular analysis also to them. So this is how I worked. Yeah in computer vision. And I know the Technologies. Hello. Hello.",4,"Thank you for sharing those experiences. It's clear that you've tackled complex problems across a range of domains. Given your extensive work in computer vision, can you discuss a specific instance where you had to optimize a model for better performance, perhaps in terms of speed or accuracy, and what strategies you employed to achieve that?","Okay. Okay. So in that particular scenario, so we'll take about the sizzle project in which I have applied computer vision. So there was a challenge like okay if someone is shooting someone and there is a gun and in that particular those games, so we annotate those frames. So there were annotators who were annotating. I was involved in moreover the model development and Improvement of due to the hyper parameters and other things so the framework was a tensorflow and the model we were using was resonate 18 reset 50. We tried with different model Inception version 3. Also we have tried that time so majorly the model was revolving around resnet 18 doesn't 50 and Inception version 3. So in that particular scenario, there was a challenge like there are multiple there are not multiple frames of one single in instance because the inside instance of shooting someone suppose. I'm taking just an example suppose if someone is shooting someone in that particular scenario, there will be not multiple frames you will get to annotate so in that particular, If there is a scenario that if you don't have data if much data to train a model that I in that particular scenario, I have applied the argumentation techniques documentation technique was a little bit constrained. So we have applied like in the contract rightness rotation zooming and other prospective to give an idea. Okay, this particular instance will look something like this and this particular instance will look something like this. So I applied different documentation technique. And there is a Dropout layer as well in that that particular training model in which diffuses which basically multiple neurons to learn from each other so that the model should not be biased towards any of the particular class. Basically if you can say because in object or that is a classification plus detection, right? So so that should not be biased so I I have cartoons some of the hyper parameters as well like those hyper parameter first. I applied some argumentation techniques which improve my accuracy from originally the accuracy was like 72 to 72% when I have applied documentation techniques logically, like different different scenarios. There are in build argumentation technique. So in Python and tensorflow now Keras is also part of tensorflow. So in that particular scenario, I have applied documentation technique then a regularization technique in which Dropout layer was a scenario where I have used a Dropout layer. and in the final stage at the time of detection, right? So for optimizing that thing I have also like change some of the hyper parameters inside the tensorflow model which gave me an accuracy of around like 189 to 90% of accuracy and they were some few other things like adding some of the more images for that particular instance. Our accuracy at that particular season was 97.98 % accurate of a model was because we were Add argument and we are keeping our and making our data set very stronger over the times like slowly slowly over the time. We have multiple data set and from those data sets if the data set has been improved and because we are focusing again three to four games right there were not much of the incense is to be stored at our basically annotators. So from the increasing the data set also it helped me a lot and from the hyper parameter tuning like in Yolo. There is a have a parameter tuning to for the bounding boxes, which is generated as cross as right in the how you know works is like as process we divide the basically frames and the bounding box center point if the object lies in that in that particular scenario that bounding box is responsible to basically detect that particular object from from the image. So that particular thing also I have applied so hyper parameter tuning regularization technique like argumentation and Dropout layer help me to achieve almost like 95 to 97% of accuracy in season which was a good accuracy and over the time we we can figure out to increase more as well. But yeah, this was the whole particular thing, which I have done in any of this was the same thing, but there was bunch of 100 returns more annotators as comparison to season 2, we have much of the data and obviously the brand logos and other things should not will not change over the time like it will not change drastically right like suddenly.Over instances are very easy to pick up and why because as comparison to gaming because in gaming sometimes it happens like the instance will be a little bit changed and there will be a challenge if something is like little bit more change than it will be a challenge. But in in terms of logo detection it is not that much difficult and we calculate the pose estimation as well. And we tell in nanometer. We also tell the clients that okay, you can place your logo from here to here so that your brand visibility will be increasing because if you think in when a batsman is playing right in that particular position it he is in a like bend down position in which this particular logo will not be that much visible if it is in the chest and if it is it will be in the stomach then it will be more visible when a batsman is playing and if some player is playing at that particular point which particular section of the boundary in I'm taking up example of cricket in which particular section like situation is always hitting in this particular. Direction so what will happen is like for the other and that was a real-time detection in which God we suggested is true like at the end in the fencing there are brandings. Right? So if Rohit Sharma is having this this particular flow of playing in this particular direction, so we suggest Brands. Okay, you pay something more and real in the real time whenever there is a boundary you will get your visibility at that particular time. So we give that particular analysis also to them. So this is how I worked. Yeah in computer vision. And I know the Technologies. Hello. Hello.",3,True
285,285,285,285,"Interviewer: It's good to hear that you've had success with PyTorch in deployment as well. Your ability to adapt and work with the tools that provide the most control is certainly valuable. Since you've worked with both frameworks in deployment, can you describe a specific instance where you optimized a PyTorch model for production use, perhaps by leveraging techniques like quantization or model pruning, to reduce the model's size and improve inference speed?  

Interviewee: yeah, so for that there is a another project that I worked on. where I was doing ball tracking so there's a sports series is a ball and the particularly the click the Sports Hall Cricket. So there is a every time a bowler bowls. We have to track the bowl and then do ball counting and to generate other tips based on that. So for that I have to train a model because there is no ready-made model or didn't model available online. So I had to collect the data first. after that in the data I had to Annotate it with the help of wood and team and after that when I updated I had to train a model. So there were two grandson model of trained all the traditional model as well as a segmentation model so for probably generation model. Yeah there. when training the model I had to use a half precision to Reduce the size of the model after it is straight. So let's say at every poke. The model is saving the best. Performance the best best accuracy model it is saving at every park or every third or every fourth of whatever he want and after it saves it it at that time. It does not do that optimization. It does it at the end of the training when the training is complete then it's the best model that got Saved and then after that it does have Precision half Precision on it to optimize it like for example in my case if the model size was 500. 50 mb sort of like that and then after adding half position at the end of training it got reduced to 136 MB. So like it's like closer to five times reduction. So yeah, that is one of the things I have used.",3,"It's good to hear that you've had success with PyTorch in deployment as well. Your ability to adapt and work with the tools that provide the most control is certainly valuable. Since you've worked with both frameworks in deployment, can you describe a specific instance where you optimized a PyTorch model for production use, perhaps by leveraging techniques like quantization or model pruning, to reduce the model's size and improve inference speed?","yeah, so for that there is a another project that I worked on. where I was doing ball tracking so there's a sports series is a ball and the particularly the click the Sports Hall Cricket. So there is a every time a bowler bowls. We have to track the bowl and then do ball counting and to generate other tips based on that. So for that I have to train a model because there is no ready-made model or didn't model available online. So I had to collect the data first. after that in the data I had to Annotate it with the help of wood and team and after that when I updated I had to train a model. So there were two grandson model of trained all the traditional model as well as a segmentation model so for probably generation model. Yeah there. when training the model I had to use a half precision to Reduce the size of the model after it is straight. So let's say at every poke. The model is saving the best. Performance the best best accuracy model it is saving at every park or every third or every fourth of whatever he want and after it saves it it at that time. It does not do that optimization. It does it at the end of the training when the training is complete then it's the best model that got Saved and then after that it does have Precision half Precision on it to optimize it like for example in my case if the model size was 500. 50 mb sort of like that and then after adding half position at the end of training it got reduced to 136 MB. So like it's like closer to five times reduction. So yeah, that is one of the things I have used.",3,True
302,302,302,302,"Interviewer: Great! I see from your resume that you've worked on a variety of projects involving machine learning, particularly at SenseHawk and Capgemini Invent. Can you walk me through how you approached the machine learning pipeline for detecting thermal issues in solar panels at SenseHawk, specifically focusing on the model selection and the reasoning behind choosing PyTorch Lightning as part of your tech stack?  


Interviewee: Okay. So, starting from the beginning, So initially, they had a little bit scattered strip of retina net that they were trying to build a model on. But, the experimentation and hyperparameter during part was used to take a lot of time. So what we when I joined I streamlined the complete process. So, basically, I developed the this, complete end to end machine learning training pipeline. And the reason why I chose PyTorch Lightning was because at that so, training the model using Lightning was easier than using Vanilla PyTorch. We don't have to write, a complete loop for training and normalization. And optimize the other step. So we can directly use part of the function where we can easily define what should be what which optimizer to use, which loss to use, and those sort of things. And it will handle all the steps by itself. So that is one of the particular reason why we chose PyTorch Lightning. Another reason why we chose PyTorch was, at that particular time, there were a lot of, models that we wanted to experiment with. And they all were in PyTorch. So we had experimental TensorFlow, but, not as much And our main priority was to get into PyTorch. So that is one of the reasons. And, and, the another part about ML pipeline that, too, when we were building that, so we tracked everything, like, artifact metrics, cyber parameter using MLflow, And, the sub we had supported different models like, for retina net, and the YOLO v eight. And, what we did was, we wanted to compare the performance, so that's why we had logged all the models and all the experimentation part. Using ML.",4,"Great! I see from your resume that you've worked on a variety of projects involving machine learning, particularly at SenseHawk and Capgemini Invent. Can you walk me through how you approached the machine learning pipeline for detecting thermal issues in solar panels at SenseHawk, specifically focusing on the model selection and the reasoning behind choosing PyTorch Lightning as part of your tech stack?","Okay. So, starting from the beginning, So initially, they had a little bit scattered strip of retina net that they were trying to build a model on. But, the experimentation and hyperparameter during part was used to take a lot of time. So what we when I joined I streamlined the complete process. So, basically, I developed the this, complete end to end machine learning training pipeline. And the reason why I chose PyTorch Lightning was because at that so, training the model using Lightning was easier than using Vanilla PyTorch. We don't have to write, a complete loop for training and normalization. And optimize the other step. So we can directly use part of the function where we can easily define what should be what which optimizer to use, which loss to use, and those sort of things. And it will handle all the steps by itself. So that is one of the particular reason why we chose PyTorch Lightning. Another reason why we chose PyTorch was, at that particular time, there were a lot of, models that we wanted to experiment with. And they all were in PyTorch. So we had experimental TensorFlow, but, not as much And our main priority was to get into PyTorch. So that is one of the reasons. And, and, the another part about ML pipeline that, too, when we were building that, so we tracked everything, like, artifact metrics, cyber parameter using MLflow, And, the sub we had supported different models like, for retina net, and the YOLO v eight. And, what we did was, we wanted to compare the performance, so that's why we had logged all the models and all the experimentation part. Using ML.",3,True
308,308,308,308,"Interviewer: It's clear that you have a strong grasp on working with large language models and the various techniques for model optimization and training. The quantization approach you mentioned is particularly interesting for reducing memory usage without significant loss of accuracy. Given your experience with fine-tuning, could you elaborate on how you approach the process of fine-tuning a pre-trained model for a specific use case, and what kind of data or domain adaptation is typically required for your projects?  

Interviewee: Yeah, great question. So let's talk about one fine tuning technique, which I am working currently. So there are a lot of different techniques which I did I can cover all of them if you if you are interested, but let's cover one technique which I did which is pretty you know unique in its own way and not people not usually people do that. So what we are trying to do is I'll try to give a domain knowledge what we are trying to I am working for a healthcare to make client where where he just not he wanted the model to not you know, give this medical names. Let's say there are a lot of medicines named Frozen and all so if a chatbot gives all these medication names it can be dangerous and it can so what what should the chatbots says if anyone asked these medical questions the chatbot should say that hey I can advise you, you know, what is the symptoms and conditions, but I cannot, you know advice. Any prescription prescription medicine you have to consult a doctor or some expert for this? So that's why this is the card rail, which we are trying to build in this chatbot. So we find tuned our model to learn this kind of question that if anyone asks, you know about these medicines and all our guardrail is going to get triggered such that it doesn't give the answer and it also gives the right advice to go to you know, the respected. Experts of this field who can give the right advice to the customer to the patient because a doctor can ask hundreds of questions. It understands the history of the of the patient and accordingly all these kind of criteria. It gives a prescribed medicine and we want the best for our patient. That's why we build this card and how did we build this guardrail is we use DP or technique? That's a direct difference optimization technique. How did we build the data set is for building this data sets. We need a question. We need to choose an answer. We need a rejected answer. So the loss function of this DPO is as such that we try to find the model such that the question and the chosen answer. The chosen answer has, you know, we try to increase the chosen answers preference increase the probability of that and decrease the rejected answers probability such that, you know, every time in the user Answer is always set by the this model. And this is how we try to you know, use a free trained model to find tune on top of a clinical data set domain. So and this is one thing let's talk about general, you know, fine tuning of General fine tuning is one instructing and another is you know continuation for fine tuning. So what is continuation and fine tuning in let's say our free training model is not expert in physics. What we can do is we can have lot of physical physics related Wikipedia articles or any articles research papers and all and we can feed it to our pre-trained model such that it can do the next word prediction and it can learn this domain of physics. That's how this is done and coming to the instructor fine tuning technique it that's where you know where the moral can learn to follow an instruction. Not only it understands the physics domain. So, you know understanding some Concepts and domain is one part and the second part is to also the chatbot has to follow the instructions given by the user. That's where instruct fine-tuning comes the data set over there. It looks like where we give the instruction we have a question and we also have an answer in this pattern. It will understand that. Hey there's a instruction and we have to follow that instruction. That's how you know the pre-training date. This is the data set for the instruction fine tuning works.",4,"It's clear that you have a strong grasp on working with large language models and the various techniques for model optimization and training. The quantization approach you mentioned is particularly interesting for reducing memory usage without significant loss of accuracy. Given your experience with fine-tuning, could you elaborate on how you approach the process of fine-tuning a pre-trained model for a specific use case, and what kind of data or domain adaptation is typically required for your projects?","Yeah, great question. So let's talk about one fine tuning technique, which I am working currently. So there are a lot of different techniques which I did I can cover all of them if you if you are interested, but let's cover one technique which I did which is pretty you know unique in its own way and not people not usually people do that. So what we are trying to do is I'll try to give a domain knowledge what we are trying to I am working for a healthcare to make client where where he just not he wanted the model to not you know, give this medical names. Let's say there are a lot of medicines named Frozen and all so if a chatbot gives all these medication names it can be dangerous and it can so what what should the chatbots says if anyone asked these medical questions the chatbot should say that hey I can advise you, you know, what is the symptoms and conditions, but I cannot, you know advice. Any prescription prescription medicine you have to consult a doctor or some expert for this? So that's why this is the card rail, which we are trying to build in this chatbot. So we find tuned our model to learn this kind of question that if anyone asks, you know about these medicines and all our guardrail is going to get triggered such that it doesn't give the answer and it also gives the right advice to go to you know, the respected. Experts of this field who can give the right advice to the customer to the patient because a doctor can ask hundreds of questions. It understands the history of the of the patient and accordingly all these kind of criteria. It gives a prescribed medicine and we want the best for our patient. That's why we build this card and how did we build this guardrail is we use DP or technique? That's a direct difference optimization technique. How did we build the data set is for building this data sets. We need a question. We need to choose an answer. We need a rejected answer. So the loss function of this DPO is as such that we try to find the model such that the question and the chosen answer. The chosen answer has, you know, we try to increase the chosen answers preference increase the probability of that and decrease the rejected answers probability such that, you know, every time in the user Answer is always set by the this model. And this is how we try to you know, use a free trained model to find tune on top of a clinical data set domain. So and this is one thing let's talk about general, you know, fine tuning of General fine tuning is one instructing and another is you know continuation for fine tuning. So what is continuation and fine tuning in let's say our free training model is not expert in physics. What we can do is we can have lot of physical physics related Wikipedia articles or any articles research papers and all and we can feed it to our pre-trained model such that it can do the next word prediction and it can learn this domain of physics. That's how this is done and coming to the instructor fine tuning technique it that's where you know where the moral can learn to follow an instruction. Not only it understands the physics domain. So, you know understanding some Concepts and domain is one part and the second part is to also the chatbot has to follow the instructions given by the user. That's where instruct fine-tuning comes the data set over there. It looks like where we give the instruction we have a question and we also have an answer in this pattern. It will understand that. Hey there's a instruction and we have to follow that instruction. That's how you know the pre-training date. This is the data set for the instruction fine tuning works.",3,True
316,316,316,316,"Interviewer: Utilizing the Mahalanobis distance and p-values for coherence testing is a sophisticated method. How did you determine the threshold for the p-value to establish the coherence of the data sources? Additionally, in the context of financial data, were there specific features or patterns you focused on to ensure that the synthetic data remained representative of the original data, especially in terms of its utility for training AI models?  


Interviewee: So specifically we used heuristics from Finance such as a volatility surface Arbitrage check. So volatility surface must have arbitrage-free must be arbitrage-free. So we would have checks for calendar Arbitrage and Butterfly Arbitrage that we would check to make sure that our generative data was correctly. And coherently generated the p-values were done empirically so I would create inflated data that I knew for a fact did not come from the distribution and I would check to see whether or not it's just specific p-value was able to reject AKA x amount of the time and then I would permute the data either leave some in or take some out and recalculate the p-value to see if I would go back and get the exact same decision for that value. And in some summary we came up with 0.025, which is a lot stronger than 0.05 as a metric because data from Financial authorities are Financial distributions tend to be skewed.",4,"Utilizing the Mahalanobis distance and p-values for coherence testing is a sophisticated method. How did you determine the threshold for the p-value to establish the coherence of the data sources? Additionally, in the context of financial data, were there specific features or patterns you focused on to ensure that the synthetic data remained representative of the original data, especially in terms of its utility for training AI models?","So specifically we used heuristics from Finance such as a volatility surface Arbitrage check. So volatility surface must have arbitrage-free must be arbitrage-free. So we would have checks for calendar Arbitrage and Butterfly Arbitrage that we would check to make sure that our generative data was correctly. And coherently generated the p-values were done empirically so I would create inflated data that I knew for a fact did not come from the distribution and I would check to see whether or not it's just specific p-value was able to reject AKA x amount of the time and then I would permute the data either leave some in or take some out and recalculate the p-value to see if I would go back and get the exact same decision for that value. And in some summary we came up with 0.025, which is a lot stronger than 0.05 as a metric because data from Financial authorities are Financial distributions tend to be skewed.",3,True
319,319,319,319,"Interviewer: Given the complexity of the model architecture for handwritten text recognition, how did you manage the training process in terms of data augmentation, batch size selection, and learning rate tuning to ensure the model converged to a satisfactory accuracy level?  

Interviewee: so like given the complexity of model it was quite difficult project You know, so firstly we have to decide how many layers as we already told you. We use five scene and layers three are three iron and one CDC layer. So then we have to decide the best. I like we use the IAM data set. It is very huge. So managing the training process for the handed and text recognition model and what separates strategies to ensure convergence to a satisfactory accuracy level while mitigating the challenges posed by model complexity. Like firstly dot augmentation techniques were employed to increase the diversity and robustness of the training data without collecting additional label samples. Augmentation techniques included random rotations translations scaling and flipping of Android and text images. then was the best size selection message selection is crucial for balancing competitions can model conversions we experimented with different bed sizes ranging from small batches to large ones to find Optimal balance larger sizes were preferred to expert pedal issue and Hardware acceleration, but we ensured that best sizes were not too large to cause memory constraints to degrade the conventions quality then learning great tuning is a was essential for controlling the rate of perimeter of dates during And training and showing stable convergence. We employed learning date schedules or adaptive learning date algorithms such as Adam and RMS prop to dynamically adjust the learning rate based on training progress. Then hypermatter hyperparameter tuning techniques such as create search or random search for employee to systematically approach the learning date hyperparameter space and then identify the optimal learning rate for the given model architecture and data set. By carefully managing data documentation best size selection learning date tuning we aim to strike a balance between model complexity and training efficiency while ensuring the model converts to a satisfactory accuracy level. Thank you.",3,"Given the complexity of the model architecture for handwritten text recognition, how did you manage the training process in terms of data augmentation, batch size selection, and learning rate tuning to ensure the model converged to a satisfactory accuracy level?","so like given the complexity of model it was quite difficult project You know, so firstly we have to decide how many layers as we already told you. We use five scene and layers three are three iron and one CDC layer. So then we have to decide the best. I like we use the IAM data set. It is very huge. So managing the training process for the handed and text recognition model and what separates strategies to ensure convergence to a satisfactory accuracy level while mitigating the challenges posed by model complexity. Like firstly dot augmentation techniques were employed to increase the diversity and robustness of the training data without collecting additional label samples. Augmentation techniques included random rotations translations scaling and flipping of Android and text images. then was the best size selection message selection is crucial for balancing competitions can model conversions we experimented with different bed sizes ranging from small batches to large ones to find Optimal balance larger sizes were preferred to expert pedal issue and Hardware acceleration, but we ensured that best sizes were not too large to cause memory constraints to degrade the conventions quality then learning great tuning is a was essential for controlling the rate of perimeter of dates during And training and showing stable convergence. We employed learning date schedules or adaptive learning date algorithms such as Adam and RMS prop to dynamically adjust the learning rate based on training progress. Then hypermatter hyperparameter tuning techniques such as create search or random search for employee to systematically approach the learning date hyperparameter space and then identify the optimal learning rate for the given model architecture and data set. By carefully managing data documentation best size selection learning date tuning we aim to strike a balance between model complexity and training efficiency while ensuring the model converts to a satisfactory accuracy level. Thank you.",3,True
324,324,324,324,"Interviewer: Turning to a different aspect of your experience, you've worked on projects involving both Natural Language Processing (NLP) and Computer Vision (CV). How do you approach the integration of NLP and CV in a project, and what are some key challenges you've encountered in this integration?  

Interviewee: Okay, so I'll give you an example where we use computer vision as well as natural language processing in the same project. So it's super Volta only I worked in a project called Nova. So in that project what we what we do is that we take the floor plan image from a user and from taking the Fortran image from a user we try to do a performer multi-level segmentation multiplayer segmentation. So that gives us the different rooms. I mean, which one is the living room? Which one is a dining room? Which one is the balcony which is a wall. So all these classes we try to extract but performing a segmentation. I mean, we have modified the cubic we have referred to the previous set of modifications to come up with this modern then what we did. Is there after getting all the different classes then we have used the opening a pipelines. I mean by plans to make some predictions to give some recommendations to the user based on the layout of the room and also place the products with some strategy in the room. So the product placement as well as the recommendation of the new newer product based on the products that are already in the room as well as the orientation of the room the space available on this thing. So this is one of the things we did when we integrated both natural numbers processing and competition and I think it is one of the main one of the most important thing because you cannot expect a single project line only one single domain integrating different from And domains and then coming up with a particular use case. That is very that could be very strong for our organization. So always helpful.",3,"Turning to a different aspect of your experience, you've worked on projects involving both Natural Language Processing (NLP) and Computer Vision (CV). How do you approach the integration of NLP and CV in a project, and what are some key challenges you've encountered in this integration?","Okay, so I'll give you an example where we use computer vision as well as natural language processing in the same project. So it's super Volta only I worked in a project called Nova. So in that project what we what we do is that we take the floor plan image from a user and from taking the Fortran image from a user we try to do a performer multi-level segmentation multiplayer segmentation. So that gives us the different rooms. I mean, which one is the living room? Which one is a dining room? Which one is the balcony which is a wall. So all these classes we try to extract but performing a segmentation. I mean, we have modified the cubic we have referred to the previous set of modifications to come up with this modern then what we did. Is there after getting all the different classes then we have used the opening a pipelines. I mean by plans to make some predictions to give some recommendations to the user based on the layout of the room and also place the products with some strategy in the room. So the product placement as well as the recommendation of the new newer product based on the products that are already in the room as well as the orientation of the room the space available on this thing. So this is one of the things we did when we integrated both natural numbers processing and competition and I think it is one of the main one of the most important thing because you cannot expect a single project line only one single domain integrating different from And domains and then coming up with a particular use case. That is very that could be very strong for our organization. So always helpful.",3,True
328,328,328,328,"Interviewer: Absolutely, please go ahead and complete your explanation of the second experiment. It's important to understand the full scope of your research and how it informed your approach to NLP challenges. 

Interviewee: Thank you, very good. So my second experiment involved something called Mirror Vic. So that was a fine tuning technique, which is closely related to language modeling which forces the model to identify deeper meanings of words. Okay. So what we do is we you know, We actually Target a single word in a sentence to learn the deeper, meaning of then mask some random other words in the same sentence and then, you know try to get a word in context representation of the word or a word in context embedding of the word. So the embedding of the word but in that context, right so this concept I extended it to idioms. So I I replace idioms with a single, you know new token that I personally created. So because that's a research that shows that idioms are treated independently other than you know pieces, you know, idioms are handled as independent tokens rather than you know, aggregate of other words, right? That is how Transformers usually try to learn ads and that is how humans actually understand. And idioms also, so I first introduced a single token in place of the idiom and then try to learn the meaning of it through this mirror week technique. So in the where we technique what we did was I masked the idiom token. Okay, so I masked the idiom token. And then tried to learn the meaning of it through context. Okay, so if the idiom is used in if the idiomatic phase potentially idiomatic phrase is used in a non-literal sense, then the embedding of it would be very different than when the idiom is used in a literal sense. Right? So we find it for language modeling using contrastive learning described in the middle and we found that it really really helps in. Let's say magnifying the performance of bird or you know, extending or increase in the performance of bird towards understanding idioms. So that was a very interesting. Mind and it really did you know. Help increase the recognition of idioms in Transformer models. So first experiment was to understand how not Transformers actually treat idioms as a part of a task or as part of language then the second experiment was to try to improve the performance of the model towards understanding idioms. That concludes my project. And thank you.",5,"Absolutely, please go ahead and complete your explanation of the second experiment. It's important to understand the full scope of your research and how it informed your approach to NLP challenges.","Thank you, very good. So my second experiment involved something called Mirror Vic. So that was a fine tuning technique, which is closely related to language modeling which forces the model to identify deeper meanings of words. Okay. So what we do is we you know, We actually Target a single word in a sentence to learn the deeper, meaning of then mask some random other words in the same sentence and then, you know try to get a word in context representation of the word or a word in context embedding of the word. So the embedding of the word but in that context, right so this concept I extended it to idioms. So I I replace idioms with a single, you know new token that I personally created. So because that's a research that shows that idioms are treated independently other than you know pieces, you know, idioms are handled as independent tokens rather than you know, aggregate of other words, right? That is how Transformers usually try to learn ads and that is how humans actually understand. And idioms also, so I first introduced a single token in place of the idiom and then try to learn the meaning of it through this mirror week technique. So in the where we technique what we did was I masked the idiom token. Okay, so I masked the idiom token. And then tried to learn the meaning of it through context. Okay, so if the idiom is used in if the idiomatic phase potentially idiomatic phrase is used in a non-literal sense, then the embedding of it would be very different than when the idiom is used in a literal sense. Right? So we find it for language modeling using contrastive learning described in the middle and we found that it really really helps in. Let's say magnifying the performance of bird or you know, extending or increase in the performance of bird towards understanding idioms. So that was a very interesting. Mind and it really did you know. Help increase the recognition of idioms in Transformer models. So first experiment was to understand how not Transformers actually treat idioms as a part of a task or as part of language then the second experiment was to try to improve the performance of the model towards understanding idioms. That concludes my project. And thank you.",4,True
341,341,341,341,"Interviewer: Your approach to cross-domain validation and the use of advanced models for bias detection is quite innovative. How do you plan to integrate these validation and bias detection processes into the system's regular workflow to ensure ongoing reliability and fairness in the responses it provides?  


Interviewee: Okay, so there is one thing called self-consistency that can be used. And using self-consistency prompting one of the really interesting things is that you can calculate Confidence Code which helps you to understand whether how much exactly a model is confident about its answer. So that can be really helpful when deploying it for a larger audience. Another ways that can help to identify bias is there. So like there are other models. That can like a llama guard is one service that you can use to identify whether the answers that are being generated are does not have any taxi toxicity or are not causing any harm to humans or the people who are interacting with the model. but for financial question answering to identify bias, specifically, I think we will need to delve deeper into research and identify whether people have already worked on this problem or not and maybe 10 Here, I think I will have to look into a particular research papers to see exactly if you want to deploy such system on a large scale. What steps can be taken to mitigate bias. and skewness",3,Your approach to cross-domain validation and the use of advanced models for bias detection is quite innovative. How do you plan to integrate these validation and bias detection processes into the system's regular workflow to ensure ongoing reliability and fairness in the responses it provides?,"Okay, so there is one thing called self-consistency that can be used. And using self-consistency prompting one of the really interesting things is that you can calculate Confidence Code which helps you to understand whether how much exactly a model is confident about its answer. So that can be really helpful when deploying it for a larger audience. Another ways that can help to identify bias is there. So like there are other models. That can like a llama guard is one service that you can use to identify whether the answers that are being generated are does not have any taxi toxicity or are not causing any harm to humans or the people who are interacting with the model. but for financial question answering to identify bias, specifically, I think we will need to delve deeper into research and identify whether people have already worked on this problem or not and maybe 10 Here, I think I will have to look into a particular research papers to see exactly if you want to deploy such system on a large scale. What steps can be taken to mitigate bias. and skewness",3,True
344,344,344,344,"Interviewer: Given the need for extensive parameter sweeps and the use of wrapper scripts to automate the process, how did you ensure the reliability and accuracy of your results, especially considering the potential for errors in such a large-scale and automated testing environment? Additionally, can you discuss any specific validation or verification techniques you employed to confirm the integrity of the data generated by these automated parameter sweeps? 


Interviewee: So like I said, we first define the the default configuration and these were the baseline scores which we trusted. These base scores were generated by manually running the script. And hence, now that I have a baseline score, which is one data point, which was again, the like, acquired by doing multiple iterations and taking the mean and considering the COVID in the data. I defined that this is my minimum performance which I should be getting. Now every sweep which was done was matched with respect to this particular baseline score which I have. And that made sure that the data which I'm getting is coherent. Also, every sweep which was done was done in multiple iterations. And the mean and covariance of each iteration was, of each data point was considered. To make sure that the data is valid. Also, the background knowledge of computer architecture helped in understanding if the why a particular, configuration is giving me better or worse performance. And every per every observation which was made was correlated with the data, background knowledge of computer architecture, which I have.",3,"Given the need for extensive parameter sweeps and the use of wrapper scripts to automate the process, how did you ensure the reliability and accuracy of your results, especially considering the potential for errors in such a large-scale and automated testing environment? Additionally, can you discuss any specific validation or verification techniques you employed to confirm the integrity of the data generated by these automated parameter sweeps?","So like I said, we first define the the default configuration and these were the baseline scores which we trusted. These base scores were generated by manually running the script. And hence, now that I have a baseline score, which is one data point, which was again, the like, acquired by doing multiple iterations and taking the mean and considering the COVID in the data. I defined that this is my minimum performance which I should be getting. Now every sweep which was done was matched with respect to this particular baseline score which I have. And that made sure that the data which I'm getting is coherent. Also, every sweep which was done was done in multiple iterations. And the mean and covariance of each iteration was, of each data point was considered. To make sure that the data is valid. Also, the background knowledge of computer architecture helped in understanding if the why a particular, configuration is giving me better or worse performance. And every per every observation which was made was correlated with the data, background knowledge of computer architecture, which I have.",3,True
350,350,350,350,"Interviewer: Great! I see from your resume that you've completed a project called Gesture Detector using Python and OpenCV. Could you walk me through the process you followed to detect and recognize gestures, and how you handled the real-time aspect of this application?  

Interviewee: Here so basically use a python Library known as media pipe, which is used in opencv. So what it does it detects some hand marks on your on your palm. So basically there are 21 hand marks. So if I will show you so the basically there are 21 hand marks on top starting from 0 to 21. So according to this the landmarks, we have created with functional dependency program, which will does this will be working on 2D coordinate system. So according to the coordinate system the all the letters of the English alphabet has been defined on the palm and if we do some proper signature there I have been working on American Sign Language. So that different sign language is for different letters. So we have input each of the letter and each of the coordinate for a specific letter and according to that in the Dynamically in live capture feed we are recognizing the letters as well as for more of the input process variable. So convert to take it to the next step and we have also tried to build a word former and according to that. We are facing screen on Life field and showing the letters and we can form the word.",3,"Great! I see from your resume that you've completed a project called Gesture Detector using Python and OpenCV. Could you walk me through the process you followed to detect and recognize gestures, and how you handled the real-time aspect of this application?","Here so basically use a python Library known as media pipe, which is used in opencv. So what it does it detects some hand marks on your on your palm. So basically there are 21 hand marks. So if I will show you so the basically there are 21 hand marks on top starting from 0 to 21. So according to this the landmarks, we have created with functional dependency program, which will does this will be working on 2D coordinate system. So according to the coordinate system the all the letters of the English alphabet has been defined on the palm and if we do some proper signature there I have been working on American Sign Language. So that different sign language is for different letters. So we have input each of the letter and each of the coordinate for a specific letter and according to that in the Dynamically in live capture feed we are recognizing the letters as well as for more of the input process variable. So convert to take it to the next step and we have also tried to build a word former and according to that. We are facing screen on Life field and showing the letters and we can form the word.",2,True
353,353,353,353,"Interviewer: Understood, it sounds like you were able to manage the computational load within reasonable limits for that project. Let's shift focus to your experience with reinforcement learning. You mentioned extending the Sampled Policy Gradient algorithm for continuous control tasks. Can you walk me through the specific modifications you made to SPG and how these changes improved its performance or stability in your experiments?  

Interviewee: Yes. So reinforcement learning is something that I love and I would like to keep talking about reinforcement learning in this particular interview. Number one SPG is a sampled policy gradient. So essentially how it works is In a really fpg is an actor critic, you know architecture. So at every instance when the actor has to choose a particular action in SPG, what we do is we sample actions by adding noise to The Chosen action. And then we see with the Q value of the chosen action and the sample action. And we compare which has a higher Q value and then we choose that particular action as our actors action and environment. This is the underlying of working of SVG. But the issue with SPG was when we have a continuous action space the you know, the number of actions that you need to choose from is very high. so when we are doing this the issue was SPG did not converge very well. So I explored I explored two things one as I proposed a new quantile truncated distribution reinforcement learning where we have a distributional learning curve that will help us in converging faster and quicker. However, this takes more computational resources.",4,"Understood, it sounds like you were able to manage the computational load within reasonable limits for that project. Let's shift focus to your experience with reinforcement learning. You mentioned extending the Sampled Policy Gradient algorithm for continuous control tasks. Can you walk me through the specific modifications you made to SPG and how these changes improved its performance or stability in your experiments?","Yes. So reinforcement learning is something that I love and I would like to keep talking about reinforcement learning in this particular interview. Number one SPG is a sampled policy gradient. So essentially how it works is In a really fpg is an actor critic, you know architecture. So at every instance when the actor has to choose a particular action in SPG, what we do is we sample actions by adding noise to The Chosen action. And then we see with the Q value of the chosen action and the sample action. And we compare which has a higher Q value and then we choose that particular action as our actors action and environment. This is the underlying of working of SVG. But the issue with SPG was when we have a continuous action space the you know, the number of actions that you need to choose from is very high. so when we are doing this the issue was SPG did not converge very well. So I explored I explored two things one as I proposed a new quantile truncated distribution reinforcement learning where we have a distributional learning curve that will help us in converging faster and quicker. However, this takes more computational resources.",3,True
359,359,359,359,"Interviewer: How do you approach the evaluation of model performance in terms of accuracy and latency, and what metrics do you prioritize to ensure that the system maintains a high level of accuracy while also meeting the latency requirements for real-time or near real-time applications?  

Interviewee: It so the when evaluating mobile performance for real-time or near real-time applications. I adopt a comprehensive approach that considers both accuracy and latency Matrix. So like accuracy metrics were classification accuracy assessing the model's ability to correctly classify handwritten text inputs. Then whatever calculating the rate of errors in recognized words compared to the ground growth. Then character similar to the ear and CEO and similar to wscr calculates the rate of Heritage at correct level providing insights into the accurate of individual characters recognized by the system. Then latency metrics like inference time. It measures that I'm taking for the model to process a single input and produce the corresponding output and to end link and see evaluate the total time elapsed from when the user inputs and it didn't text to when the system generates the recognition result. Then we have analysis like aggregated of conduct. It conducted of analysis to balance accuracy and latency requirements based on the application needs and his performance Divinity evaluating the stability of Performance Marketing metrics across different input conditions. Like then we have there is device benchmarking testing the performance of handwritten text recognition system across the range of mobile devices with varying Hardware specification that was important, like people have different smartphones. So yeah. So by prioritizing accuracy and latency Matrix conducting trade-off analysis and bridging Benchmark and profiling techniques. I showed that the handwritten text recognition system maintains high performance and mobile devices while meeting the demands of real time on near real-time applications. Thank you.",3,"How do you approach the evaluation of model performance in terms of accuracy and latency, and what metrics do you prioritize to ensure that the system maintains a high level of accuracy while also meeting the latency requirements for real-time or near real-time applications?","It so the when evaluating mobile performance for real-time or near real-time applications. I adopt a comprehensive approach that considers both accuracy and latency Matrix. So like accuracy metrics were classification accuracy assessing the model's ability to correctly classify handwritten text inputs. Then whatever calculating the rate of errors in recognized words compared to the ground growth. Then character similar to the ear and CEO and similar to wscr calculates the rate of Heritage at correct level providing insights into the accurate of individual characters recognized by the system. Then latency metrics like inference time. It measures that I'm taking for the model to process a single input and produce the corresponding output and to end link and see evaluate the total time elapsed from when the user inputs and it didn't text to when the system generates the recognition result. Then we have analysis like aggregated of conduct. It conducted of analysis to balance accuracy and latency requirements based on the application needs and his performance Divinity evaluating the stability of Performance Marketing metrics across different input conditions. Like then we have there is device benchmarking testing the performance of handwritten text recognition system across the range of mobile devices with varying Hardware specification that was important, like people have different smartphones. So yeah. So by prioritizing accuracy and latency Matrix conducting trade-off analysis and bridging Benchmark and profiling techniques. I showed that the handwritten text recognition system maintains high performance and mobile devices while meeting the demands of real time on near real-time applications. Thank you.",2,True
363,363,363,363,"Interviewer: Data augmentation is indeed a powerful technique for addressing the limitations of a small dataset. Now, let's shift focus to your experience with PyTorch. Can you tell me about a specific project where you used PyTorch and what led you to choose PyTorch over other frameworks for that particular application?  

Interviewee: I have always preferred pytorch, even though I start with tensorflow. I gradually went on to use Python for all of my projects. So majority projects are done in pytorch, including the medical image analysis with Pytorch in my segmentation pipeline stood up later on as you've done with pytorch my llm trainings and the other models that have done. So the research paper server worked on everything. They have implemented in pytorch. And the main reason that I levitated towards my torch is the amount of Freedom that it gives you Well tensorflow is very high level abstracted. It's very useful for people who are beginning to learn deep learning and that's great because they can just dive deeper into it, but I thought you will have to End up defining every single moment. Like for example, you are a kind of defining all the Lost calculated and after that you have to end up deciding dot step backward so you can calculate the loss and so on and I find this very useful because with this fundamentals I can end up creating whatever kind of Deep Web block I want and this gets me a lot of flexibility, even though there's a bit of learning curve. The second reason is the Frameworks. I very frequently use fast Ai and pytorch lightning in my research project, especially when I'm at a model. This is allows me for quick iteration quick training quick ideation of thoughts. So this makes me very efficient in that manner.",4,"Data augmentation is indeed a powerful technique for addressing the limitations of a small dataset. Now, let's shift focus to your experience with PyTorch. Can you tell me about a specific project where you used PyTorch and what led you to choose PyTorch over other frameworks for that particular application?","I have always preferred pytorch, even though I start with tensorflow. I gradually went on to use Python for all of my projects. So majority projects are done in pytorch, including the medical image analysis with Pytorch in my segmentation pipeline stood up later on as you've done with pytorch my llm trainings and the other models that have done. So the research paper server worked on everything. They have implemented in pytorch. And the main reason that I levitated towards my torch is the amount of Freedom that it gives you Well tensorflow is very high level abstracted. It's very useful for people who are beginning to learn deep learning and that's great because they can just dive deeper into it, but I thought you will have to End up defining every single moment. Like for example, you are a kind of defining all the Lost calculated and after that you have to end up deciding dot step backward so you can calculate the loss and so on and I find this very useful because with this fundamentals I can end up creating whatever kind of Deep Web block I want and this gets me a lot of flexibility, even though there's a bit of learning curve. The second reason is the Frameworks. I very frequently use fast Ai and pytorch lightning in my research project, especially when I'm at a model. This is allows me for quick iteration quick training quick ideation of thoughts. So this makes me very efficient in that manner.",3,True
369,369,369,369,"Interviewer: Given your experience with both pre-trained models and fine-tuning, how do you approach the trade-off between model accuracy and computational efficiency, especially in the context of the fish freshness detection system where real-time or near real-time processing might be necessary?  

Interviewee: Oh, so when considering the trade of between model accuracy and computational resources, we follow a systematic approach to ensure Optimal Performance while managing resource constraints effectively. So like model selection We Begin by selecting model architecture the strikes a balance between accuracy and computational efficiency. This involves evaluating various models based on their like, you know. What was the complexity number of parameters and inference time? We prioritize models. That offer competitive performance on the task at hand. Then quantifying the source requirements. We quantify the competition resources required for training in France and deployment of each model. This includes considerations such as memory usage gpus CPU, utilization and inference speed. Finding and optimization was also a major factor like we explore techniques for fine tuning and optimizing model parameters to improve accuracy while mitigating competition overhead. So overall our approach involves careful consideration of modern context City performance metrics and competition requirements to strike an optimal balance between accuracy and resource efficiency for the event tasks and deployment environment. Just as we did with a Precision fish farming project using ml. Yeah.",3,"Given your experience with both pre-trained models and fine-tuning, how do you approach the trade-off between model accuracy and computational efficiency, especially in the context of the fish freshness detection system where real-time or near real-time processing might be necessary?","Oh, so when considering the trade of between model accuracy and computational resources, we follow a systematic approach to ensure Optimal Performance while managing resource constraints effectively. So like model selection We Begin by selecting model architecture the strikes a balance between accuracy and computational efficiency. This involves evaluating various models based on their like, you know. What was the complexity number of parameters and inference time? We prioritize models. That offer competitive performance on the task at hand. Then quantifying the source requirements. We quantify the competition resources required for training in France and deployment of each model. This includes considerations such as memory usage gpus CPU, utilization and inference speed. Finding and optimization was also a major factor like we explore techniques for fine tuning and optimizing model parameters to improve accuracy while mitigating competition overhead. So overall our approach involves careful consideration of modern context City performance metrics and competition requirements to strike an optimal balance between accuracy and resource efficiency for the event tasks and deployment environment. Just as we did with a Precision fish farming project using ml. Yeah.",3,True
371,371,371,371,"Interviewer: Understood, let's continue. Could you explain the approach you took to implement Continuous Training Pipeline Architecture for AI models at Phenom? Specifically, how did you manage model versioning, data drift, and concept drift in this pipeline?  

Interviewee: So basically what we try to do is we try to Leverage The AWS tools like Crossing job training job and it's just like we created a pipeline where we try to fetch the data from different we create the logic for it and create from that. We just make the code with the logic and create an ECR Docker image out of it. So for the crossing as well as the training job, so what we will do is whenever we see there is any issue like drift happened basically, like if there is any features drift happen, we use like statistical test like kale Divergence test. So this helps us to find the drift which happened in each of the feature. So if the number of features were high, Drift happened so we can say there is a data trip happened. Similarly. There is a more we try to find the metric often East Batcher and the metric is been declining for some particular batches. We can say like a model drift has happened. So if anyone of the drift happened what will lose you'll try to trigger this continuous training pipeline where already we have our courts ready, right Crossing job and training job. So we use this AWS processing up by triggering using this Docker image. So once the processing job is completed then we will just trigger the training job. So this training job. What it will do is it will try to take the model which was locked in the ml flow. So and it will load the model and just fine tune it with this new data, which came from the production. Once we find you need right? We'll just again do this validation on some of the new production data, which is not used for the training and we just check the production model as well as the newly trained model and will decide based on the metric. So if you are currently trained model metric is better, what will do is we will just log this model in mlflow and register it with the newer version. So this is how exactly the city pipe Print Works",4,"Understood, let's continue. Could you explain the approach you took to implement Continuous Training Pipeline Architecture for AI models at Phenom? Specifically, how did you manage model versioning, data drift, and concept drift in this pipeline?","So basically what we try to do is we try to Leverage The AWS tools like Crossing job training job and it's just like we created a pipeline where we try to fetch the data from different we create the logic for it and create from that. We just make the code with the logic and create an ECR Docker image out of it. So for the crossing as well as the training job, so what we will do is whenever we see there is any issue like drift happened basically, like if there is any features drift happen, we use like statistical test like kale Divergence test. So this helps us to find the drift which happened in each of the feature. So if the number of features were high, Drift happened so we can say there is a data trip happened. Similarly. There is a more we try to find the metric often East Batcher and the metric is been declining for some particular batches. We can say like a model drift has happened. So if anyone of the drift happened what will lose you'll try to trigger this continuous training pipeline where already we have our courts ready, right Crossing job and training job. So we use this AWS processing up by triggering using this Docker image. So once the processing job is completed then we will just trigger the training job. So this training job. What it will do is it will try to take the model which was locked in the ml flow. So and it will load the model and just fine tune it with this new data, which came from the production. Once we find you need right? We'll just again do this validation on some of the new production data, which is not used for the training and we just check the production model as well as the newly trained model and will decide based on the metric. So if you are currently trained model metric is better, what will do is we will just log this model in mlflow and register it with the newer version. So this is how exactly the city pipe Print Works",3,True
380,380,380,380,"Interviewer: Vision Transformers are relatively new in the field of computer vision. Could you discuss the challenges you faced while working with Vision Transformer models, particularly in terms of data preprocessing, model tuning, or any unexpected hurdles that arose during your project? How did you overcome these challenges?  

Interviewee: The most important part of the vision Transformers it requires a huge amount of label data, but as I was working on an imaginate model where I need to classify an image into 10 different types of classes, so one thing that actually instead of convolutional neural networks, the image Transformers were better is the vision Transformer actually splits and image into a different segments or like Subs parts of images. Now these parts are encoded using Transformer similar to how we use a Bird Transformer for word embeddings. So these Vision embeddings are captured was this Vision embeddings are captured positions of those regions embedding are considered to get a good like to extract different features the local features and the global features from the images. That's the main tasks that helps which are Transformers a super performance also to achieve a good performance for vision Transformer. The most important part is for data preprocessing. So I processed data each and every image to 224 by 2224 Dimension with three channels that are GP. So overall shape of the image was to 24 by 2224 by 3 with this the image was rescaled to 224 by 2224 and was padded accordingly. if not, the model was also trained and it model was the modest data was splitted into three subsets that is model training validation and model test set with pytorch as a framework model training and validation was based on the models loss which was cross entropy loss function which helped in getting a very low score with using Adams optimizer. initialized by pythons as well",3,"Vision Transformers are relatively new in the field of computer vision. Could you discuss the challenges you faced while working with Vision Transformer models, particularly in terms of data preprocessing, model tuning, or any unexpected hurdles that arose during your project? How did you overcome these challenges?","The most important part of the vision Transformers it requires a huge amount of label data, but as I was working on an imaginate model where I need to classify an image into 10 different types of classes, so one thing that actually instead of convolutional neural networks, the image Transformers were better is the vision Transformer actually splits and image into a different segments or like Subs parts of images. Now these parts are encoded using Transformer similar to how we use a Bird Transformer for word embeddings. So these Vision embeddings are captured was this Vision embeddings are captured positions of those regions embedding are considered to get a good like to extract different features the local features and the global features from the images. That's the main tasks that helps which are Transformers a super performance also to achieve a good performance for vision Transformer. The most important part is for data preprocessing. So I processed data each and every image to 224 by 2224 Dimension with three channels that are GP. So overall shape of the image was to 24 by 2224 by 3 with this the image was rescaled to 224 by 2224 and was padded accordingly. if not, the model was also trained and it model was the modest data was splitted into three subsets that is model training validation and model test set with pytorch as a framework model training and validation was based on the models loss which was cross entropy loss function which helped in getting a very low score with using Adams optimizer. initialized by pythons as well",3,True
383,383,383,383,"Interviewer: Your methodical approach to model selection and feature engineering is impressive. How do you envision leveraging more advanced techniques, such as neural networks or ensemble methods like XGBoost, in your future projects to further enhance predictive accuracy or solve more complex problems?  

Interviewee: I am coming to it right now. I am in my fourth year of second semester currently and I am working on a project. It is a leukemia detection classification project here. We have a picture of 10661 images of micro micro pictures of the blood cells here. We have only two labels one nine zero one in the sense. The cancer cells are the black cells 0 in a sense mLs are the cells that are normal. I am predicting if the patient whether has a cancer or not using these using the photos of the microwave pics of their cells, which are Inked by violent violent or else you can set die here. I'm using a fight torch that is a framework that we actually that we famously use we have Keras and fight of both of them but we use pythons because we can customize our neural networks in the pi torch and I have done it using mobile. Sorry in Mobile. I have got a 98% of accuracy which is a higher than the current or else the current literature review model right now. It is 95.75 station for the current model, but I have increased it by 98% days here in leukemia sells. I have a face a lots of problem in data preprocessing. As these cells are very identical to each other. They are very ambiguous. To remove that ambiguity I use data generator for a taking the classes and of data preprocessing was a very big hard in the initial stages, but eventually I have after using all the data preprocessing methods like data generator is more and other things. I have a process the data very much and the ambiguity of the data is decreased a lot. Then I have used mobile later V2, which is a very famous architecture of neural networks. And I have also added a few other layers like dense and Dropout at the end we give you the softmax layer, which is activation layer with the loss of loss of binary cross entropy as we are using as we want to get output of one in 0 only and I have used SGD Optimizer Socastee gradient descent for the better prediction of the blood cell after this. I have completed my model. I am right now over working on my jio orange spider or else flask and other Frameworks for my and I am really glad that I have this project is a very opening thing and it has been a great thing for me to learn from computer vision projects that",4,"Your methodical approach to model selection and feature engineering is impressive. How do you envision leveraging more advanced techniques, such as neural networks or ensemble methods like XGBoost, in your future projects to further enhance predictive accuracy or solve more complex problems?","I am coming to it right now. I am in my fourth year of second semester currently and I am working on a project. It is a leukemia detection classification project here. We have a picture of 10661 images of micro micro pictures of the blood cells here. We have only two labels one nine zero one in the sense. The cancer cells are the black cells 0 in a sense mLs are the cells that are normal. I am predicting if the patient whether has a cancer or not using these using the photos of the microwave pics of their cells, which are Inked by violent violent or else you can set die here. I'm using a fight torch that is a framework that we actually that we famously use we have Keras and fight of both of them but we use pythons because we can customize our neural networks in the pi torch and I have done it using mobile. Sorry in Mobile. I have got a 98% of accuracy which is a higher than the current or else the current literature review model right now. It is 95.75 station for the current model, but I have increased it by 98% days here in leukemia sells. I have a face a lots of problem in data preprocessing. As these cells are very identical to each other. They are very ambiguous. To remove that ambiguity I use data generator for a taking the classes and of data preprocessing was a very big hard in the initial stages, but eventually I have after using all the data preprocessing methods like data generator is more and other things. I have a process the data very much and the ambiguity of the data is decreased a lot. Then I have used mobile later V2, which is a very famous architecture of neural networks. And I have also added a few other layers like dense and Dropout at the end we give you the softmax layer, which is activation layer with the loss of loss of binary cross entropy as we are using as we want to get output of one in 0 only and I have used SGD Optimizer Socastee gradient descent for the better prediction of the blood cell after this. I have completed my model. I am right now over working on my jio orange spider or else flask and other Frameworks for my and I am really glad that I have this project is a very opening thing and it has been a great thing for me to learn from computer vision projects that",4,True
388,388,388,388,"Interviewer: It sounds like you had a comprehensive experience with Azure and C#. Given your proficiency in C#, how do you ensure that your code is both efficient and maintainable, especially when working with a complex system like Azure's billing? Can you also discuss any design patterns or best practices you follow in your C# development?  

Interviewee: Um, definitely one of the design patterns that I learned first of all was dependence injection. That was one of the things that I learned to use a working with NET Framework. And another thing was that I had to build my own classes. Like I had the original the whole code of this of the billing pipeline or part of your departments and what I basically did was create a new class which had which which consisted of my future what I was actually trying to do to test and make sure that they are very Loosely coupled because we do not want one of the object to be very dependent heavily dependent on the other object. So if you made changes in one of them it would you know, Some of the problems in future, so I had to make sure they are very Loosely coupled and it is very flexible to make a very flexible in order to make any further changes so that if my POC Works while the production is going on even during production, they come up across some new ways of like filtering like in my base is was kind of trying to filter our edges devices information before we are proceeding to having them build we had to know from which region they had were coming through which tenants are there all of that and initially we had to know for testing purposes. We had selected a particular region. We had selected some particular tenants for our test for purposes and later on if it was successful, we would try to include more regions more tenants and tour so that changing should have been very Easy and should have much it should have been done without making very large changes to the main code. If there's very Loosely couple you need to really make change at one place. And that is how I describe how efficient like how flexible a code can be and have differently speaking about the efficiency like after all if I have made any changes I have to test if all it hasn't negatively impacted any code and definitely and once I've done the whole setting of the code, I'll make sure if there's anything I can do to optimize the code for its speed if there is any latency or any kind of like all of the data section algorithms that I've learned on try to see what can be used to optimize where and where am I using any some operation that is just using a little more time than it can be actually done in a more efficient way. So in that way, yeah.",3,"It sounds like you had a comprehensive experience with Azure and C#. Given your proficiency in C#, how do you ensure that your code is both efficient and maintainable, especially when working with a complex system like Azure's billing? Can you also discuss any design patterns or best practices you follow in your C# development?","Um, definitely one of the design patterns that I learned first of all was dependence injection. That was one of the things that I learned to use a working with NET Framework. And another thing was that I had to build my own classes. Like I had the original the whole code of this of the billing pipeline or part of your departments and what I basically did was create a new class which had which which consisted of my future what I was actually trying to do to test and make sure that they are very Loosely coupled because we do not want one of the object to be very dependent heavily dependent on the other object. So if you made changes in one of them it would you know, Some of the problems in future, so I had to make sure they are very Loosely coupled and it is very flexible to make a very flexible in order to make any further changes so that if my POC Works while the production is going on even during production, they come up across some new ways of like filtering like in my base is was kind of trying to filter our edges devices information before we are proceeding to having them build we had to know from which region they had were coming through which tenants are there all of that and initially we had to know for testing purposes. We had selected a particular region. We had selected some particular tenants for our test for purposes and later on if it was successful, we would try to include more regions more tenants and tour so that changing should have been very Easy and should have much it should have been done without making very large changes to the main code. If there's very Loosely couple you need to really make change at one place. And that is how I describe how efficient like how flexible a code can be and have differently speaking about the efficiency like after all if I have made any changes I have to test if all it hasn't negatively impacted any code and definitely and once I've done the whole setting of the code, I'll make sure if there's anything I can do to optimize the code for its speed if there is any latency or any kind of like all of the data section algorithms that I've learned on try to see what can be used to optimize where and where am I using any some operation that is just using a little more time than it can be actually done in a more efficient way. So in that way, yeah.",3,True
411,411,411,411,"Interviewer: That's a clever way to ensure data consistency when dealing with dynamic content. Now, considering your experience with NLP, could you walk me through how you approached the challenge of response generation for pandemic-related queries? What NLP techniques or models did you find most effective for this task?  

Interviewee: Okay, so the NLP tasks that we were tackling in that project was that we had to data sets one was a user data set and one was an expert data set the expert data set considered consisted of long. explanatory answers to the questions while user data set consisted of concise answers given by Common People and hence was meant to show the approach or the thinking people had for the disease, so What had been done was the aim of the project was to extract explanatory and a single word answers while also correcting the common Notions that were present if they were wrong. So the main task was to detect whether the common notion is wrong and the other task was to give an actual answer based on the export a data set answers that have been given and the government So for this we had used the bm25 model for the first part of a court cleaning like from the big explanation paragraphs the top 10 tenses were used were extracted using the bm25 model. The Board model was then used to rank these top. 10 sentences and get the top 10 phrases that matched the queries answer after the word re-ranking. The bm25 model is again used to get the top five words so that we can give the concise answer and all the re-rank sentences that we have gotten from the Board model were used to give the exponent or answers. So this way both sort of answers will give in and the bm25 in the board models were found to be the most effective. So they were kept as a part of the final model. other techniques used were the basic tokenization and cleaning techniques from the energy toolkit available.",3,"That's a clever way to ensure data consistency when dealing with dynamic content. Now, considering your experience with NLP, could you walk me through how you approached the challenge of response generation for pandemic-related queries? What NLP techniques or models did you find most effective for this task?","Okay, so the NLP tasks that we were tackling in that project was that we had to data sets one was a user data set and one was an expert data set the expert data set considered consisted of long. explanatory answers to the questions while user data set consisted of concise answers given by Common People and hence was meant to show the approach or the thinking people had for the disease, so What had been done was the aim of the project was to extract explanatory and a single word answers while also correcting the common Notions that were present if they were wrong. So the main task was to detect whether the common notion is wrong and the other task was to give an actual answer based on the export a data set answers that have been given and the government So for this we had used the bm25 model for the first part of a court cleaning like from the big explanation paragraphs the top 10 tenses were used were extracted using the bm25 model. The Board model was then used to rank these top. 10 sentences and get the top 10 phrases that matched the queries answer after the word re-ranking. The bm25 model is again used to get the top five words so that we can give the concise answer and all the re-rank sentences that we have gotten from the Board model were used to give the exponent or answers. So this way both sort of answers will give in and the bm25 in the board models were found to be the most effective. So they were kept as a part of the final model. other techniques used were the basic tokenization and cleaning techniques from the energy toolkit available.",3,True
417,417,417,417,"Interviewer: Great! Let's dive into your resume. I see you led a project on the PUBG Match Outcome Predictor. Can you walk me through how you selected the features for your model and why you chose those particular algorithms (GBR, LGBM, CatBoost, Random Forest) for this prediction problem?  


Interviewee: Yes, sir. So I have I have found a pubg data set online and I have a domain knowledge in the specific game too. I know how the parameters work and the parameters where the you know, the basic game related mattresses and match outcomes like the weapons used the Number of players in the game the teams number of teams and the total distance traveled number of cares those kind of parameters. Were there including categorical features for Android phones. I Process pre-process the data I checked for missing values and anomalies because since I have the domain knowledge in the specific field, I know what is possible and what is not possible. So I have checked for the you know, identification of bots. So people play with bots. So I have checked because if the person is not moving, but they are killed rate. Which is the country that means yeah, they brought so that those columns need a clean and representative to get a good predicting model. And then I have subject to the data set through scaling and normalizing and then feature engineering I did with as I said handling anomalies and normalizing, you know, in some matches there are like 100 players and the number of kills will be 20 and there are matches where there are 20 players and the number of kills or 20. So the second scenario has more weightage than the first one so that should not be neglect negligible. Right? So I have included a normalization parameter in it and which ensured that all the features and trolls gets enough importance and also have dead. Hyper parameter to tuning in the models that I have mentioned in my resume, like, you know cat Bowl cat boost and random Forest. I have did hyper parameter tuning including grid search and first I have implemented in random way and but I did not get the good optimized results. So to overcome that I have included, you know grit search method which in which I have finally found the good parameters and That gave me the result of 82 percentage.",4,"Great! Let's dive into your resume. I see you led a project on the PUBG Match Outcome Predictor. Can you walk me through how you selected the features for your model and why you chose those particular algorithms (GBR, LGBM, CatBoost, Random Forest) for this prediction problem?","Yes, sir. So I have I have found a pubg data set online and I have a domain knowledge in the specific game too. I know how the parameters work and the parameters where the you know, the basic game related mattresses and match outcomes like the weapons used the Number of players in the game the teams number of teams and the total distance traveled number of cares those kind of parameters. Were there including categorical features for Android phones. I Process pre-process the data I checked for missing values and anomalies because since I have the domain knowledge in the specific field, I know what is possible and what is not possible. So I have checked for the you know, identification of bots. So people play with bots. So I have checked because if the person is not moving, but they are killed rate. Which is the country that means yeah, they brought so that those columns need a clean and representative to get a good predicting model. And then I have subject to the data set through scaling and normalizing and then feature engineering I did with as I said handling anomalies and normalizing, you know, in some matches there are like 100 players and the number of kills will be 20 and there are matches where there are 20 players and the number of kills or 20. So the second scenario has more weightage than the first one so that should not be neglect negligible. Right? So I have included a normalization parameter in it and which ensured that all the features and trolls gets enough importance and also have dead. Hyper parameter to tuning in the models that I have mentioned in my resume, like, you know cat Bowl cat boost and random Forest. I have did hyper parameter tuning including grid search and first I have implemented in random way and but I did not get the good optimized results. So to overcome that I have included, you know grit search method which in which I have finally found the good parameters and That gave me the result of 82 percentage.",3,True
439,439,439,439,"Interviewer: Creating a custom metric that aligns with the specific needs of your project is a smart approach. It's clear that you're focused on real-world performance. Shifting gears a bit, I noticed you also have experience with NLP and large language models. Can you discuss a project where you worked with NLP, particularly with large language models, and how you addressed any challenges related to model size or training data?  

Interviewee: Okay. All right. So NLP something which I which I love to do. I'm pretty I'm pretty confident to say that I am I have been doing this for the past few years. And for the past one year. I've been doing a lot of llm projects. So I'm pretty confident about the skill and I would love to talk more about this. You can ask any question in it. So yeah coming to your question the model size and training right? So model size as you said, you know this these are large language models and these models are pretty large. Let's say we have Falcon we have Mr. We have mixture now and now there are a lot of 70b models. So the model size is determined by the the parameter the parameters how many parameters the model has so there are seven billion model. There are 35 billion parameter models and they are 70 billion parameter models also. So we were in Middle where we were trying to do the 13th parameter model of llama. So we were using this. Kind of model and yeah, this is these are pretty large and how do we do it? How do we store in our own GPU is we do the quantization technique where we tried to sew one Precision is 32 bit float. What we did is we quantize this model to 4 bit so that we can reduce the memory for that large language model and try to optimize it and even though if we reduce these many Precision points, the accuracy was almost as far as the full precision. So we didn't also lose accuracy. Maybe there is a 0.1% of accuracy decreased but that really doesn't much matter because we wanted much more speeder and lighter kind of a model. That's how we deal with all these things and coming to training training these large language model. There are different accepts of there are a lot of different, you know ways to do it one is parameter efficient tuning test. There is Laura where we use Laura then there is Q Laura then there is I am also doing this there is supervised fine-tuning and there is also this DPO which is direct preference optimization. I have done a lot of a lot of These Fine tunings and trainings on top of it. So there are a lot of free training so which we use but we didn't pre-train the whole model because these models take a lot of time to train. So what we did is we try to take the already pre-trained model and we find tuned on top on top of it for our custom use case. So that's what we did.",5,"Creating a custom metric that aligns with the specific needs of your project is a smart approach. It's clear that you're focused on real-world performance. Shifting gears a bit, I noticed you also have experience with NLP and large language models. Can you discuss a project where you worked with NLP, particularly with large language models, and how you addressed any challenges related to model size or training data?","Okay. All right. So NLP something which I which I love to do. I'm pretty I'm pretty confident to say that I am I have been doing this for the past few years. And for the past one year. I've been doing a lot of llm projects. So I'm pretty confident about the skill and I would love to talk more about this. You can ask any question in it. So yeah coming to your question the model size and training right? So model size as you said, you know this these are large language models and these models are pretty large. Let's say we have Falcon we have Mr. We have mixture now and now there are a lot of 70b models. So the model size is determined by the the parameter the parameters how many parameters the model has so there are seven billion model. There are 35 billion parameter models and they are 70 billion parameter models also. So we were in Middle where we were trying to do the 13th parameter model of llama. So we were using this. Kind of model and yeah, this is these are pretty large and how do we do it? How do we store in our own GPU is we do the quantization technique where we tried to sew one Precision is 32 bit float. What we did is we quantize this model to 4 bit so that we can reduce the memory for that large language model and try to optimize it and even though if we reduce these many Precision points, the accuracy was almost as far as the full precision. So we didn't also lose accuracy. Maybe there is a 0.1% of accuracy decreased but that really doesn't much matter because we wanted much more speeder and lighter kind of a model. That's how we deal with all these things and coming to training training these large language model. There are different accepts of there are a lot of different, you know ways to do it one is parameter efficient tuning test. There is Laura where we use Laura then there is Q Laura then there is I am also doing this there is supervised fine-tuning and there is also this DPO which is direct preference optimization. I have done a lot of a lot of These Fine tunings and trainings on top of it. So there are a lot of free training so which we use but we didn't pre-train the whole model because these models take a lot of time to train. So what we did is we try to take the already pre-trained model and we find tuned on top on top of it for our custom use case. So that's what we did.",3,True
441,441,441,441,"Interviewer: That's great to hear about your experience with salient object detection. Could you elaborate on how you leveraged AWS for this project? For instance, which AWS services did you use for model training, deployment, and monitoring? And how did you ensure the security and scalability of your solution on AWS?  

Interviewee: Yeah, so for AWS, I used ec2 instance. So there I Wrote the whole pipeline for training and I deployed it. So first of all for the for all the testing purposes, I used AWS maker where I had access to gpus. So where I did all the testing I had data set collected from Amazon which had objects with the white background. So it was easier to create labels for those data sets. And after doing that it is it is a segmentation problem. Right? And the model that I used the more architecture. I used was unit plus plus which is the 5mm model which is very suitable for running on low end devices with no low RAM and I converted that after training the model which I for who's the code for which I wrote in Python in pytorch and uplifted I completed that into 1X so that after it when I deployed in the server, it does not have any pytorch or tensorflow dependency. So after converting into its I just did infants using opencv and yeah, so the use case here is just if you gave her input image of an object and the model will segregate the background from the foreground. So it will just crop your image and discard the background. So it is something like that. And for that I used ec2 instance where I deployed it. So it was one of the first projects I did. And as far as scalability is concerned and worked on that to be honest, and I basically couldn't API endpoint at ec2 instance and there. I just tested it tested it using Postman and then I forwarded that to my front end team who can access it. So yeah, that's all.",3,"That's great to hear about your experience with salient object detection. Could you elaborate on how you leveraged AWS for this project? For instance, which AWS services did you use for model training, deployment, and monitoring? And how did you ensure the security and scalability of your solution on AWS?","Yeah, so for AWS, I used ec2 instance. So there I Wrote the whole pipeline for training and I deployed it. So first of all for the for all the testing purposes, I used AWS maker where I had access to gpus. So where I did all the testing I had data set collected from Amazon which had objects with the white background. So it was easier to create labels for those data sets. And after doing that it is it is a segmentation problem. Right? And the model that I used the more architecture. I used was unit plus plus which is the 5mm model which is very suitable for running on low end devices with no low RAM and I converted that after training the model which I for who's the code for which I wrote in Python in pytorch and uplifted I completed that into 1X so that after it when I deployed in the server, it does not have any pytorch or tensorflow dependency. So after converting into its I just did infants using opencv and yeah, so the use case here is just if you gave her input image of an object and the model will segregate the background from the foreground. So it will just crop your image and discard the background. So it is something like that. And for that I used ec2 instance where I deployed it. So it was one of the first projects I did. And as far as scalability is concerned and worked on that to be honest, and I basically couldn't API endpoint at ec2 instance and there. I just tested it tested it using Postman and then I forwarded that to my front end team who can access it. So yeah, that's all.",3,True
445,445,445,445,"Interviewer: It's clear that Python has been a significant part of your work, especially with PySpark and Airflow for orchestration. Given your experience with Java, how do you compare its use in building scalable systems to Python, and can you share an instance where you chose Java over Python for a specific requirement? 

Interviewee: Yes, we have used a Java as well as python based on the use cases in my past project where we developed a cloud-based ETL tool. The primary data injection the trader transformation and execution of the badge jobs were returned in Python because like because of the application seems to be simple and that very much. Robustness for that reasons. We have choose a python for that. But when it comes to the testing and data parity checks the data parity check in the sense like the weather the pipeline work fine or not and also the data correctness and data validity checks were written in Java. We have choose a Java for that specific use case just because like it is very much useful in terms of having it multiple classes and object oriented programming. Which gave us a proper abstraction over the data? and also the inheritance And calling in between the classes this very much useful and can be implemented better in Java rather than python. So for this data quality and validation checks and also for the testing of our framework, we have returned the code in Java for that use case and the rest of the things based on Simplicity and Readiness of the spark framework with a python we have chosen python for that transformation and injections.",4,"It's clear that Python has been a significant part of your work, especially with PySpark and Airflow for orchestration. Given your experience with Java, how do you compare its use in building scalable systems to Python, and can you share an instance where you chose Java over Python for a specific requirement?","Yes, we have used a Java as well as python based on the use cases in my past project where we developed a cloud-based ETL tool. The primary data injection the trader transformation and execution of the badge jobs were returned in Python because like because of the application seems to be simple and that very much. Robustness for that reasons. We have choose a python for that. But when it comes to the testing and data parity checks the data parity check in the sense like the weather the pipeline work fine or not and also the data correctness and data validity checks were written in Java. We have choose a Java for that specific use case just because like it is very much useful in terms of having it multiple classes and object oriented programming. Which gave us a proper abstraction over the data? and also the inheritance And calling in between the classes this very much useful and can be implemented better in Java rather than python. So for this data quality and validation checks and also for the testing of our framework, we have returned the code in Java for that use case and the rest of the things based on Simplicity and Readiness of the spark framework with a python we have chosen python for that transformation and injections.",3,True
452,452,452,452,"Interviewer: Thank you for that. Now, how would successfully implementing this real-time sentiment analysis tool benefit brands and their online reputation management? 


Interviewee: Okay. So there is a there are some several ways it can help, like, proactive repetition management in which early detection of issues were there. Real time monitoring allows brands to quickly detect negative sentiment and emerging issues before they escalate. This enables timely intervention and resolution. And there is a crisis management also in there. Like, during a PR crisis, the tool can provide immediate feedback on the effectiveness of response strategy, helping to adapt quickly to mitigate damage. And there is 1 more, like, custom customer insights and engagement in which to understand the customer sentiments is important, where brands can gauge customers' feeling about products, services, and campaigns in real time, allowing them to understand what works and what doesn't. And in the competitive analysis, there are benchmarking. The tool can track sentiment not only for the brand itself, but also for competitors, providing valuable insights into relative performance and positioning. And there is a market trends also. By analyzing industry wide sentiments, brands can identify emerging trends customer preferences, and potential market opportunities. And in the marketing and campaign optimization, there is a real time feedback in which brands can receive immediate feedback on marketing campaigns allowing them to, adjust managing messaging and strategies on the fly to maximize the impact. And there is a target campaign also in which insights into customer sentiment can help in crafting more targeted and effective marketing campaigns that resonate with the audience. And there is 1 more, like, product development improvement in which the feature request and bug reports, in which analysis of customer feedback and high highlight common feature requests and bug reports guiding product development improvements. And, also, there is a user experience also in which understanding sentiment around user experience can help prioritize usability enhancements and resolve pain points. And there are more like a brand advocacy and influencer engagement strategic decision making, enhancing customer support, Like, example use case use cases if I can give you. Can I give you that?",3,"Thank you for that. Now, how would successfully implementing this real-time sentiment analysis tool benefit brands and their online reputation management?","Okay. So there is a there are some several ways it can help, like, proactive repetition management in which early detection of issues were there. Real time monitoring allows brands to quickly detect negative sentiment and emerging issues before they escalate. This enables timely intervention and resolution. And there is a crisis management also in there. Like, during a PR crisis, the tool can provide immediate feedback on the effectiveness of response strategy, helping to adapt quickly to mitigate damage. And there is 1 more, like, custom customer insights and engagement in which to understand the customer sentiments is important, where brands can gauge customers' feeling about products, services, and campaigns in real time, allowing them to understand what works and what doesn't. And in the competitive analysis, there are benchmarking. The tool can track sentiment not only for the brand itself, but also for competitors, providing valuable insights into relative performance and positioning. And there is a market trends also. By analyzing industry wide sentiments, brands can identify emerging trends customer preferences, and potential market opportunities. And in the marketing and campaign optimization, there is a real time feedback in which brands can receive immediate feedback on marketing campaigns allowing them to, adjust managing messaging and strategies on the fly to maximize the impact. And there is a target campaign also in which insights into customer sentiment can help in crafting more targeted and effective marketing campaigns that resonate with the audience. And there is 1 more, like, product development improvement in which the feature request and bug reports, in which analysis of customer feedback and high highlight common feature requests and bug reports guiding product development improvements. And, also, there is a user experience also in which understanding sentiment around user experience can help prioritize usability enhancements and resolve pain points. And there are more like a brand advocacy and influencer engagement strategic decision making, enhancing customer support, Like, example use case use cases if I can give you. Can I give you that?",3,True
468,468,468,468,"Interviewer: Fascinating approach to data security and training efficiency. Shifting focus to your AI model development, you mentioned using ResNet for pricing models. Can you discuss how you selected the architecture for the ResNet model? Specifically, what criteria did you consider to determine that ResNet was the most suitable architecture for your financial asset pricing model, and how did you handle model interpretability in the context of financial decision-making?  


Interviewee: So the residual Network or what I refer to as reznet contains skip connections which enable the solution to something called The Vanishing gradient problem. That is why we selected this architecture and because it's a regression problem where the output of the pricer is usually a quantitative value in dollar amount for an option. The accuracy of these numbers are very very relevant. So by employing the residual skip connections we were able to increase our capacity increase our accuracy. In terms of whether or not the architecture was viable. We would actually try to recreate the Monte Carlo based pricer as a result. So we knew exactly what the output should be and therefore we could train to a certain degree of accuracy and generate more data as needed until we achieve that accuracy via the Monte Carlo price.",4,"Fascinating approach to data security and training efficiency. Shifting focus to your AI model development, you mentioned using ResNet for pricing models. Can you discuss how you selected the architecture for the ResNet model? Specifically, what criteria did you consider to determine that ResNet was the most suitable architecture for your financial asset pricing model, and how did you handle model interpretability in the context of financial decision-making?",So the residual Network or what I refer to as reznet contains skip connections which enable the solution to something called The Vanishing gradient problem. That is why we selected this architecture and because it's a regression problem where the output of the pricer is usually a quantitative value in dollar amount for an option. The accuracy of these numbers are very very relevant. So by employing the residual skip connections we were able to increase our capacity increase our accuracy. In terms of whether or not the architecture was viable. We would actually try to recreate the Monte Carlo based pricer as a result. So we knew exactly what the output should be and therefore we could train to a certain degree of accuracy and generate more data as needed until we achieve that accuracy via the Monte Carlo price.,4,True
475,475,475,475,"Interviewer: I appreciate the detailed explanation of how you've leveraged transfer learning in your keyword recommendation system. Could you now discuss the challenges you might encounter when training a large language model and how you would address those challenges? 

Interviewee: Right. So in my experience, the the challenge is that I've observed when it comes to large language models is that not always a particular language model is appropriate for the use case that you are trying to deal with sometimes it is the model and sometimes it is the data that you have that you have come up with or the data that you have managed to scrape for this particular tasks. It becomes really vital to go through your data and perform a general Eda in the beginning to ensure that you have all relevant features with many a times these features or they are not transformed in a manner that will have meaningful contribution to the finding that your performing and that might lead to the data or the model massively underfitting based on the data. This is where we need to take into account whether our data is the most clean and appropriate and does not include a lot of garbage rows or conflicting roles in such scenarios. The training does not lead to a productive convergence and the debugging of it becomes a little difficult when it comes to fine-tuning the specific models. Some models are more retrained on. A different kind of data set than the one that you are utilizing. So it becomes important to pick up a model that is more suited to your context.",3,I appreciate the detailed explanation of how you've leveraged transfer learning in your keyword recommendation system. Could you now discuss the challenges you might encounter when training a large language model and how you would address those challenges?,"Right. So in my experience, the the challenge is that I've observed when it comes to large language models is that not always a particular language model is appropriate for the use case that you are trying to deal with sometimes it is the model and sometimes it is the data that you have that you have come up with or the data that you have managed to scrape for this particular tasks. It becomes really vital to go through your data and perform a general Eda in the beginning to ensure that you have all relevant features with many a times these features or they are not transformed in a manner that will have meaningful contribution to the finding that your performing and that might lead to the data or the model massively underfitting based on the data. This is where we need to take into account whether our data is the most clean and appropriate and does not include a lot of garbage rows or conflicting roles in such scenarios. The training does not lead to a productive convergence and the debugging of it becomes a little difficult when it comes to fine-tuning the specific models. Some models are more retrained on. A different kind of data set than the one that you are utilizing. So it becomes important to pick up a model that is more suited to your context.",3,True
489,489,489,489,"Interviewer: Your insights on query expansion and model enhancement are quite valuable. When it comes to integrating new datasets or models for query expansion and dynamic prompting, how do you manage the potential for bias or skewed focus in the retrieved documents or generated responses?  


Interviewee: Okay. Yeah, so identifying bias and retrieval tasks basically. Can be can be done using a different data set. So like for if we fine tune our retrieval model using fin QA we can also test it on another dataset that is conversational fin QA and see whether the after adding query expansion whether it is just helping retriever accuracy or retrieval scores on Finca or does it helping both in queue and conversational in QA? so if both if the new method better on both the data sets then and additional data sets like natural question answering and some other open domain question answering data sets. If the query expansion is help is actually really helpful. We will get better retrieval scores across all the data sets. So that could actually help us identify whether they just Q on specific to financial domain or not for model enhancement or from similar approach can be tried but I think a better approach will be sampling 100 or 200 answers generated from the model and going through the manually to understand whether model is taking character is reasoning steps or we can also do like Can use GPT 4 which are really Advanced language models to check the answers generated by. Our financial question answering model to check whether it is outputting skewed results or there is problem. There is a specific problem that is happening the model performance.",3,"Your insights on query expansion and model enhancement are quite valuable. When it comes to integrating new datasets or models for query expansion and dynamic prompting, how do you manage the potential for bias or skewed focus in the retrieved documents or generated responses?","Okay. Yeah, so identifying bias and retrieval tasks basically. Can be can be done using a different data set. So like for if we fine tune our retrieval model using fin QA we can also test it on another dataset that is conversational fin QA and see whether the after adding query expansion whether it is just helping retriever accuracy or retrieval scores on Finca or does it helping both in queue and conversational in QA? so if both if the new method better on both the data sets then and additional data sets like natural question answering and some other open domain question answering data sets. If the query expansion is help is actually really helpful. We will get better retrieval scores across all the data sets. So that could actually help us identify whether they just Q on specific to financial domain or not for model enhancement or from similar approach can be tried but I think a better approach will be sampling 100 or 200 answers generated from the model and going through the manually to understand whether model is taking character is reasoning steps or we can also do like Can use GPT 4 which are really Advanced language models to check the answers generated by. Our financial question answering model to check whether it is outputting skewed results or there is problem. There is a specific problem that is happening the model performance.",3,True
493,493,493,493,"Interviewer: That's impressive. Faster project delivery and reduced manual effort can indeed have a significant impact. Now, considering your background, what would be the key steps or milestones you would prioritize in implementing this proposed solution? 


Interviewee: Okay. So one would be the training pipeline process. So our first, pro would be identifying all the pain points, like how data drift could be, or is there any possibility of data drift? And, where are the points where the data may change significantly? So one is that. So another is identifying all the key aspects, like what are the things for the orthos that, may affect the model's detections, like I mentioned, GSD, So one would be the preprocessing. Next step would be the selecting the architecture for, detection models and segmentation model. Which model do we want to experiment with, which would be the best model that we want to, use for our use case and put that in production. And, finally, would be the evaluation. Like, if our model does not perform better than what we currently have or if it's inference speed is very low, that it won't make sense to put that into production. So the 3 things, one would be preprocess and identify identifying all the pinpoints, like, where the things may got go wrong and the data may get, changed or something like that. Next would be the model selection, like, which architecture we want to use. Again, the if the model is very large, it will take a time to infer and even train. So that thing. And another would be the evaluation part. So we want to have good evaluation metrics, like, what do we want to accomplish for that. So these easings.",3,"That's impressive. Faster project delivery and reduced manual effort can indeed have a significant impact. Now, considering your background, what would be the key steps or milestones you would prioritize in implementing this proposed solution?","Okay. So one would be the training pipeline process. So our first, pro would be identifying all the pain points, like how data drift could be, or is there any possibility of data drift? And, where are the points where the data may change significantly? So one is that. So another is identifying all the key aspects, like what are the things for the orthos that, may affect the model's detections, like I mentioned, GSD, So one would be the preprocessing. Next step would be the selecting the architecture for, detection models and segmentation model. Which model do we want to experiment with, which would be the best model that we want to, use for our use case and put that in production. And, finally, would be the evaluation. Like, if our model does not perform better than what we currently have or if it's inference speed is very low, that it won't make sense to put that into production. So the 3 things, one would be preprocess and identify identifying all the pinpoints, like, where the things may got go wrong and the data may get, changed or something like that. Next would be the model selection, like, which architecture we want to use. Again, the if the model is very large, it will take a time to infer and even train. So that thing. And another would be the evaluation part. So we want to have good evaluation metrics, like, what do we want to accomplish for that. So these easings.",3,True
503,503,503,503,"Interviewer: That's a comprehensive approach to enhancing search capabilities. With the transition to semantic search and the integration of BERT for contextual understanding, how do you measure and ensure the accuracy of the search results, especially with the dynamic nature of user queries and content updates?  

Interviewee: Up. So yeah, that's a good question. So again, we are using the MTV benchmarks that that are posted on the hugging face as our base understanding of Which models are doing better. Another thing that we have is we have a team a relevant C testing basically here. So we have this our own benchmarking system. Number one is manual, right? So we asked 10 or 15 users who are using the search. So just go through some queries and see how the results are coming and how the results are looking. That's number one second. We have again our road benchmarking system. We have this inbuilt in not inbuilt. We have our custom make algorithm which which we use on some customers data we have their Expected documents in Withers and the like expecting documents for these queries basically the top search results like that. So they have provided us this with so we test our hypothesis on it. And when we are showed like this could work and only then we are roll out the future.",3,"That's a comprehensive approach to enhancing search capabilities. With the transition to semantic search and the integration of BERT for contextual understanding, how do you measure and ensure the accuracy of the search results, especially with the dynamic nature of user queries and content updates?","Up. So yeah, that's a good question. So again, we are using the MTV benchmarks that that are posted on the hugging face as our base understanding of Which models are doing better. Another thing that we have is we have a team a relevant C testing basically here. So we have this our own benchmarking system. Number one is manual, right? So we asked 10 or 15 users who are using the search. So just go through some queries and see how the results are coming and how the results are looking. That's number one second. We have again our road benchmarking system. We have this inbuilt in not inbuilt. We have our custom make algorithm which which we use on some customers data we have their Expected documents in Withers and the like expecting documents for these queries basically the top search results like that. So they have provided us this with so we test our hypothesis on it. And when we are showed like this could work and only then we are roll out the future.",2,True
509,509,509,509,"Interviewer: Great! Can you tell me more about your experience developing the digit recognition model using TensorFlow and Keras at NeuroNexus Innovations? What were the key challenges you faced? 


Interviewee: Yeah. While the objective of my technical endeavor was to develop a machine learning model, that can accurately recognize handwritten digits from an image And, like, I used Python programming language for developing the project and various Python libraries such as NumPy, Pandas, MyBotClip, and Skit learn for data preprocessing visualization, and modeling. And for the digit recognition task, I used convolutional neural network, CNN, is a deep learning algorithm that works well with image data Specifically, I use the Keras frame framework to build the CNN model. And there are some challenges like, the biggest challenge in this project was to identifying the best CNN architecture and hyperparameters to achieve high accuracy in digit recognition. Additionally, preprocessing the data to remove noise and ensure uniformity across different images was also challenges. And during the project, I gained a deeper understanding of deep learning algorithm, specifically CNN. And how they can use, to solve image recognition tasks. I learned how to preprocess image data for training and testing and how to tune parameters, hyperparameters, to optimize model performance and how to evaluate model accuracy. And, there are some steps like data processing, model building, model compilation, model training, model evaluation, model saving, and model predictions.",3,Great! Can you tell me more about your experience developing the digit recognition model using TensorFlow and Keras at NeuroNexus Innovations? What were the key challenges you faced?,"Yeah. While the objective of my technical endeavor was to develop a machine learning model, that can accurately recognize handwritten digits from an image And, like, I used Python programming language for developing the project and various Python libraries such as NumPy, Pandas, MyBotClip, and Skit learn for data preprocessing visualization, and modeling. And for the digit recognition task, I used convolutional neural network, CNN, is a deep learning algorithm that works well with image data Specifically, I use the Keras frame framework to build the CNN model. And there are some challenges like, the biggest challenge in this project was to identifying the best CNN architecture and hyperparameters to achieve high accuracy in digit recognition. Additionally, preprocessing the data to remove noise and ensure uniformity across different images was also challenges. And during the project, I gained a deeper understanding of deep learning algorithm, specifically CNN. And how they can use, to solve image recognition tasks. I learned how to preprocess image data for training and testing and how to tune parameters, hyperparameters, to optimize model performance and how to evaluate model accuracy. And, there are some steps like data processing, model building, model compilation, model training, model evaluation, model saving, and model predictions.",4,True
518,518,518,518,"Interviewer: Implementing a streaming approach and hosting a dedicated server are both solid strategies for latency reduction. Given your experience with Azure and OpenAI, how comfortable are you with AWS services, particularly those that are relevant to ML and NLP workloads, such as SageMaker or Lambda? Can you describe a scenario where you've utilized AWS in your machine learning projects? 

Interviewee: Um, yeah, I I have used AWS sagemaker. Actually, we use AWS a lot in order to activities, even the chatbot that we have the that we host. We hosted on you know, ec2 and Docker and we you know serve multiple several thousand users a day in real time. So I have experience of deploying and scaling services on AWS. Secondly. I worked on AWS sagemaker towards mostly CI shirdi pipelines, so I have you know developed models then ran them or train them on ec2 by you know, spawning it from an AWS engagement. And then after the training is finished, I have successfully deployed it on ECR. So that is my experience is saying makeup. So I have done some work on sagemaker specifically. Lambda is not really. A familiar to me also I would say I know where it is used. But in my you know day-to-day activities, I really use Lambda so that we use Lambda as a company. So mostly our backend team handles it towards the you know, data refresher services and stuff like that. But I personally I deal with you know, ECR and sagemaker sometimes. but really",3,"Implementing a streaming approach and hosting a dedicated server are both solid strategies for latency reduction. Given your experience with Azure and OpenAI, how comfortable are you with AWS services, particularly those that are relevant to ML and NLP workloads, such as SageMaker or Lambda? Can you describe a scenario where you've utilized AWS in your machine learning projects?","Um, yeah, I I have used AWS sagemaker. Actually, we use AWS a lot in order to activities, even the chatbot that we have the that we host. We hosted on you know, ec2 and Docker and we you know serve multiple several thousand users a day in real time. So I have experience of deploying and scaling services on AWS. Secondly. I worked on AWS sagemaker towards mostly CI shirdi pipelines, so I have you know developed models then ran them or train them on ec2 by you know, spawning it from an AWS engagement. And then after the training is finished, I have successfully deployed it on ECR. So that is my experience is saying makeup. So I have done some work on sagemaker specifically. Lambda is not really. A familiar to me also I would say I know where it is used. But in my you know day-to-day activities, I really use Lambda so that we use Lambda as a company. So mostly our backend team handles it towards the you know, data refresher services and stuff like that. But I personally I deal with you know, ECR and sagemaker sometimes. but really",4,True
522,522,522,522,"Interviewer: Given the complexity of caching and its hardware-level implementation, how did you practically apply this understanding, either in coursework or in your professional projects? Additionally, can you explain a scenario where a deeper understanding of caching directly influenced a design or optimization decision in one of your projects? 


Interviewee: So the the part where the part where I applied caching was during my internship at AMD. So in AMD, we do a lot of sensitivity studies, which also include sensitivity to caches and how different sizes of caches impact my performance. My deeper level of understanding of caches was used when we, when there was a particular instance where my workload was showing very negligible sensitivity to cash. To, LC level of cash specifically. And I understood that it is showing this sensitivity because large language models have a very high memory footprint. So it made sense that since the language model is, since the size of the language model is very high, which is approximately 30 to 40 GBs, it's not possible for the, CPU architecture to cache all of it in my l three cache, which is approximately 64 GB. Sorry. 64 MB. So it makes sense that every l three, hit every l three access was a l three miss. And hence, my data also showed that, there were a lot of misses in l three and very few hits in l three. Hence, I my deeper understanding of, caches helped me in my application at AMD.",4,"Given the complexity of caching and its hardware-level implementation, how did you practically apply this understanding, either in coursework or in your professional projects? Additionally, can you explain a scenario where a deeper understanding of caching directly influenced a design or optimization decision in one of your projects?","So the the part where the part where I applied caching was during my internship at AMD. So in AMD, we do a lot of sensitivity studies, which also include sensitivity to caches and how different sizes of caches impact my performance. My deeper level of understanding of caches was used when we, when there was a particular instance where my workload was showing very negligible sensitivity to cash. To, LC level of cash specifically. And I understood that it is showing this sensitivity because large language models have a very high memory footprint. So it made sense that since the language model is, since the size of the language model is very high, which is approximately 30 to 40 GBs, it's not possible for the, CPU architecture to cache all of it in my l three cache, which is approximately 64 GB. Sorry. 64 MB. So it makes sense that every l three, hit every l three access was a l three miss. And hence, my data also showed that, there were a lot of misses in l three and very few hits in l three. Hence, I my deeper understanding of, caches helped me in my application at AMD.",4,True
523,523,523,523,"Interviewer: Your approach to selecting the right model based on the dataset, problem complexity, and available resources is well-reasoned. When you mentioned using data augmentation and population techniques for a smaller dataset with traditional ML algorithms, could you elaborate on the specific data augmentation techniques you might employ for a tabular or structured dataset, traditionally more inclined towards traditional ML algorithms?  


Interviewee: Yeah, so, you know populating data set while using traditional algorithms involved feature engineering where you know, we can derive more data from the already existing data and we can make more features on the same. For example, even label encoding books with the same. And you know, I have worked with the project in my past experience. I have worked with a project. It was a fun project. It was Titanic survival prediction. So I just did it to have a fun time with my friends and what I in the data set what I got was there was many unrelated features. Whereas like ticket number name and name was considered not important until I figured it out that there is a parameter not a parameter. There is a part of the name which shows the respect term like Mrs. Dr. Kind of and there were missing values in the missing values in the data set in the age column. So to fill the missing values in another column, I have used the salutation part of their names and I have calculated the mean mode and Our is more suitable for the feet. I have later on the already existing columns have filled the missing values instead of dropping the entire feature or row. So by that way we can optimize our data set in a better way rather than the whole data set. I can specifically pick one feature and work on it and fill the columns by and by doing so I'm doing I'm implementing feature engineering which would ultimately result in populating the data set.",3,"Your approach to selecting the right model based on the dataset, problem complexity, and available resources is well-reasoned. When you mentioned using data augmentation and population techniques for a smaller dataset with traditional ML algorithms, could you elaborate on the specific data augmentation techniques you might employ for a tabular or structured dataset, traditionally more inclined towards traditional ML algorithms?","Yeah, so, you know populating data set while using traditional algorithms involved feature engineering where you know, we can derive more data from the already existing data and we can make more features on the same. For example, even label encoding books with the same. And you know, I have worked with the project in my past experience. I have worked with a project. It was a fun project. It was Titanic survival prediction. So I just did it to have a fun time with my friends and what I in the data set what I got was there was many unrelated features. Whereas like ticket number name and name was considered not important until I figured it out that there is a parameter not a parameter. There is a part of the name which shows the respect term like Mrs. Dr. Kind of and there were missing values in the missing values in the data set in the age column. So to fill the missing values in another column, I have used the salutation part of their names and I have calculated the mean mode and Our is more suitable for the feet. I have later on the already existing columns have filled the missing values instead of dropping the entire feature or row. So by that way we can optimize our data set in a better way rather than the whole data set. I can specifically pick one feature and work on it and fill the columns by and by doing so I'm doing I'm implementing feature engineering which would ultimately result in populating the data set.",3,True
526,526,526,526,"Interviewer: It's interesting to hear about your experience with generative AI and NLP, but I'm particularly interested in your computer vision work. Could you tell me about a project where you specifically applied computer vision techniques, such as the Alzheimer's Disease Research Toolkit or the real-time brand detection for sports events, and what methodologies you employed in those contexts?  

Interviewee: Are talking about my computer vision Journeys which started from my masters degree from my Master's Degree I had to compete so I completed a master's degree from triple IIT bookmyshow one of the renowned Institute in India. So from that from in my Master's this is itself. I started basically work on Matlab and in which I have done video compression algorithm with the help of these this particular algorithm, which we have research. I took participate on Smart India hackathon and smart India hackathon. I we are the we went first and we developed a tool for a Ministry of Sciences. So we have done after that two internships and those two in townships was based on basically the object reduction and one was based on the C plus plus two basically stabilized the movie videos and after that I done one internship in which I have done objectives using tensorflow. So that is one of the project after that. I have done multiple projects like face detection. I have done and one of the project which I started in manual was basically a sports company earlier and they have a data science team over there. And what we do is like we take the video frames from the nanotechnology video frames of the sports events like FIFA Cricket orphan race. So we have the video frames we extract those video frames using open CV and from that those frames we have some bunch of annotated. Over there we created a data Lake for the different brands like Coca-Cola Pepsi and other brands which is present at the time of sports event. So we did for pkl Chennai. Sorry IPL for Chennai Super King specifically those that was our client F1 race that particular is our also client. So we calculate the return of investment using object detection techniques. So we used YOLO technique and we you use your flow. We use Cafe and Cafe to at that time because it was from 2017 to 2919 where I worked in nanotech. So basically the idea was to calculate the brand visibility using of the direction technique after that. I moved to seasonal Bangalore there was doing so that company was based out of an online gaming company and that was an early stage of startup which I have done like only four to four or five members were there. So my role over there was a computer vision technique in which so I applied objection technique in like four to five. Games, so the main target was to extract the video frame from the twitch stream. So which is a platform where online Gamers usually play games and where we have all the like we targeted 4 to 5 Games over there. So there are different moments like someone is killing. Someone is poking someone so those particular frames we have again a bunch of annotators with us and we focus on three to four games with different events. So they are basically measure something like this. The help of sound engineer sound basically as well from at the back end there was a sound as well. So with the help of sound like gun firing shot is there so that sound will also help us and the objection technique. So the flow was video comes into us and from that particular video, we extract the frames. We have annotators who annotates different events in the different games and from those annotation. We train an object detection model using tensorflow and from that tensorflow whenever we are giving a new particular video related to those three to four games. We are focusing on it automatically detects those frames and it stores into a place and those detected frames afterwards. The collection of the detected frame afterwards is rendered back using open CV to create a whole video which was the actually highlight generation at that particular point. So this is how the whole deep learning of for the computer vision at work for creating automatic highlights generation apart from that I worked in An lpo as well over there because there is a chat section at the right hand side in the twitch Stream So from the chat section, we were calculating the sentiment and Analysis. We are doing the sentimental analysis. So I was involved in that shows from the sentiment analysis. We take basically majorly. Like what is the funny moment cool moment or someone something is happening in the game. So that particular instance will get from the NLP. But the major idea was to have the computer vision model over there to automatically generate.Every sixth frame like sixth frame 12 frame 18 frame likewise to reduce the computation time basically and we have in-house GPU at that time. But right now I am working in Azure and AWS as well.",3,"It's interesting to hear about your experience with generative AI and NLP, but I'm particularly interested in your computer vision work. Could you tell me about a project where you specifically applied computer vision techniques, such as the Alzheimer's Disease Research Toolkit or the real-time brand detection for sports events, and what methodologies you employed in those contexts?","Are talking about my computer vision Journeys which started from my masters degree from my Master's Degree I had to compete so I completed a master's degree from triple IIT bookmyshow one of the renowned Institute in India. So from that from in my Master's this is itself. I started basically work on Matlab and in which I have done video compression algorithm with the help of these this particular algorithm, which we have research. I took participate on Smart India hackathon and smart India hackathon. I we are the we went first and we developed a tool for a Ministry of Sciences. So we have done after that two internships and those two in townships was based on basically the object reduction and one was based on the C plus plus two basically stabilized the movie videos and after that I done one internship in which I have done objectives using tensorflow. So that is one of the project after that. I have done multiple projects like face detection. I have done and one of the project which I started in manual was basically a sports company earlier and they have a data science team over there. And what we do is like we take the video frames from the nanotechnology video frames of the sports events like FIFA Cricket orphan race. So we have the video frames we extract those video frames using open CV and from that those frames we have some bunch of annotated. Over there we created a data Lake for the different brands like Coca-Cola Pepsi and other brands which is present at the time of sports event. So we did for pkl Chennai. Sorry IPL for Chennai Super King specifically those that was our client F1 race that particular is our also client. So we calculate the return of investment using object detection techniques. So we used YOLO technique and we you use your flow. We use Cafe and Cafe to at that time because it was from 2017 to 2919 where I worked in nanotech. So basically the idea was to calculate the brand visibility using of the direction technique after that. I moved to seasonal Bangalore there was doing so that company was based out of an online gaming company and that was an early stage of startup which I have done like only four to four or five members were there. So my role over there was a computer vision technique in which so I applied objection technique in like four to five. Games, so the main target was to extract the video frame from the twitch stream. So which is a platform where online Gamers usually play games and where we have all the like we targeted 4 to 5 Games over there. So there are different moments like someone is killing. Someone is poking someone so those particular frames we have again a bunch of annotators with us and we focus on three to four games with different events. So they are basically measure something like this. The help of sound engineer sound basically as well from at the back end there was a sound as well. So with the help of sound like gun firing shot is there so that sound will also help us and the objection technique. So the flow was video comes into us and from that particular video, we extract the frames. We have annotators who annotates different events in the different games and from those annotation. We train an object detection model using tensorflow and from that tensorflow whenever we are giving a new particular video related to those three to four games. We are focusing on it automatically detects those frames and it stores into a place and those detected frames afterwards. The collection of the detected frame afterwards is rendered back using open CV to create a whole video which was the actually highlight generation at that particular point. So this is how the whole deep learning of for the computer vision at work for creating automatic highlights generation apart from that I worked in An lpo as well over there because there is a chat section at the right hand side in the twitch Stream So from the chat section, we were calculating the sentiment and Analysis. We are doing the sentimental analysis. So I was involved in that shows from the sentiment analysis. We take basically majorly. Like what is the funny moment cool moment or someone something is happening in the game. So that particular instance will get from the NLP. But the major idea was to have the computer vision model over there to automatically generate.Every sixth frame like sixth frame 12 frame 18 frame likewise to reduce the computation time basically and we have in-house GPU at that time. But right now I am working in Azure and AWS as well.",3,True
530,530,530,530,"Interviewer: Great! I see from your resume that you've been working on improving demand forecasting at THOUCENTRIC. Can you tell me about a specific machine learning model you've implemented for this purpose and how you integrated it with the existing system?  

Interviewee: See the project is for a client PMI and the objective is to find the optimal with the objective is to predict an optimal value of the illicit trade that will happen in the next three years and the data that we had was that annual level. So we had around 10 or 12 year data depending upon the markets. So for let's say for some Market. We have 10 here data while for other we have eight here data something like this. So the objective is to just predict illicit trade that will happen in the next couple of years. For this what we did was we we cannot use the complex model like LG Boost or xgboost since we have very less data points. We have around 10 data points. So the only way is to go with the regression model. We had tried with various regression model and we finally opted for the Richard creation. Now what we did over there was we created a lot of simulations on different features. We had also done an extensive feature engineering depending upon the market. So let's say there's a market which has a factory of which is a few factors that generate those. Is it tobacco's or counterfeit products then what we did was We Gather the data. From the beginning the data and asking them that what should admit which rate the factories are getting closed. So using that metric we forecasted for that is specific market now since 2020 was a covid year. We had thought of using an using a new feature as a covid. So let's say it is a category Feature Feature like anomaly yes, or false so for 2021 and 22 early we had used as true. To check with that model like how it performs like since it is a normally ER people right go with the cheaper variant rather than the original one, or if you take into account the annual household median income at data that we got it from Oxford social economic data. it gives the data for how European market and Yeah. also, we are taking into account the GDP per capita of a Nation since the it's a high. Since this let's say if the GDP is increasing constantly over in here, then most most likely that these condition of people will increase the living condition of people will increase and hence. They will buy. They will buy a original product rather than buying a counterfeit one or illicit one. Apart from it since the major things that the issue that we faced was in certain markets where the the effect of covid was quite large. So for that we had done a few extensive. Feature engineering which had included the median household income and there was one more factor for European markets like they had open borders. So let's say a protein has been manufactured in nation a they can just take the product in the cartridge and ship it to Nation B. And set it over there that let's say 50% cheap price. So we had also taking into account of the price and the data of the neighboring countries presents neighboring countries for this. Yeah, and that's it for",3,Great! I see from your resume that you've been working on improving demand forecasting at THOUCENTRIC. Can you tell me about a specific machine learning model you've implemented for this purpose and how you integrated it with the existing system?,"See the project is for a client PMI and the objective is to find the optimal with the objective is to predict an optimal value of the illicit trade that will happen in the next three years and the data that we had was that annual level. So we had around 10 or 12 year data depending upon the markets. So for let's say for some Market. We have 10 here data while for other we have eight here data something like this. So the objective is to just predict illicit trade that will happen in the next couple of years. For this what we did was we we cannot use the complex model like LG Boost or xgboost since we have very less data points. We have around 10 data points. So the only way is to go with the regression model. We had tried with various regression model and we finally opted for the Richard creation. Now what we did over there was we created a lot of simulations on different features. We had also done an extensive feature engineering depending upon the market. So let's say there's a market which has a factory of which is a few factors that generate those. Is it tobacco's or counterfeit products then what we did was We Gather the data. From the beginning the data and asking them that what should admit which rate the factories are getting closed. So using that metric we forecasted for that is specific market now since 2020 was a covid year. We had thought of using an using a new feature as a covid. So let's say it is a category Feature Feature like anomaly yes, or false so for 2021 and 22 early we had used as true. To check with that model like how it performs like since it is a normally ER people right go with the cheaper variant rather than the original one, or if you take into account the annual household median income at data that we got it from Oxford social economic data. it gives the data for how European market and Yeah. also, we are taking into account the GDP per capita of a Nation since the it's a high. Since this let's say if the GDP is increasing constantly over in here, then most most likely that these condition of people will increase the living condition of people will increase and hence. They will buy. They will buy a original product rather than buying a counterfeit one or illicit one. Apart from it since the major things that the issue that we faced was in certain markets where the the effect of covid was quite large. So for that we had done a few extensive. Feature engineering which had included the median household income and there was one more factor for European markets like they had open borders. So let's say a protein has been manufactured in nation a they can just take the product in the cartridge and ship it to Nation B. And set it over there that let's say 50% cheap price. So we had also taking into account of the price and the data of the neighboring countries presents neighboring countries for this. Yeah, and that's it for",3,True
534,534,534,534,"Interviewer: It's clear you've tackled the challenge of working with limited data by employing transfer learning and data augmentation techniques. Now, considering your experience with both PyTorch and TensorFlow, can you compare your experiences with these two frameworks, particularly in the context of the lung X-ray disease classification project? What factors influenced your choice of framework for that project?  

Interviewee: actually I used a pytorch in my lung X-ray disease classification model. So the reason I use pytorch is the was that because it contained the torch Vision Library, which was very useful to me in the techniques like data augmentation where our image processing where the free. Like predefined functions were there so that the process of training the data pre-processing the data and cleaning the data should be fast. So that's why I use pythons also pytorch contains the torch Vision Library which helps us to plot different kinds of image images free data and try to manipulate them or try to understand those data. That's why I use Fighters. I could have also used tensorflow, but I Did for with because pytorch also have a great Community Support all so where the things were easily found in Google tensorflow. I found it difficult to found find my things like some function or where I'm getting stuck. So torch Vision was A definitely a good choice for me for the image classification model. So yeah, that's why I used to fight.",3,"It's clear you've tackled the challenge of working with limited data by employing transfer learning and data augmentation techniques. Now, considering your experience with both PyTorch and TensorFlow, can you compare your experiences with these two frameworks, particularly in the context of the lung X-ray disease classification project? What factors influenced your choice of framework for that project?","actually I used a pytorch in my lung X-ray disease classification model. So the reason I use pytorch is the was that because it contained the torch Vision Library, which was very useful to me in the techniques like data augmentation where our image processing where the free. Like predefined functions were there so that the process of training the data pre-processing the data and cleaning the data should be fast. So that's why I use pythons also pytorch contains the torch Vision Library which helps us to plot different kinds of image images free data and try to manipulate them or try to understand those data. That's why I use Fighters. I could have also used tensorflow, but I Did for with because pytorch also have a great Community Support all so where the things were easily found in Google tensorflow. I found it difficult to found find my things like some function or where I'm getting stuck. So torch Vision was A definitely a good choice for me for the image classification model. So yeah, that's why I used to fight.",3,True
535,535,535,535,"Interviewer: Certainly. I was asking about the challenges you encountered with the continuous action space in reinforcement learning and how the quantile truncated distribution approach helped address those challenges. Additionally, I'm interested in how you managed the increased computational resources required for this method.  

Interviewee: Yes, so number one. So when we have a plane fpg algorithm algorithm, I compared that with different architectures one is ddpg and then td3 and I derived SPG equivalent of td3 and ddpg algorithms. And this setup a baseline for me about what is the expected performance with sample policy gradient approach on the td3 architecture and ddpg architecture. Next when we when I started exploring the distributional reinforcement learning number one the main. The main approach that I had over here was in front of having a scale our reward function. We'll have a distribution, right? So there are two things that I did here. I looked at a paper called as truncated quantile critics where what they did was this overcame the overestimation bias. And it took the advantage of distributional learning in three ways one. critics are given a distributional representation And we truncate all these representations of the critics predictions. And then we have an ensemble of multiple critics. So this is what is called as truncated quantile critics. So here we have a mixture of critics. There are n approximators and then each critic gives a particular Q value. And then we have a mixture of so many critics and then we take this truncated mixture and then we compare this Q value with spgs or Q value.",4,"Certainly. I was asking about the challenges you encountered with the continuous action space in reinforcement learning and how the quantile truncated distribution approach helped address those challenges. Additionally, I'm interested in how you managed the increased computational resources required for this method.","Yes, so number one. So when we have a plane fpg algorithm algorithm, I compared that with different architectures one is ddpg and then td3 and I derived SPG equivalent of td3 and ddpg algorithms. And this setup a baseline for me about what is the expected performance with sample policy gradient approach on the td3 architecture and ddpg architecture. Next when we when I started exploring the distributional reinforcement learning number one the main. The main approach that I had over here was in front of having a scale our reward function. We'll have a distribution, right? So there are two things that I did here. I looked at a paper called as truncated quantile critics where what they did was this overcame the overestimation bias. And it took the advantage of distributional learning in three ways one. critics are given a distributional representation And we truncate all these representations of the critics predictions. And then we have an ensemble of multiple critics. So this is what is called as truncated quantile critics. So here we have a mixture of critics. There are n approximators and then each critic gives a particular Q value. And then we have a mixture of so many critics and then we take this truncated mixture and then we compare this Q value with spgs or Q value.",3,True
540,540,540,540,"Interviewer: Given the memory management challenges in Python, your approach to leveraging C++ for critical data operations is quite innovative. Could you elaborate on how you integrated C++ code with your Python workflow, particularly in terms of managing data consistency and ensuring that the results from C++ operations were seamlessly integrated back into the Python environment for further analysis?  


Interviewee: It so there are two approaches to this. One of them is creating a dynamic library and then calling it from python. So the dynamic library is basically a static Library which you compile with a standard C+ compiler like GCC or clang and then you would call it via a subprocess in Python. And the only way you were transfer data between the dynamic library and the python is through the python perimeters which are written in C. Um, another way of alleviating this position where it's much more advanced to something called piind 11 with pipeline. You can basically do one-to-one mappings between Python object like a numpy array down to AC plus of floating Point representation either float 32 or double or so on so forth. So effectively you can either do static Dynamic Library linking or using a pipeline",3,"Given the memory management challenges in Python, your approach to leveraging C++ for critical data operations is quite innovative. Could you elaborate on how you integrated C++ code with your Python workflow, particularly in terms of managing data consistency and ensuring that the results from C++ operations were seamlessly integrated back into the Python environment for further analysis?","It so there are two approaches to this. One of them is creating a dynamic library and then calling it from python. So the dynamic library is basically a static Library which you compile with a standard C+ compiler like GCC or clang and then you would call it via a subprocess in Python. And the only way you were transfer data between the dynamic library and the python is through the python perimeters which are written in C. Um, another way of alleviating this position where it's much more advanced to something called piind 11 with pipeline. You can basically do one-to-one mappings between Python object like a numpy array down to AC plus of floating Point representation either float 32 or double or so on so forth. So effectively you can either do static Dynamic Library linking or using a pipeline",4,True
558,558,558,558,"Interviewer: Great! Let's start with your RHYM Threat Intellect project. Can you elaborate on the techniques and processes you used for collecting and structuring threat intelligence data? 

Interviewee: Okay, so for that we have Builder threat intelligence platform. So we have collected data from different open sources like my track and we are having cve details in four and nvd data set. So we have collected all the data using with scrapping so in web scrapping we have used both selenium and Beautiful soap techniques and after collecting the data, we have pre-processed the data by using nltk toolkits. So by using NLP techniques only so we have done tokenization limitations and all the stemming techniques and everything. So after pre-processing, we have played any are techniques so where it can identify the entities, for example, if there is a location where the threat is happening, so it is able to find out the location if it is a technical organization, it will try to find it the tag name as a company name. So something like so we have applied this any yet techniques to get the info from that data. We have collected after that. We have build an ontology graph for the trick happening. So if some group is There a PT 28 so for the data, we have collected all the information so like malware names. So what are the males and their CVS from where they are targeting? What are the places they are located in and what are the organizations are what platforms there are targeting like for example few groups might be good retail industry, but remaining groups may not be right. So similarly for each domain, they will be some groups at I mean attached to that so we have Done all these processing for this technology platform. Yeah, that's all.",4,Great! Let's start with your RHYM Threat Intellect project. Can you elaborate on the techniques and processes you used for collecting and structuring threat intelligence data?,"Okay, so for that we have Builder threat intelligence platform. So we have collected data from different open sources like my track and we are having cve details in four and nvd data set. So we have collected all the data using with scrapping so in web scrapping we have used both selenium and Beautiful soap techniques and after collecting the data, we have pre-processed the data by using nltk toolkits. So by using NLP techniques only so we have done tokenization limitations and all the stemming techniques and everything. So after pre-processing, we have played any are techniques so where it can identify the entities, for example, if there is a location where the threat is happening, so it is able to find out the location if it is a technical organization, it will try to find it the tag name as a company name. So something like so we have applied this any yet techniques to get the info from that data. We have collected after that. We have build an ontology graph for the trick happening. So if some group is There a PT 28 so for the data, we have collected all the information so like malware names. So what are the males and their CVS from where they are targeting? What are the places they are located in and what are the organizations are what platforms there are targeting like for example few groups might be good retail industry, but remaining groups may not be right. So similarly for each domain, they will be some groups at I mean attached to that so we have Done all these processing for this technology platform. Yeah, that's all.",3,True
563,563,563,563,"Interviewer: That's a solid choice, leveraging AWS's auto-scaling and load balancing features to handle traffic spikes and ensure high availability. Now, considering your experience with both front-end and back-end technologies, how do you approach full-stack development, and can you give an example of a project where you effectively integrated both the front-end and back-end, perhaps using Node.js and React?  

Interviewee: Um, definitely this is a project that is kind of very close to me as this is something that I wanted to open as a service to my College college students. It was kind of a course recommend course reviewing subsides because before the before the semester starts like as a semester starts in the first week, we have this address period where we kind of have two sit in different courses see if this is this course works for us and during that shopping period you can drop an ekos. You can add any course and during that time I actually faced people approaching to me. How is this course? How is that caused over me a lot with that. So what I did, I actually the first approach that I started with was I just did a survey what's the need of it? Oh, I just said something and um after that I kind of understood what is the need of the course's what is the need of reviews that are required and was a new what is actually the demand what is what is actually in demand? I kind of went and laid out a system design where I was actually Understanding the various servers that would actually handle designing. What is to be in the back end. What is to be included in my back? And what is to be in my front end and After that being that I made a low Fidelity, I made a low Fidelity prototype in which I'm just roughly designing things. Then I made a mid fairly design abstracting out concrete sizing think different things and finally making a high fidelity design where um, kind of I know when I've designed the whole webpage and everything now starting with implementation. I first go I first walked on the back end. I made user authentication people can authenticate it and make sure that they have a valid session. So they have to kind of have like to have a valid session. I use something known as a ewt token with kind of gives you you know, kind of makes you helps you sustain a state while you are accessing different things while you are logged in with that is often authorized. What authorization do you have to it? That being said I made different API calls for like what like for example You you will get request to get the courses you get to do. You do an API call to get the views. You have a API called the courses to rate the courses and various of that once I had a very I had a tested back in ready. All I had to do was design on start creating my friend from The High Fidelity design that had already designed before it's like I had the picture of what I had to do something. All I needed to was actually now coded I used the app for that and after you after I had my kind of Front-end working. The last thing that was left was just using cors or different fetching mechanisms. I had to fetch the information. I had to make my front tenant backend talk to each other and press the information such that so this is how not of estimation actually went on creating from an idea to a whole stack project. And definitely I have to have it deploy them thinking to deploy it on digital oceans as it kind of is a very cheap thing. It's a very new thing it offers me again the gross scale as you grow facility and I will be going for that approaches.",3,"That's a solid choice, leveraging AWS's auto-scaling and load balancing features to handle traffic spikes and ensure high availability. Now, considering your experience with both front-end and back-end technologies, how do you approach full-stack development, and can you give an example of a project where you effectively integrated both the front-end and back-end, perhaps using Node.js and React?","Um, definitely this is a project that is kind of very close to me as this is something that I wanted to open as a service to my College college students. It was kind of a course recommend course reviewing subsides because before the before the semester starts like as a semester starts in the first week, we have this address period where we kind of have two sit in different courses see if this is this course works for us and during that shopping period you can drop an ekos. You can add any course and during that time I actually faced people approaching to me. How is this course? How is that caused over me a lot with that. So what I did, I actually the first approach that I started with was I just did a survey what's the need of it? Oh, I just said something and um after that I kind of understood what is the need of the course's what is the need of reviews that are required and was a new what is actually the demand what is what is actually in demand? I kind of went and laid out a system design where I was actually Understanding the various servers that would actually handle designing. What is to be in the back end. What is to be included in my back? And what is to be in my front end and After that being that I made a low Fidelity, I made a low Fidelity prototype in which I'm just roughly designing things. Then I made a mid fairly design abstracting out concrete sizing think different things and finally making a high fidelity design where um, kind of I know when I've designed the whole webpage and everything now starting with implementation. I first go I first walked on the back end. I made user authentication people can authenticate it and make sure that they have a valid session. So they have to kind of have like to have a valid session. I use something known as a ewt token with kind of gives you you know, kind of makes you helps you sustain a state while you are accessing different things while you are logged in with that is often authorized. What authorization do you have to it? That being said I made different API calls for like what like for example You you will get request to get the courses you get to do. You do an API call to get the views. You have a API called the courses to rate the courses and various of that once I had a very I had a tested back in ready. All I had to do was design on start creating my friend from The High Fidelity design that had already designed before it's like I had the picture of what I had to do something. All I needed to was actually now coded I used the app for that and after you after I had my kind of Front-end working. The last thing that was left was just using cors or different fetching mechanisms. I had to fetch the information. I had to make my front tenant backend talk to each other and press the information such that so this is how not of estimation actually went on creating from an idea to a whole stack project. And definitely I have to have it deploy them thinking to deploy it on digital oceans as it kind of is a very cheap thing. It's a very new thing it offers me again the gross scale as you grow facility and I will be going for that approaches.",3,True
565,565,565,565,"Interviewer: That sounds like a robust use of AWS's EMR and ECS for your ETL processes. Shifting focus to machine learning, as an ML Engineer, how have you leveraged computer vision in your past projects, and which frameworks or libraries did you use to implement your solutions? 

Interviewee: Yes regarding ml. Like I have here on 17 months of experience working with the machine learning natural language processing and computer vision together. I have worked on one of the computer vision startup calls 50i where we were developing the real-time computer vision applications for the retail industry. Where our use case is to use the computer vision to detect a products in the departmental stores the count of the products and the emptiness of the shelves there and also to Monitor and do the test detection as well for that purpose. We have integrated the computer vision with our mission learning algorithms combinedly work together and do the job there the Frameworks which we use their or specifically from the Kara's And also we have used. Convolutional neural network, which we made a custom made architecture within the framework that gave us an optimal accuracy of around 92% which you eventually does a job and we have deployed this framework as we can consider at considered it as a algorithmic model on AI Jetson Nano devices so that they can work on robust and they will work on their itself with the artificial intelligence and the Models we have deployed.",3,"That sounds like a robust use of AWS's EMR and ECS for your ETL processes. Shifting focus to machine learning, as an ML Engineer, how have you leveraged computer vision in your past projects, and which frameworks or libraries did you use to implement your solutions?","Yes regarding ml. Like I have here on 17 months of experience working with the machine learning natural language processing and computer vision together. I have worked on one of the computer vision startup calls 50i where we were developing the real-time computer vision applications for the retail industry. Where our use case is to use the computer vision to detect a products in the departmental stores the count of the products and the emptiness of the shelves there and also to Monitor and do the test detection as well for that purpose. We have integrated the computer vision with our mission learning algorithms combinedly work together and do the job there the Frameworks which we use their or specifically from the Kara's And also we have used. Convolutional neural network, which we made a custom made architecture within the framework that gave us an optimal accuracy of around 92% which you eventually does a job and we have deployed this framework as we can consider at considered it as a algorithmic model on AI Jetson Nano devices so that they can work on robust and they will work on their itself with the artificial intelligence and the Models we have deployed.",3,True
566,566,566,566,"Interviewer: Great! I see from your resume that you have experience with computer vision. Could you tell me about a specific project where you applied computer vision techniques, and what tools or libraries you used to implement it?  

Interviewee: Yep. I have been working a lot in computer vision. So one of my project, you know, I was working on this where I had to detect a ball in the game of cricket. So Cricket is a game where it's played in India a lot. So a ball is very small in size. So and it's and it also moves very fast. So we had to build an object detection model which first has to recognize where the ball is and also track that ball wherever it is going all over the field. So I used you know, YOLO YOLO is the one of the fastest detection algorithm to detect all the objects in it. I've used python I've used opencv to do a lot of filtering and all so these were some of the tools which are used to do this.",3,"Great! I see from your resume that you have experience with computer vision. Could you tell me about a specific project where you applied computer vision techniques, and what tools or libraries you used to implement it?","Yep. I have been working a lot in computer vision. So one of my project, you know, I was working on this where I had to detect a ball in the game of cricket. So Cricket is a game where it's played in India a lot. So a ball is very small in size. So and it's and it also moves very fast. So we had to build an object detection model which first has to recognize where the ball is and also track that ball wherever it is going all over the field. So I used you know, YOLO YOLO is the one of the fastest detection algorithm to detect all the objects in it. I've used python I've used opencv to do a lot of filtering and all so these were some of the tools which are used to do this.",3,True
573,573,573,573,"Interviewer: Fascinating approach. Regarding the binary classifier for document retrieval, could you detail the specific clustering method you used for negative sampling? And for the in-context learning examples, how did you ensure the diversity and coverage of the reasoning steps in the training data questions you generated?  


Interviewee: Okay. So for the retrieval task, actually we combined methods from both tensor travel and sparse retrieval. So sparsity travel agent bm25 and TF IDF. For dense retrieval be used open source methods like embeddings from like calculating similarity using embeddings from Ada 002 from openai and there is another model called sentence Transformer. So we use similarity scores from all these pilots and dense methods combined them together to create a vector representation for every sentence with the question. So whatever.",3,"Fascinating approach. Regarding the binary classifier for document retrieval, could you detail the specific clustering method you used for negative sampling? And for the in-context learning examples, how did you ensure the diversity and coverage of the reasoning steps in the training data questions you generated?","Okay. So for the retrieval task, actually we combined methods from both tensor travel and sparse retrieval. So sparsity travel agent bm25 and TF IDF. For dense retrieval be used open source methods like embeddings from like calculating similarity using embeddings from Ada 002 from openai and there is another model called sentence Transformer. So we use similarity scores from all these pilots and dense methods combined them together to create a vector representation for every sentence with the question. So whatever.",3,True
582,582,582,582,"Interviewer: Data cleaning and preprocessing are indeed crucial steps. Your use of Keras and TensorFlow for these tasks, along with data augmentation techniques, shows a strong grasp of preparing datasets for machine learning. Moving on to model deployment, can you describe your experience with deploying machine learning models, particularly any challenges you faced with model performance or scalability in production environments? 

Interviewee: Here. Okay the deployment of this more Machine learning models, like primarily we will serialize this model so that we can transport and store and retrieve and use it anywhere wherever we use our microservices to retrieve the data from this models. So we have used a pickle library and also dot HDMI based on the requirement. We will serialize this models. And we will store it in a remote places such as like it may be a cloud or the AWS. We will make this put this model. And also we have our own artifactory in our company where we can store our all these models. and after that we have our own platforms, which we download such as web applications or microservices or any platform what we have we connect with this with this model with respect to the apis. We have integrated to use this models and this apis will go with the proper load balancing routing and everything will happen in place. And which gave us a proper? Availability of the model and we scale it across multiple regions also on deploying different AWS account with respect to the region and readily make this model available and the major really one of the model where we worked on this case is in my current company. We have a Sophie and Discovery two of the Michelin models for the two big banks in us where we have developed the machine learning models for them and we have deployed that within our services and the clients in that specific Banks such as Sophia and Discovery, they will connect with the API. We just models and they will interact with the police staff based application with the queries that we provided.",3,"Data cleaning and preprocessing are indeed crucial steps. Your use of Keras and TensorFlow for these tasks, along with data augmentation techniques, shows a strong grasp of preparing datasets for machine learning. Moving on to model deployment, can you describe your experience with deploying machine learning models, particularly any challenges you faced with model performance or scalability in production environments?","Here. Okay the deployment of this more Machine learning models, like primarily we will serialize this model so that we can transport and store and retrieve and use it anywhere wherever we use our microservices to retrieve the data from this models. So we have used a pickle library and also dot HDMI based on the requirement. We will serialize this models. And we will store it in a remote places such as like it may be a cloud or the AWS. We will make this put this model. And also we have our own artifactory in our company where we can store our all these models. and after that we have our own platforms, which we download such as web applications or microservices or any platform what we have we connect with this with this model with respect to the apis. We have integrated to use this models and this apis will go with the proper load balancing routing and everything will happen in place. And which gave us a proper? Availability of the model and we scale it across multiple regions also on deploying different AWS account with respect to the region and readily make this model available and the major really one of the model where we worked on this case is in my current company. We have a Sophie and Discovery two of the Michelin models for the two big banks in us where we have developed the machine learning models for them and we have deployed that within our services and the clients in that specific Banks such as Sophia and Discovery, they will connect with the API. We just models and they will interact with the police staff based application with the queries that we provided.",3,True
585,585,585,585,"Interviewer: Understood, you've tailored your solution to the specific constraints of semiconductor design, which is crucial for accuracy. Leveraging GPU for the final product is a smart move for performance. Shifting focus a bit, I noticed you've also worked with time series analysis. Can you discuss how you've applied your machine learning expertise to time series data, perhaps in the context of anomaly detection or any other relevant application?  

Interviewee: Are multiple projects in which I have applied the time series application. So one of them is for example for the anomaly detection is the in the oea status OES is a special kind of the data in the semiconductor. That is the optical emission spectroscope data where the we get the web length and now we had to and now we know that the semiconductor manufacturing is a fast process as well as a as well as a complicated one. So that's why we have this thing known as the inline detection. So in line detection means that the anomaly will be detected as and when the semiconductor is is produced like there are multiple ways one is the offline offline will mean that once everything has already been done. Then you try to analyze and diagnose if any of the Wafers at some problem but in line will mean that as and when the wafer is inside the chamber and everything is going on you try to detect that now to do that. What I do is I I created this anomaly detection algorithm using the arima method now, I simply started with the arima because like I could not move to A complicated a very like the robust model because of the insufficiency of the data because the data is also limited while working in the semiconductor like you need to filter all kind of things and then get a very small data set and that's why I use the arima and using the arima what I used is based on all the previous Wafers. I tried to forecast the values for the like the next waveform and when the next wave will go in then that will produce a actual value and then I will try to calculate the difference between these two and when I calculate the difference between this two I try to see that how different is the actual from the real one and I will try to take the three sigma difference because if the predicted value and the actual value is three Sigma different from each other then that will mean that there is probably some anomaly. Otherwise if it is within the three sigma range, then we will consider it as a normal way for and we will just move on because we need to make sure that we like other than detecting the anomaly. We also had to make sure that we don't raise too many false alarm because that will create a hindrance in the production. And the other thing that I did with the time series data is the time series clustering that I will use in the in the Conductor there is a special kind of another special kind of the data which is known as the FDC data and FDC is the sensor data present in like the sensor data, which is collected as and when the wafer goes in so for each of the wafer or all the different sensors that are inside inside the chamber will be activated and they will record some kind of values the values may be the pressure temperature or some kind of the thing. And now what I will do is when I have to complete data for the whole way for I will try to Cluster them and For and using clustering like whenever there is some animal away for or something like that. Then it will be then it's Behavior will be different from what I already have Insight like from the from the previous way for inside the library and and I will try to like see that how is the anomalous way for is different from the all the correct way for us? Yeah.",4,"Understood, you've tailored your solution to the specific constraints of semiconductor design, which is crucial for accuracy. Leveraging GPU for the final product is a smart move for performance. Shifting focus a bit, I noticed you've also worked with time series analysis. Can you discuss how you've applied your machine learning expertise to time series data, perhaps in the context of anomaly detection or any other relevant application?","Are multiple projects in which I have applied the time series application. So one of them is for example for the anomaly detection is the in the oea status OES is a special kind of the data in the semiconductor. That is the optical emission spectroscope data where the we get the web length and now we had to and now we know that the semiconductor manufacturing is a fast process as well as a as well as a complicated one. So that's why we have this thing known as the inline detection. So in line detection means that the anomaly will be detected as and when the semiconductor is is produced like there are multiple ways one is the offline offline will mean that once everything has already been done. Then you try to analyze and diagnose if any of the Wafers at some problem but in line will mean that as and when the wafer is inside the chamber and everything is going on you try to detect that now to do that. What I do is I I created this anomaly detection algorithm using the arima method now, I simply started with the arima because like I could not move to A complicated a very like the robust model because of the insufficiency of the data because the data is also limited while working in the semiconductor like you need to filter all kind of things and then get a very small data set and that's why I use the arima and using the arima what I used is based on all the previous Wafers. I tried to forecast the values for the like the next waveform and when the next wave will go in then that will produce a actual value and then I will try to calculate the difference between these two and when I calculate the difference between this two I try to see that how different is the actual from the real one and I will try to take the three sigma difference because if the predicted value and the actual value is three Sigma different from each other then that will mean that there is probably some anomaly. Otherwise if it is within the three sigma range, then we will consider it as a normal way for and we will just move on because we need to make sure that we like other than detecting the anomaly. We also had to make sure that we don't raise too many false alarm because that will create a hindrance in the production. And the other thing that I did with the time series data is the time series clustering that I will use in the in the Conductor there is a special kind of another special kind of the data which is known as the FDC data and FDC is the sensor data present in like the sensor data, which is collected as and when the wafer goes in so for each of the wafer or all the different sensors that are inside inside the chamber will be activated and they will record some kind of values the values may be the pressure temperature or some kind of the thing. And now what I will do is when I have to complete data for the whole way for I will try to Cluster them and For and using clustering like whenever there is some animal away for or something like that. Then it will be then it's Behavior will be different from what I already have Insight like from the from the previous way for inside the library and and I will try to like see that how is the anomalous way for is different from the all the correct way for us? Yeah.",3,True
589,589,589,589,"Interviewer: How do you handle the trade-off between model complexity and computational efficiency, especially when deploying models on resource-constrained devices? 


Interviewee: For that, the first approach is to go go through all different kind of models available for object detection and find the best ones suited for our needs. So as I I mentioned earlier also, we would go for a model that is lightweight even though it's accuracy is less. If we want to improve the accuracy lightweight model, what we will do is that we will train the lightweight model on a very large data dataset, and that will produce a very high accuracy in its or, PyTorch PyTorch version after converting it to TensorFlow Lite version, there would be slightly decrease in accuracy. So we have to make we have to make sure that we start with a very high accuracy accuracy. And for deploying it on Android, Android platform, we also have to quantize the model so that it becomes of lighter weight. And so the only thing that we need to make sure is that, we will we will set a threshold like what accuracies we want on production environment. And considering that, that threshold, how what what should be the accuracy in the training environment, like, using the data using the dataset that we are using for training. So if this is if we are able to achieve the accuracy in production environment, so we would, try to go with the smaller models, which are trained on large dataset, with an intention to achieve high accuracy so that even if it's accuracy fall, it is still above our requirements.",3,"How do you handle the trade-off between model complexity and computational efficiency, especially when deploying models on resource-constrained devices?","For that, the first approach is to go go through all different kind of models available for object detection and find the best ones suited for our needs. So as I I mentioned earlier also, we would go for a model that is lightweight even though it's accuracy is less. If we want to improve the accuracy lightweight model, what we will do is that we will train the lightweight model on a very large data dataset, and that will produce a very high accuracy in its or, PyTorch PyTorch version after converting it to TensorFlow Lite version, there would be slightly decrease in accuracy. So we have to make we have to make sure that we start with a very high accuracy accuracy. And for deploying it on Android, Android platform, we also have to quantize the model so that it becomes of lighter weight. And so the only thing that we need to make sure is that, we will we will set a threshold like what accuracies we want on production environment. And considering that, that threshold, how what what should be the accuracy in the training environment, like, using the data using the dataset that we are using for training. So if this is if we are able to achieve the accuracy in production environment, so we would, try to go with the smaller models, which are trained on large dataset, with an intention to achieve high accuracy so that even if it's accuracy fall, it is still above our requirements.",3,True
595,595,595,595,"Interviewer: Great! I see from your resume that you've developed a fish freshness detection system using SVM and CNN. Can you walk me through how you decided to use both SVM and CNN for this project, and what specific aspects of the problem each algorithm was addressing?  

Interviewee: It so in Precision fish farming. My major role were my major aim was to detect fish freshness using the algorithms. So like we aim to develop a fish freshness detection system based on Fisher images. We decided to use both svm and CNN for this task due to their distincts and suitability for different aspects of the problem like in svm. We employed svm primary for its Effectiveness in handling structure data and binary classification tasks. As VM was used initially for its ability to classify images based on extracted features and to establish a baseline for comparison with a complex CNN model. And CNN's excel in tasks involving image recognition and classification making them well suited for our projects in image based analysis. We utilize CNN's to leverage their capability to automatically learn hierarchical features directly from raw image Charter. So yeah, that was it by combining svm and CNN to exploit the complementary strands of both algorithms. As we have provided a robust Baseline and effectively captured essential features while CNN enhance the system's ability to learn complex patterns and features directly from the raw image data.",3,"Great! I see from your resume that you've developed a fish freshness detection system using SVM and CNN. Can you walk me through how you decided to use both SVM and CNN for this project, and what specific aspects of the problem each algorithm was addressing?","It so in Precision fish farming. My major role were my major aim was to detect fish freshness using the algorithms. So like we aim to develop a fish freshness detection system based on Fisher images. We decided to use both svm and CNN for this task due to their distincts and suitability for different aspects of the problem like in svm. We employed svm primary for its Effectiveness in handling structure data and binary classification tasks. As VM was used initially for its ability to classify images based on extracted features and to establish a baseline for comparison with a complex CNN model. And CNN's excel in tasks involving image recognition and classification making them well suited for our projects in image based analysis. We utilize CNN's to leverage their capability to automatically learn hierarchical features directly from raw image Charter. So yeah, that was it by combining svm and CNN to exploit the complementary strands of both algorithms. As we have provided a robust Baseline and effectively captured essential features while CNN enhance the system's ability to learn complex patterns and features directly from the raw image data.",3,True
603,603,603,603,"Interviewer: Given the complexity of your model and the need for both supervised and unsupervised learning, how did you balance the trade-off between model accuracy and computational efficiency, especially considering the real-time requirements of traffic monitoring?  

Interviewee: Here, so this project was actually funded by the California Transportation. So they had a base data which was available. So by using a small amount, let's say 30% of annotated data as compared to 70% of unsupervised data. So what I did was I trained multi-object detection model which used to track the vehicles using yellow V5 and get the count of the different vehicles on the highways now once we get the count then we use an unsupervised patient temporal clustering model to actually know how many trucks are together that is whether they are in nearest neighbors. So as you see on generally on highways if there are four trucks in similar in seamless, let's say there is a four lane highway there are four trucks which are going parallelly so that leads to traffic conditions. So these are some of the different methods which we can improvise using unsupervised special temporal clustering and with a supervised learning approach of multi object detection with yolo version 5 you we can actually detect those trucks and then we can cluster daughter's accordingly and Give a good traffic estimate as to how much traffic is present. Why exactly it is happening. And so",4,"Given the complexity of your model and the need for both supervised and unsupervised learning, how did you balance the trade-off between model accuracy and computational efficiency, especially considering the real-time requirements of traffic monitoring?","Here, so this project was actually funded by the California Transportation. So they had a base data which was available. So by using a small amount, let's say 30% of annotated data as compared to 70% of unsupervised data. So what I did was I trained multi-object detection model which used to track the vehicles using yellow V5 and get the count of the different vehicles on the highways now once we get the count then we use an unsupervised patient temporal clustering model to actually know how many trucks are together that is whether they are in nearest neighbors. So as you see on generally on highways if there are four trucks in similar in seamless, let's say there is a four lane highway there are four trucks which are going parallelly so that leads to traffic conditions. So these are some of the different methods which we can improvise using unsupervised special temporal clustering and with a supervised learning approach of multi object detection with yolo version 5 you we can actually detect those trucks and then we can cluster daughter's accordingly and Give a good traffic estimate as to how much traffic is present. Why exactly it is happening. And so",3,True
607,607,607,607,"Interviewer: Certainly! In Python, the ""yield"" keyword is used in the context of defining a generator function. When you use ""yield"" in a function instead of ""return"", it transforms the function into a generator. The key difference is that ""return"" terminates a function and sends a specified value back to its caller, while ""yield"" pauses the function, saves its state, and then continues from where it left off the next time it's called. This ability to pause and resume execution is what makes generators memory efficient, as they don't need to store all their values in memory at once. Instead, they generate values on the fly, which can significantly reduce memory usage, especially with large datasets or computations. Given this explanation, how might you now consider using the ""yield"" keyword in your Python projects, particularly in the context of AI and ML where memory efficiency is a concern?  

Interviewee: Yes, so I said that I'm doing a project related to Active Learning methodologies right in this actual learning methodologies. What we do is we don't train the model with the entire data set first. We train with the small Point small set of data set. We calculate the representative less information less scores using uncertainty sampling method and we Give it to the user to label them. And again, we'll retrain with the new label dataset in this country text finding information and representativeness, which is a combination of entropy marginal difference and tsne pay us similarities everything. We have to pass the many images to those functions so which is not so memory efficient. So I would like to use this New Concept which is you said as a converting the function to the generator using the ill function, I would definitely use this concept in my actual learning project which can minimize so much of my memory usage because I was using literally a 10000 300 images for my training data so I can leverage my memory efficiency by using this generator concept. Well, so thank you for that. Europe",3,"Certainly! In Python, the ""yield"" keyword is used in the context of defining a generator function. When you use ""yield"" in a function instead of ""return"", it transforms the function into a generator. The key difference is that ""return"" terminates a function and sends a specified value back to its caller, while ""yield"" pauses the function, saves its state, and then continues from where it left off the next time it's called. This ability to pause and resume execution is what makes generators memory efficient, as they don't need to store all their values in memory at once. Instead, they generate values on the fly, which can significantly reduce memory usage, especially with large datasets or computations. Given this explanation, how might you now consider using the ""yield"" keyword in your Python projects, particularly in the context of AI and ML where memory efficiency is a concern?","Yes, so I said that I'm doing a project related to Active Learning methodologies right in this actual learning methodologies. What we do is we don't train the model with the entire data set first. We train with the small Point small set of data set. We calculate the representative less information less scores using uncertainty sampling method and we Give it to the user to label them. And again, we'll retrain with the new label dataset in this country text finding information and representativeness, which is a combination of entropy marginal difference and tsne pay us similarities everything. We have to pass the many images to those functions so which is not so memory efficient. So I would like to use this New Concept which is you said as a converting the function to the generator using the ill function, I would definitely use this concept in my actual learning project which can minimize so much of my memory usage because I was using literally a 10000 300 images for my training data so I can leverage my memory efficiency by using this generator concept. Well, so thank you for that. Europe",2,True
611,611,611,611,"Interviewer: Great! I see from your resume that you've worked on a project involving one-shot learning for medical image analysis. Could you walk me through the approach you took for this project and how you implemented it using TensorFlow?  

Interviewee: Sure, so the current problem that I faced doing that particular time was we had a particular data set of value of just a few hundred of labeled data set for something called chromosome analysis. So we had to build a model for classification regarding that so we couldn't go with the conventional class CNN because the problem is with such less dated always under fits and it's very difficult to generalize and the option that I took was going with contrastive learning. Which turned out to be very well at good and generalizing compared to a CNN based approach. So the approach is very simple. We had around 100 100 to 200 label data. So I built a service network, which is basically two Networks. Of the same network. So what this does is it takes an image as an input and gives you an output as an embedding. And the idea is that you give it pairs of images sometimes even triplet if you using triplet loss. So I used to close in this case. So it took three images One images called anchor image. The other image is an image, which is similar to this anchor image, right? The other image is something that's not so it's not the same classes jacket image. So what it does is it tries to create embeddings such that the distance or The euclidean distance you can say between the embeddings of the ones in the same class is minimized and the embedded distance between the embeddings of the ones and the different class is a maximized. So at the end when you convert an image with a Samus Network into embedding space it tries to minimize the distance between inter-class and Max is a difference between indoor. inter class so this is how much short learning for you used and the main benefit in this particular thing was that we were able to generalize very well, even though we had a very less data set compared to training a deep CNN which was not able to generalize later on when we got more data Weasley shifted to deep sea animals, but with this limited data one shot learning with Sam's networks worked great.",5,Great! I see from your resume that you've worked on a project involving one-shot learning for medical image analysis. Could you walk me through the approach you took for this project and how you implemented it using TensorFlow?,"Sure, so the current problem that I faced doing that particular time was we had a particular data set of value of just a few hundred of labeled data set for something called chromosome analysis. So we had to build a model for classification regarding that so we couldn't go with the conventional class CNN because the problem is with such less dated always under fits and it's very difficult to generalize and the option that I took was going with contrastive learning. Which turned out to be very well at good and generalizing compared to a CNN based approach. So the approach is very simple. We had around 100 100 to 200 label data. So I built a service network, which is basically two Networks. Of the same network. So what this does is it takes an image as an input and gives you an output as an embedding. And the idea is that you give it pairs of images sometimes even triplet if you using triplet loss. So I used to close in this case. So it took three images One images called anchor image. The other image is an image, which is similar to this anchor image, right? The other image is something that's not so it's not the same classes jacket image. So what it does is it tries to create embeddings such that the distance or The euclidean distance you can say between the embeddings of the ones in the same class is minimized and the embedded distance between the embeddings of the ones and the different class is a maximized. So at the end when you convert an image with a Samus Network into embedding space it tries to minimize the distance between inter-class and Max is a difference between indoor. inter class so this is how much short learning for you used and the main benefit in this particular thing was that we were able to generalize very well, even though we had a very less data set compared to training a deep CNN which was not able to generalize later on when we got more data Weasley shifted to deep sea animals, but with this limited data one shot learning with Sam's networks worked great.",4,True
620,620,620,620,"Interviewer: You've outlined a comprehensive strategy for managing model performance, including reverting to a previous model state, fine-tuning with additional data, and the use of A/B testing to validate new models before full deployment. These are all excellent practices for maintaining and improving model quality. Shifting focus a bit, I noticed you have experience with both TensorFlow and PyTorch. Can you discuss a scenario where you chose one framework over the other for a particular project, and what factors influenced your decision?  

Interviewee: Okay, so there's this project. For the same project where where I use Salient object detection. So in that project I had to use. a callbacks and a lot of intermediate steps To monitor my training to display specific logs that I want to see in the output while when the training is happening. So if it turns the flow. It's possible but it is a little bit restricted because they are already classes written. We have to know those predefined classes present their inherited them and then use callbacks to add them as a training step. So yeah, so we have one has to be aware of the documentation and how to do it. That is I think is one of the drawbacks of tensorflow so then I switched that code to buy torch where I have granular access so that granularity of pythons is what I love and If I want to train, so there is no predefined class. There is you have to literally write a for Loop and whatever callbacks or whatever intermediate things you want to add like for example, a list shopping page patients and Displaying some specific things validation Matrix and whatever it is, you can write it then in the for Loop. There is no predefined class that you have to remember or see the documentation and write it. So what can I do before starting any pork? What can I do after the pork ends and what what I want to display so all this granularity access is there and which is a simple python. I don't have to inherit a separate class and then follow this strategy. Like there is in tensorflow. So this is one of the reasons why where one of the situations where I have used pytorch switch to White Horse from tensorflow. Yeah.",3,"You've outlined a comprehensive strategy for managing model performance, including reverting to a previous model state, fine-tuning with additional data, and the use of A/B testing to validate new models before full deployment. These are all excellent practices for maintaining and improving model quality. Shifting focus a bit, I noticed you have experience with both TensorFlow and PyTorch. Can you discuss a scenario where you chose one framework over the other for a particular project, and what factors influenced your decision?","Okay, so there's this project. For the same project where where I use Salient object detection. So in that project I had to use. a callbacks and a lot of intermediate steps To monitor my training to display specific logs that I want to see in the output while when the training is happening. So if it turns the flow. It's possible but it is a little bit restricted because they are already classes written. We have to know those predefined classes present their inherited them and then use callbacks to add them as a training step. So yeah, so we have one has to be aware of the documentation and how to do it. That is I think is one of the drawbacks of tensorflow so then I switched that code to buy torch where I have granular access so that granularity of pythons is what I love and If I want to train, so there is no predefined class. There is you have to literally write a for Loop and whatever callbacks or whatever intermediate things you want to add like for example, a list shopping page patients and Displaying some specific things validation Matrix and whatever it is, you can write it then in the for Loop. There is no predefined class that you have to remember or see the documentation and write it. So what can I do before starting any pork? What can I do after the pork ends and what what I want to display so all this granularity access is there and which is a simple python. I don't have to inherit a separate class and then follow this strategy. Like there is in tensorflow. So this is one of the reasons why where one of the situations where I have used pytorch switch to White Horse from tensorflow. Yeah.",3,True
623,623,623,623,"Interviewer: Of course. I see from your resume that you've had a strong focus on computer vision, particularly in your role as a Computer Vision Research Intern at NIT Allahabad. Could you tell me about a specific project where you applied object detection to identify prohibited items in X-ray baggage images, and what challenges you faced during that research?  

Interviewee: Exactly. So in the NIT allahabad the research intern what we are trying to do is to find the prohibited items from x-ray images of the baggages at airports and train stations. So we were we are using various object detection techniques like YOLO models, especially like YOLO and faster rcnn. Which helps finding these are the novel architectures to find or to do object detection of these items and the one of the most major problem, which we are facing is right. Now the occlusion problem it is to a couple of two or more than items of the same or the prohibited items. They occlude occlude with each other to formal singular image of single materialistic image, which are model is finding it very difficult to find. So this is the challenge we face and other problem we are facing right now is the class in Balance problem because the data set availability is very difficult and very scarce. That's why we have a problem of that also. So right now we are trying to solve these two problems that is or the challenges occlusion problem and the class imbalance problem in the object detection.",4,"Of course. I see from your resume that you've had a strong focus on computer vision, particularly in your role as a Computer Vision Research Intern at NIT Allahabad. Could you tell me about a specific project where you applied object detection to identify prohibited items in X-ray baggage images, and what challenges you faced during that research?","Exactly. So in the NIT allahabad the research intern what we are trying to do is to find the prohibited items from x-ray images of the baggages at airports and train stations. So we were we are using various object detection techniques like YOLO models, especially like YOLO and faster rcnn. Which helps finding these are the novel architectures to find or to do object detection of these items and the one of the most major problem, which we are facing is right. Now the occlusion problem it is to a couple of two or more than items of the same or the prohibited items. They occlude occlude with each other to formal singular image of single materialistic image, which are model is finding it very difficult to find. So this is the challenge we face and other problem we are facing right now is the class in Balance problem because the data set availability is very difficult and very scarce. That's why we have a problem of that also. So right now we are trying to solve these two problems that is or the challenges occlusion problem and the class imbalance problem in the object detection.",3,True
625,625,625,625,"Interviewer: That's a very innovative approach to enhancing a Transformer model's ability to discern idiomatic expressions. The introduction of a unique token for idioms and the use of the Mirror BERT technique for context-specific embeddings are particularly intriguing. Given these findings and the techniques you've developed, how would you integrate this knowledge into a commercial NLP system to improve its handling of idiomatic language? Additionally, how might you utilize large language models to further refine the system's performance in this area? 

Interviewee: Um depends on the you know resources I have so I would, you know get a large Corpus first to you know, train the model accordingly. so I would start by. Taking a small model. Let's say but model or let's say still but so to be even smaller and I would first put my efforts into getting the right data. So a good part of my experiment there was also to collect the data and curate it towards this task. So for fortunately I have some data sets in mind that I used and which are actually, you know, competitively good. So I would rather data first then train a model and get the Baseline before finding it and evaluate the model before finding it then start fine tuning the model and evaluate it again. And then if the results are good, I would you know, put it to implementation in the real world. So the biggest challenge I found especially when using large language models in production or settings is that they have a good a bigger latency, right? So I would first try to you know use a small model to get the task done and have a good latency and serve it in production. You know with less cost and less latency. So if that works good enough, then I don't see the need to transition to large language models. But if I really do need to then I would do the same thing. I would find you in them. But as we can say large language models are really very very, you know, expensive to train and very hard to even infer. So I would use our you know, parameter efficient finding like Laura to use adapters, you know specialized adapters to find tune the llm towards a single task. So I would use adapters in this case also to find in the model and essentially then in the production settings just you know implement or not Implement just import the adapters for that particular task. And then we can use it in production, hopefully.",4,"That's a very innovative approach to enhancing a Transformer model's ability to discern idiomatic expressions. The introduction of a unique token for idioms and the use of the Mirror BERT technique for context-specific embeddings are particularly intriguing. Given these findings and the techniques you've developed, how would you integrate this knowledge into a commercial NLP system to improve its handling of idiomatic language? Additionally, how might you utilize large language models to further refine the system's performance in this area?","Um depends on the you know resources I have so I would, you know get a large Corpus first to you know, train the model accordingly. so I would start by. Taking a small model. Let's say but model or let's say still but so to be even smaller and I would first put my efforts into getting the right data. So a good part of my experiment there was also to collect the data and curate it towards this task. So for fortunately I have some data sets in mind that I used and which are actually, you know, competitively good. So I would rather data first then train a model and get the Baseline before finding it and evaluate the model before finding it then start fine tuning the model and evaluate it again. And then if the results are good, I would you know, put it to implementation in the real world. So the biggest challenge I found especially when using large language models in production or settings is that they have a good a bigger latency, right? So I would first try to you know use a small model to get the task done and have a good latency and serve it in production. You know with less cost and less latency. So if that works good enough, then I don't see the need to transition to large language models. But if I really do need to then I would do the same thing. I would find you in them. But as we can say large language models are really very very, you know, expensive to train and very hard to even infer. So I would use our you know, parameter efficient finding like Laura to use adapters, you know specialized adapters to find tune the llm towards a single task. So I would use adapters in this case also to find in the model and essentially then in the production settings just you know implement or not Implement just import the adapters for that particular task. And then we can use it in production, hopefully.",2,True
628,628,628,628,"Interviewer: It's great to hear that you have automated processes in place for monitoring model performance. Could you elaborate on the specific tasks involved in these automated processes, such as the use of logging, alerting, or scheduled retraining, and how you ensure that these processes are robust enough to handle model drift or degradation?  

Interviewee: Shop, so firstly first and important the first one task is data drift monitoring. I continuously monitor the distribution of incoming data to detect shifts or changes in the data characteristics over time this involves comparing the statistical properties of new data with the historical data and identifying the deviations that may indicate data drift. Another is the model drift detection. I monitor the performance of the deployed model by tracking key performance metrics such as accuracy precision recall and a fun school over the time in addition to monitoring data drift. I also monitor called monitor the quality of the data to ensure that it meets the expected standards and requirements. Finally. I set up alerting mechanisms and log in Frameworks to notify operators and shareholders of any issues or anomalies detected in the deployed model. When model drift or degradation is detected. I take proactive measures to address the issue by returning the model on a new data or updating the model architecture or hyper parameters continuous integration and development deployment that is cicd pipelines are used to automate the process of model retraining and deployment ensuring the deployed model remains up-to-date effective over the time.",4,"It's great to hear that you have automated processes in place for monitoring model performance. Could you elaborate on the specific tasks involved in these automated processes, such as the use of logging, alerting, or scheduled retraining, and how you ensure that these processes are robust enough to handle model drift or degradation?","Shop, so firstly first and important the first one task is data drift monitoring. I continuously monitor the distribution of incoming data to detect shifts or changes in the data characteristics over time this involves comparing the statistical properties of new data with the historical data and identifying the deviations that may indicate data drift. Another is the model drift detection. I monitor the performance of the deployed model by tracking key performance metrics such as accuracy precision recall and a fun school over the time in addition to monitoring data drift. I also monitor called monitor the quality of the data to ensure that it meets the expected standards and requirements. Finally. I set up alerting mechanisms and log in Frameworks to notify operators and shareholders of any issues or anomalies detected in the deployed model. When model drift or degradation is detected. I take proactive measures to address the issue by returning the model on a new data or updating the model architecture or hyper parameters continuous integration and development deployment that is cicd pipelines are used to automate the process of model retraining and deployment ensuring the deployed model remains up-to-date effective over the time.",3,True
632,632,632,632,"Interviewer: That's a great application of Seaborn to analyze customer purchase data. Could you elaborate on the specific Seaborn plots or functions you used in this scenario to visualize the spending patterns across different days of the week and compare the payment methods? How did you choose the type of Seaborn plot to use for each aspect of the analysis, and what insights did each type of plot reveal?  

Interviewee: Yeah first I've used the box plot SNS dot box plot method in which I gave the exact access day of week y-axis purchase amount and data. I have matched it to e-commerce data and the palette I've used this series. So this gives so box plot of day of week versus purchase amount and similar way of created a bar plot. I mean I've used plot dot figure function to set the figure size and then as soon as per plot method to give the bar plot. The same way in the bar plot function. I've used the parameters such as X Y to give the access and buy access and data as he Commerce data and estimator function to give the sum of the data and see I set it to none with the pilot as muted. So this gives a bath plot. I mean to say as a explained it provides the insights to the spending patterns across different days of the week and compare the total purchase amounts associated with different payment methods.",3,"That's a great application of Seaborn to analyze customer purchase data. Could you elaborate on the specific Seaborn plots or functions you used in this scenario to visualize the spending patterns across different days of the week and compare the payment methods? How did you choose the type of Seaborn plot to use for each aspect of the analysis, and what insights did each type of plot reveal?",Yeah first I've used the box plot SNS dot box plot method in which I gave the exact access day of week y-axis purchase amount and data. I have matched it to e-commerce data and the palette I've used this series. So this gives so box plot of day of week versus purchase amount and similar way of created a bar plot. I mean I've used plot dot figure function to set the figure size and then as soon as per plot method to give the bar plot. The same way in the bar plot function. I've used the parameters such as X Y to give the access and buy access and data as he Commerce data and estimator function to give the sum of the data and see I set it to none with the pilot as muted. So this gives a bath plot. I mean to say as a explained it provides the insights to the spending patterns across different days of the week and compare the total purchase amounts associated with different payment methods.,2,True
638,638,638,638,"Interviewer: It sounds like you've had a comprehensive experience with web scraping, data cleaning, and model integration. You mentioned using the RAG (Retrieval-Augmented Generation) technique for question answering. Could you elaborate on how you implemented RAG in your project and what challenges you faced while integrating it with the data from your database?  

Interviewee: Are retrievable augmented generation is basically we retrieve the data suppose if we take any website, we scrape the data and then clean it and then we split the data into small small chunks of sizes. And these chunks could be done using many techniques one will be like recursive character testing and there is token testing for the first time we have used with recursive character testing technique where we split the characters and until some special characters are engaged in the test, like multiple level lines symbols or something like that and these small small chunks are then I can if I if you want a general overview, they will be like 150 token or size in length. And these are given to add as 002 that will convert the test into embeddings and these embeddings are stored as postgres vectors. And whenever user gives a query in our chatbot, the chatbot is again converted as an embedding using the add as 002 and this same embedding is to the other posters DB and we made calls in the Prisma. Okay, and the similar to between the various available in the TV is done with this user query and whatever the top five we specifically chose the top five chunks which are similar to this user query. Are achieved and they were sent again back to charge EBT for generalizing them. Like we may not know like if we have got five 150 character test. After length, so these are given to rgbt for enhancing the user readability like take this five chunks of content and make it meaningful. So that meaningful answers are given back to the user and like this we have done with that static website based answers real-time answers like crypto price. And what is the latest defi data like exchanging information particular coin prices on different exchanges. Please kind of answers it can generate.",3,"It sounds like you've had a comprehensive experience with web scraping, data cleaning, and model integration. You mentioned using the RAG (Retrieval-Augmented Generation) technique for question answering. Could you elaborate on how you implemented RAG in your project and what challenges you faced while integrating it with the data from your database?","Are retrievable augmented generation is basically we retrieve the data suppose if we take any website, we scrape the data and then clean it and then we split the data into small small chunks of sizes. And these chunks could be done using many techniques one will be like recursive character testing and there is token testing for the first time we have used with recursive character testing technique where we split the characters and until some special characters are engaged in the test, like multiple level lines symbols or something like that and these small small chunks are then I can if I if you want a general overview, they will be like 150 token or size in length. And these are given to add as 002 that will convert the test into embeddings and these embeddings are stored as postgres vectors. And whenever user gives a query in our chatbot, the chatbot is again converted as an embedding using the add as 002 and this same embedding is to the other posters DB and we made calls in the Prisma. Okay, and the similar to between the various available in the TV is done with this user query and whatever the top five we specifically chose the top five chunks which are similar to this user query. Are achieved and they were sent again back to charge EBT for generalizing them. Like we may not know like if we have got five 150 character test. After length, so these are given to rgbt for enhancing the user readability like take this five chunks of content and make it meaningful. So that meaningful answers are given back to the user and like this we have done with that static website based answers real-time answers like crypto price. And what is the latest defi data like exchanging information particular coin prices on different exchanges. Please kind of answers it can generate.",3,True
641,641,641,641,"Interviewer: Could you now explain the concept of transfer learning and how you have applied it in your machine learning projects? 

Interviewee: it Sure, so when it comes to transfer learning what it means is that when a particular model is being trained for a for a single task that model now becomes equipped to better equipped to be utilized for another task as well. So a small example of that is how we first trained in the in the use case for keyword recommendation on a publisher URLs in at media.net. What we were trying to do here is that we were trying to find the birth model so that we the embeddings are optimized for the pairwise classification task of predicting a click versus no click. So for a given URL and a given keyword, we wanted to know whether a keyword will get clicked on a particular URL and for this particular task, we wanted to find you on the court model to generate embeddings. So we utilize we use the burst bird based. On Case Model here. So before we find tune this model on the pairwise classification task first. We went ahead with a fine tuning on the month mask language modeling task. So what how it helped here is that it ensure that the model learned the patterns the language patterns and the syntax and the semantics used in our keywords as well as in the URL. What's the URL? Of the clean URLs that we were using once the model was trained on this particular MLM task this this specific fine-tuned model was then utilized and for the pairwise classification tasks as well this particular learning helped ensure that we could find tune the model to generate better and more contextually more contextual embeddings for our task.",4,Could you now explain the concept of transfer learning and how you have applied it in your machine learning projects?,"it Sure, so when it comes to transfer learning what it means is that when a particular model is being trained for a for a single task that model now becomes equipped to better equipped to be utilized for another task as well. So a small example of that is how we first trained in the in the use case for keyword recommendation on a publisher URLs in at media.net. What we were trying to do here is that we were trying to find the birth model so that we the embeddings are optimized for the pairwise classification task of predicting a click versus no click. So for a given URL and a given keyword, we wanted to know whether a keyword will get clicked on a particular URL and for this particular task, we wanted to find you on the court model to generate embeddings. So we utilize we use the burst bird based. On Case Model here. So before we find tune this model on the pairwise classification task first. We went ahead with a fine tuning on the month mask language modeling task. So what how it helped here is that it ensure that the model learned the patterns the language patterns and the syntax and the semantics used in our keywords as well as in the URL. What's the URL? Of the clean URLs that we were using once the model was trained on this particular MLM task this this specific fine-tuned model was then utilized and for the pairwise classification tasks as well this particular learning helped ensure that we could find tune the model to generate better and more contextually more contextual embeddings for our task.",3,True
643,643,643,643,"Interviewer: I see, so the focus was on social network algorithms rather than a combination of NLP and computer vision. Given your experience with ranking models, could you elaborate on the specific techniques or algorithms you implemented for the edge rank-based approach? Additionally, how did you measure the success of this transition, and what impact did it have on user engagement within the platform?  

Interviewee: So the edge rank algorithm was like most more customized based on the use case that we were implementing and not really something that is standard practice out there. If the idea of the age rank was incorporated based on The engagement and engagement score. So the engagement score was more integral to what are product team wanted. Like what what were the which kind of Engagement scored higher for our users? So in our case, we give a higher weightage to the type of posts that the users were having an interaction with then next came the specific users or the specific users in the network with whom the user interacted with and then in within the engagement as well. The comments were ranked higher than the likes and this is how based on these factors there was a score that was evaluated for each of the post in the users feed and using this code. We had ranked the posts in a sequential manner. This is how the algorithm worked. And when it came to how much it succeeded as opposed to the initial recency based approach. I would say that it saw a significant Improvement in engagement. That is how we evaluated the performance as opposed to the previous performance the previous performance which was recently based saw a prevented the users from seeing a lot of posts that were more relevant to them because they appeared or way in the day as opposed to the recent post. And hence. They did not get to engage with those posts. We saw that about in a period of a single week. There was five times increase in the amount of likes that uses made overall on different posts. There were three or 300% increase in the comments that they used were making these are some of the observations that we made which gave us an idea that now the users were each able to view more relevant content that they were initially able to view based on just the recent ranking. methodology",4,"I see, so the focus was on social network algorithms rather than a combination of NLP and computer vision. Given your experience with ranking models, could you elaborate on the specific techniques or algorithms you implemented for the edge rank-based approach? Additionally, how did you measure the success of this transition, and what impact did it have on user engagement within the platform?","So the edge rank algorithm was like most more customized based on the use case that we were implementing and not really something that is standard practice out there. If the idea of the age rank was incorporated based on The engagement and engagement score. So the engagement score was more integral to what are product team wanted. Like what what were the which kind of Engagement scored higher for our users? So in our case, we give a higher weightage to the type of posts that the users were having an interaction with then next came the specific users or the specific users in the network with whom the user interacted with and then in within the engagement as well. The comments were ranked higher than the likes and this is how based on these factors there was a score that was evaluated for each of the post in the users feed and using this code. We had ranked the posts in a sequential manner. This is how the algorithm worked. And when it came to how much it succeeded as opposed to the initial recency based approach. I would say that it saw a significant Improvement in engagement. That is how we evaluated the performance as opposed to the previous performance the previous performance which was recently based saw a prevented the users from seeing a lot of posts that were more relevant to them because they appeared or way in the day as opposed to the recent post. And hence. They did not get to engage with those posts. We saw that about in a period of a single week. There was five times increase in the amount of likes that uses made overall on different posts. There were three or 300% increase in the comments that they used were making these are some of the observations that we made which gave us an idea that now the users were each able to view more relevant content that they were initially able to view based on just the recent ranking. methodology",3,True
645,645,645,645,"Interviewer: Certainly. Could you explain the concept of 'dropout' in neural networks and how it contributes to the generalization of a model? 

Interviewee: Dropout is a technique where randomly selected neurons are ignored during training. They are dropped out randomly. This means that their contribution to the activation of Downstream neurons is temporarily removed and on the forward pass and any weight updates are not applied to the neuron on the backward pass.",3,Certainly. Could you explain the concept of 'dropout' in neural networks and how it contributes to the generalization of a model?,Dropout is a technique where randomly selected neurons are ignored during training. They are dropped out randomly. This means that their contribution to the activation of Downstream neurons is temporarily removed and on the forward pass and any weight updates are not applied to the neuron on the backward pass.,3,True
648,648,648,648,"Interviewer: I completely understand that your focus has shifted more towards data science, and I appreciate your candor. Let's delve into your data science experience then. You've mentioned proficiency in Python, NumPy, Pandas, Matplotlib, Scikit-learn, TensorFlow, Keras, OpenCV, Transformers, SQL, Spacy, and PyTorch. Can you discuss a particularly challenging data science problem you've solved using these tools, and how you leveraged them to reach a solution?  

Interviewee: Exactly. So most of the problems of data science would require these libraries. First of all, one of the questions I would take would be let's say default prediction. In MX data set or it would be a phase generation using the conditional Gans or it would be style transfer. Let's say we talk about the style transfer. So for using this time for the style transfer project what I have to do was first full extract the style from the for from the image and overlap it or implemented upon another images, right so that way what we had to do was we have to first of all and lies the images. So with the help of opencv we read the images and analyze them we had to work with numpy to solve the two like resize these metrics or the image arrays so which we were created which were created by the reading the images. So we had to use numpy then we use tensorflow and pytorch like obviously and that time I used and supplier, but we could have used Fighters. So for developing the model using residual blocks and using various CNN layers to extract the information, right and after that we were so that's why it's blue and white horse could be used. Yeah. So these are one of the examples of the libraries which I had made all of the office. I had mentioned other example was as I said DeForest prediction in the AmEx data set, so as most of the time these tablet data sets are in the form of CSV files so we had To use pandas to read first of all them we have to use Partners to remove the what we can say Nan values or to not rather than removing it. to solve the issues of the missing values by if by checking out how much percentage of the values are missing then other like Shifting The Columns Shifting The Columns and Using it to like visualize visualize or rather than like change the data set into various formats to like use it for for the training processes or training. We use skill on models like logistic regression and what we can say random Force classifier for classification. Then we encountered the problem of class imbalance. So we took the task into the hands of kin to find the two work it as a anomaly detection task, so we used Fighters for that. Thank you.",4,"I completely understand that your focus has shifted more towards data science, and I appreciate your candor. Let's delve into your data science experience then. You've mentioned proficiency in Python, NumPy, Pandas, Matplotlib, Scikit-learn, TensorFlow, Keras, OpenCV, Transformers, SQL, Spacy, and PyTorch. Can you discuss a particularly challenging data science problem you've solved using these tools, and how you leveraged them to reach a solution?","Exactly. So most of the problems of data science would require these libraries. First of all, one of the questions I would take would be let's say default prediction. In MX data set or it would be a phase generation using the conditional Gans or it would be style transfer. Let's say we talk about the style transfer. So for using this time for the style transfer project what I have to do was first full extract the style from the for from the image and overlap it or implemented upon another images, right so that way what we had to do was we have to first of all and lies the images. So with the help of opencv we read the images and analyze them we had to work with numpy to solve the two like resize these metrics or the image arrays so which we were created which were created by the reading the images. So we had to use numpy then we use tensorflow and pytorch like obviously and that time I used and supplier, but we could have used Fighters. So for developing the model using residual blocks and using various CNN layers to extract the information, right and after that we were so that's why it's blue and white horse could be used. Yeah. So these are one of the examples of the libraries which I had made all of the office. I had mentioned other example was as I said DeForest prediction in the AmEx data set, so as most of the time these tablet data sets are in the form of CSV files so we had To use pandas to read first of all them we have to use Partners to remove the what we can say Nan values or to not rather than removing it. to solve the issues of the missing values by if by checking out how much percentage of the values are missing then other like Shifting The Columns Shifting The Columns and Using it to like visualize visualize or rather than like change the data set into various formats to like use it for for the training processes or training. We use skill on models like logistic regression and what we can say random Force classifier for classification. Then we encountered the problem of class imbalance. So we took the task into the hands of kin to find the two work it as a anomaly detection task, so we used Fighters for that. Thank you.",4,True
651,651,651,651,"Interviewer: Certainly, I can repeat the question. In the context of your work on EEG signal interpretation, could you discuss the trade-offs between using a shallow and wide neural network architecture versus a deep and narrow one? 

Interviewee: Yes, so trade-off between a shallow and a deep a deep learning model is two things. Number one shallow models. Usually learn very simple structures. So in EEG data set that we were extracting from humans. We had a lot of electrode data that was stored in our database. And shallow models only learned very simple structures and most of the time they wouldn't really converge. The accuracy rate that we were getting were hardly about. 50 to 60% which is essentially not so useful for EEG data, especially in medical scenario. and good examples of Shalom models are You know a neural network with only three layers. Right, whereas a deep learning model, which has more number of layers. There are two there are there. It's a complex structure so it can understand and process a lot more complex data. And each layer can be seen as a feature extracted that transport the transforms the given input into the next layer and every layer starts getting more and more abstract data and meaningful for the task at hand. Right? So deep models are more powerful and flexible and hence. They are, you know, If they are used for large. large complex tasks, but also they are more tough to train and interpret smaller models are easier to interpret whereas deep models are much more challenging to interpret about the decision that they took",4,"Certainly, I can repeat the question. In the context of your work on EEG signal interpretation, could you discuss the trade-offs between using a shallow and wide neural network architecture versus a deep and narrow one?","Yes, so trade-off between a shallow and a deep a deep learning model is two things. Number one shallow models. Usually learn very simple structures. So in EEG data set that we were extracting from humans. We had a lot of electrode data that was stored in our database. And shallow models only learned very simple structures and most of the time they wouldn't really converge. The accuracy rate that we were getting were hardly about. 50 to 60% which is essentially not so useful for EEG data, especially in medical scenario. and good examples of Shalom models are You know a neural network with only three layers. Right, whereas a deep learning model, which has more number of layers. There are two there are there. It's a complex structure so it can understand and process a lot more complex data. And each layer can be seen as a feature extracted that transport the transforms the given input into the next layer and every layer starts getting more and more abstract data and meaningful for the task at hand. Right? So deep models are more powerful and flexible and hence. They are, you know, If they are used for large. large complex tasks, but also they are more tough to train and interpret smaller models are easier to interpret whereas deep models are much more challenging to interpret about the decision that they took",3,True
652,652,652,652,"Interviewer: Minimizing the number of arithmetic operations is indeed a key strategy for reducing gas costs. In addition to that, did you employ any other gas optimization techniques, such as using storage efficiently, or perhaps batching transactions? How did you ensure that your smart contracts were as gas-efficient as possible without compromising on functionality?  

Interviewee: Yeah, so for that I actually used as you said already using unnecessary storage reads and writes. So I used that that technique so wherein I was reducing the storage reader and writes. So reducing storage heater rights are among the most expensive operations in the term of gas cost. Look use of local variables and memory instead of forest storage variables wherever feasible. So basically I avoided the use of local variables and memory Institute of using you search of storage. So storage will cost me a large amount of gas cost. So you use that I used variables and memory And also like best transaction, so combining multiple. So whenever I'm doing some operations or transaction, so I try to combine all the possible transaction in a single transaction that is called the batch transaction. So I also implemented this procedure as well. Yeah. Even I can also say that reducing contract compressor reviews to that level possible. possible to that level so that Overall overall the I'm not disturbing the actual logic of my code or in disturbing the flow of my code. Also trying to reduce the gas cost. Also also I can say that gas optimization also took me to actually so yeah. So I also avoided to do gas transaction when there was Network congestion. So Network condition is like when I'm using the taste Network, so when I'm using the real time real networks, like ethereum has a blockchain native the actual one. So I tried to do the transaction when possible only when the gas fees are the minimum or the modern one not not going about the motor to one ensure that I am spending the less amount of the least or the most possible low amount of gas fees in my transactions.",3,"Minimizing the number of arithmetic operations is indeed a key strategy for reducing gas costs. In addition to that, did you employ any other gas optimization techniques, such as using storage efficiently, or perhaps batching transactions? How did you ensure that your smart contracts were as gas-efficient as possible without compromising on functionality?","Yeah, so for that I actually used as you said already using unnecessary storage reads and writes. So I used that that technique so wherein I was reducing the storage reader and writes. So reducing storage heater rights are among the most expensive operations in the term of gas cost. Look use of local variables and memory instead of forest storage variables wherever feasible. So basically I avoided the use of local variables and memory Institute of using you search of storage. So storage will cost me a large amount of gas cost. So you use that I used variables and memory And also like best transaction, so combining multiple. So whenever I'm doing some operations or transaction, so I try to combine all the possible transaction in a single transaction that is called the batch transaction. So I also implemented this procedure as well. Yeah. Even I can also say that reducing contract compressor reviews to that level possible. possible to that level so that Overall overall the I'm not disturbing the actual logic of my code or in disturbing the flow of my code. Also trying to reduce the gas cost. Also also I can say that gas optimization also took me to actually so yeah. So I also avoided to do gas transaction when there was Network congestion. So Network condition is like when I'm using the taste Network, so when I'm using the real time real networks, like ethereum has a blockchain native the actual one. So I tried to do the transaction when possible only when the gas fees are the minimum or the modern one not not going about the motor to one ensure that I am spending the less amount of the least or the most possible low amount of gas fees in my transactions.",3,True
667,667,667,667,"Interviewer: Creating custom Python functions to check for data quality issues is a proactive approach. When you encountered these data quality issues, how did you prioritize which issues to address first, and what was your strategy for communicating these issues and their resolutions to the rest of your team or to stakeholders?  

Interviewee: So for that we have created Azure monitor API. Basically it goes the data it goes to Azure Monitor and basically it monitors in real time that how much what percentage of data is corrupt. What percentage of data is missing what percentage of data is having unclear values and corrupted values. So on real-time TVs team is able to see the dashboard and able to find out which data is not corrected correctly transformed into final sink. So this was the first approach we have used other approach basically be creating some reports for dash for Senior Management that they are able to go through all these information in a meaningful Manner and maybe emailing them on their system so that they can go and also change the Created python ETL pipelines. So that's how we can ensure that end-to-end. There is a data monitoring and we are able to check properly that the data is coming. It's validated and we are informed about it. So that in future let's say there are some more problems returning data machine learning Engineers can inform data. And so that these pipelines are fairly and please go through that and change. So that's how it's kind of a life monitor.",3,"Creating custom Python functions to check for data quality issues is a proactive approach. When you encountered these data quality issues, how did you prioritize which issues to address first, and what was your strategy for communicating these issues and their resolutions to the rest of your team or to stakeholders?",So for that we have created Azure monitor API. Basically it goes the data it goes to Azure Monitor and basically it monitors in real time that how much what percentage of data is corrupt. What percentage of data is missing what percentage of data is having unclear values and corrupted values. So on real-time TVs team is able to see the dashboard and able to find out which data is not corrected correctly transformed into final sink. So this was the first approach we have used other approach basically be creating some reports for dash for Senior Management that they are able to go through all these information in a meaningful Manner and maybe emailing them on their system so that they can go and also change the Created python ETL pipelines. So that's how we can ensure that end-to-end. There is a data monitoring and we are able to check properly that the data is coming. It's validated and we are informed about it. So that in future let's say there are some more problems returning data machine learning Engineers can inform data. And so that these pipelines are fairly and please go through that and change. So that's how it's kind of a life monitor.,3,True
669,669,669,669,"Interviewer: It's clear you've made significant improvements to the system. Given the issues with the initial Python code for language detection, how did you address the accuracy and speed concerns, and what impact did your changes have on the system's performance?  

Interviewee: Okay. Um, so because currently I'm working in legal domain and you know in legal domain the contracts they can be in any language. Right and there can be Snippets of different languages in each other's contracts and clauses Etc. So this language detection part always it is not the most important. part of the pipeline but it is where it all starts and that's why when I looked at the code code is let's say it's a flask based plus we are using Kafka topics, so It's async in nature. Right and there were some issues with the uses of multi-processing as well. Um, so that was one thing another thing was that let's say the way they were loading the models. In the flask app that was not proper, right? So they were a couple of things that I noticed there were small things. I think that they're not so like not very What do you call it is not something that impactful according to me. But still there were some nitpicks that I found and eventually they turned out to be a good optimization techniques, like for example in language detections. They were a lot of rules and reduces because their existing system was not very well lased for Asian languages and I simplified that logic I I used the GBA tokenizer and I looked into it. It's for specifically for each languages specifically Chinese and Mandarin, so Yeah, I looked into that tokenizer and I tried to find a better logic to segment our sentences as well, especially quickly in Chinese. Yeah, so that was the language detection. But secondly as I told you in language translation because I used Todd serve and I was serving the models on kubernetes directly and they were CPU based. Right? So and I was using Kida, so that means that it was Auto scaling right? So we were You it was pretty efficient. Yeah.",4,"It's clear you've made significant improvements to the system. Given the issues with the initial Python code for language detection, how did you address the accuracy and speed concerns, and what impact did your changes have on the system's performance?","Okay. Um, so because currently I'm working in legal domain and you know in legal domain the contracts they can be in any language. Right and there can be Snippets of different languages in each other's contracts and clauses Etc. So this language detection part always it is not the most important. part of the pipeline but it is where it all starts and that's why when I looked at the code code is let's say it's a flask based plus we are using Kafka topics, so It's async in nature. Right and there were some issues with the uses of multi-processing as well. Um, so that was one thing another thing was that let's say the way they were loading the models. In the flask app that was not proper, right? So they were a couple of things that I noticed there were small things. I think that they're not so like not very What do you call it is not something that impactful according to me. But still there were some nitpicks that I found and eventually they turned out to be a good optimization techniques, like for example in language detections. They were a lot of rules and reduces because their existing system was not very well lased for Asian languages and I simplified that logic I I used the GBA tokenizer and I looked into it. It's for specifically for each languages specifically Chinese and Mandarin, so Yeah, I looked into that tokenizer and I tried to find a better logic to segment our sentences as well, especially quickly in Chinese. Yeah, so that was the language detection. But secondly as I told you in language translation because I used Todd serve and I was serving the models on kubernetes directly and they were CPU based. Right? So and I was using Kida, so that means that it was Auto scaling right? So we were You it was pretty efficient. Yeah.",3,True
680,680,680,680,"Interviewer: Incorporating a security team for scrutiny and personally experimenting with new tools and frameworks to assess their fit for your use cases is a proactive approach. Could you discuss a specific instance where you had to optimize a solution for resource efficiency, and the techniques or tools you employed to achieve this?  

Interviewee: It's not basically tool that I would say some of the open source more is when we try to deploy it, right so there might be a case where we need to use the GPU efficient things. So as we already know like GPU corresponding instances were very costly. So in order to reduce that one, we have adapted a thing called spot instances. So this part is something where it can be taken back that instance can be taken back with a two minutes Interruption warning. So for that one, we had leveraged this case Auto scaling aspect. So what we try to do is one once this Interruption have warning comes right we try to spin up another new spot instance. If no spot instance is available in try to create a backup on demand instance. So as we already know like spot instance is having a very Less almost half of the price of the only man. So I think that way we can try to do a cost efficient and reliable setup. of the model",3,"Incorporating a security team for scrutiny and personally experimenting with new tools and frameworks to assess their fit for your use cases is a proactive approach. Could you discuss a specific instance where you had to optimize a solution for resource efficiency, and the techniques or tools you employed to achieve this?","It's not basically tool that I would say some of the open source more is when we try to deploy it, right so there might be a case where we need to use the GPU efficient things. So as we already know like GPU corresponding instances were very costly. So in order to reduce that one, we have adapted a thing called spot instances. So this part is something where it can be taken back that instance can be taken back with a two minutes Interruption warning. So for that one, we had leveraged this case Auto scaling aspect. So what we try to do is one once this Interruption have warning comes right we try to spin up another new spot instance. If no spot instance is available in try to create a backup on demand instance. So as we already know like spot instance is having a very Less almost half of the price of the only man. So I think that way we can try to do a cost efficient and reliable setup. of the model",3,True
689,689,689,689,"Interviewer: Yes, that's correct. How would you approach developing this real-time object detection system? 


Interviewee: So first of all, what I will do is that I will create a create a dataset in it in which I would, include lots of humans, like, in their different setting position, working position, doing all other stuff in the performing all other accents. So first of all, dataset would be created in which there will be human, and we'll create another datas we'll in same dataset, there would be 3 there would be 2 objects. Like, we don't need to we don't need to collect inanimate objects. They will be there. What we'll do is that we will, we will include humans and pets So we'll train a model that will, that will object detection model that will identify hue humans and pets. Like, 2 class 2 class object detection because this everything is inanimate object. Fine. So this is what I'm concluding. But if you are so after after, creating that dataset, we will I I'll prefer using YOLO. YOLO as very good object detection object detection models, and the conversion to their conversion to TF Lite model is also very easy and fast. So I'll but I'll prefer using YOLO versions like YOLOv5v. Right now, we even have YOLOv10. I can even explore that because it has been proved that it hasn't found that YOLO v10 10 is very effective on real time object detection. So I'll bet and also it has very low latency works very fast on low end devices, especially it's YOLO versions small version model.",3,"Yes, that's correct. How would you approach developing this real-time object detection system?","So first of all, what I will do is that I will create a create a dataset in it in which I would, include lots of humans, like, in their different setting position, working position, doing all other stuff in the performing all other accents. So first of all, dataset would be created in which there will be human, and we'll create another datas we'll in same dataset, there would be 3 there would be 2 objects. Like, we don't need to we don't need to collect inanimate objects. They will be there. What we'll do is that we will, we will include humans and pets So we'll train a model that will, that will object detection model that will identify hue humans and pets. Like, 2 class 2 class object detection because this everything is inanimate object. Fine. So this is what I'm concluding. But if you are so after after, creating that dataset, we will I I'll prefer using YOLO. YOLO as very good object detection object detection models, and the conversion to their conversion to TF Lite model is also very easy and fast. So I'll but I'll prefer using YOLO versions like YOLOv5v. Right now, we even have YOLOv10. I can even explore that because it has been proved that it hasn't found that YOLO v10 10 is very effective on real time object detection. So I'll bet and also it has very low latency works very fast on low end devices, especially it's YOLO versions small version model.",3,True
699,699,699,699,"Interviewer: Yes, please go ahead. How would you build and train the sentiment analysis model for this tool? What algorithms and frameworks would you consider? 


Interviewee: Okay. So a pretrained model, like we used pretrained model like BERT, Robert's, or a DistillBurt, fine tune for sentiment analysis, and the custom models that train your own model using label datasets like IMDB, Yelp, reviews, or specialized datasets for social media and multi class classification classify, which classifies sentiments, into positive, negative, and neutral classes. And these are some, data, sentiment analysis model, which are which I have used. And, can I talk about model training and evaluation?",3,"Yes, please go ahead. How would you build and train the sentiment analysis model for this tool? What algorithms and frameworks would you consider?","Okay. So a pretrained model, like we used pretrained model like BERT, Robert's, or a DistillBurt, fine tune for sentiment analysis, and the custom models that train your own model using label datasets like IMDB, Yelp, reviews, or specialized datasets for social media and multi class classification classify, which classifies sentiments, into positive, negative, and neutral classes. And these are some, data, sentiment analysis model, which are which I have used. And, can I talk about model training and evaluation?",3,True
709,709,709,709,"Interviewer: Understood. Now, could you outline the key steps or milestones you would prioritize in implementing this solution from start to finish? 


Interviewee: Yeah. So I talk about the solutions, then there are some problems, like, in the data reprocessing challenges, there are noise data noisy data, and the solution is, like, use advanced any NLP techniques for robust reprocessing, including custom, tokenizer and spell checker incorporate user disc dictionaries to handle slang and abbreviations, In the model training challenges, there is a imbalance data, and solution is you use techniques like Smoot, and, like, in that domain specific TCP is also a problem, in which the solution is to fine tune pretrained models on domain specific datasets. And in the real time processing challenge. Like, there is a lengthy latency problem, and solution is to optimize the processing pipeline using efficient algorithms and distributed processing systems like Apache, Kafka, and Spark. And if we talk about the model development challenges, then there is a model solving problem in which we have to ensure the model serve sub service service reduction quickly and reliably. And the solution is, like, to use model serving platforms like TensorFlow Serving, or TorchServe, which are optimized for performance and implement health checks and load balancing. And the monitoring and maintenance challenge, there is a problem of model drift in which the models performance may degrade over time as language and social media trends evolve. And the solution is to continuously monitor model performance and retrain with new data periodock periodically",3,"Understood. Now, could you outline the key steps or milestones you would prioritize in implementing this solution from start to finish?","Yeah. So I talk about the solutions, then there are some problems, like, in the data reprocessing challenges, there are noise data noisy data, and the solution is, like, use advanced any NLP techniques for robust reprocessing, including custom, tokenizer and spell checker incorporate user disc dictionaries to handle slang and abbreviations, In the model training challenges, there is a imbalance data, and solution is you use techniques like Smoot, and, like, in that domain specific TCP is also a problem, in which the solution is to fine tune pretrained models on domain specific datasets. And in the real time processing challenge. Like, there is a lengthy latency problem, and solution is to optimize the processing pipeline using efficient algorithms and distributed processing systems like Apache, Kafka, and Spark. And if we talk about the model development challenges, then there is a model solving problem in which we have to ensure the model serve sub service service reduction quickly and reliably. And the solution is, like, to use model serving platforms like TensorFlow Serving, or TorchServe, which are optimized for performance and implement health checks and load balancing. And the monitoring and maintenance challenge, there is a problem of model drift in which the models performance may degrade over time as language and social media trends evolve. And the solution is to continuously monitor model performance and retrain with new data periodock periodically",3,True
712,712,712,712,"Interviewer: That's a very interesting approach, leveraging contrastive learning for a small dataset. Given that you transitioned to using deep CNNs later, can you discuss the trade-offs between the two approaches in terms of model performance and computational efficiency? Also, how did you ensure that the Siamese network was not overfitting with such a small dataset?  

Interviewee: Okay. So the first thing is when we transition to a deep CNN. The obvious rate of was the first thing we needed to have a large amount of data for it to train on. So once we got to liquidated we have clinics started giving us a lot of data. So use that data to train a deep CNN. But one of the good things about the defense is that it does not require generating and storing embeddings for each data set to compare against during inference time. So the inference was obviously was faster and deep Channel. And with more data we could officially trained it to be more generalized. So I would say it was better than the contrast learning approach which should be used only if you have to because otherwise there's problem one that is when you are trying for inference you have to Take everything from each and every class related data set original data set and store it so that you can check against closest when you are infected. So that's one problem is networks. So I think there's not much shade of it was a good thing that we use and later with more data now coming to your question about how we generalize the data with this particular. Service networks, even though it was a small data set. Well one thing was used a lot of data augmentation because of the small amount of data set. We had we changed the angles the blooding effect we added. Bluntness, we are lines where so many different types of data augmentation methods. We have applied to make sure that we do not offer it in this particular scenario. And then we didn't experiment of CNN versus semester at that level that amount of data set. The time is network is providing more generalization.",4,"That's a very interesting approach, leveraging contrastive learning for a small dataset. Given that you transitioned to using deep CNNs later, can you discuss the trade-offs between the two approaches in terms of model performance and computational efficiency? Also, how did you ensure that the Siamese network was not overfitting with such a small dataset?","Okay. So the first thing is when we transition to a deep CNN. The obvious rate of was the first thing we needed to have a large amount of data for it to train on. So once we got to liquidated we have clinics started giving us a lot of data. So use that data to train a deep CNN. But one of the good things about the defense is that it does not require generating and storing embeddings for each data set to compare against during inference time. So the inference was obviously was faster and deep Channel. And with more data we could officially trained it to be more generalized. So I would say it was better than the contrast learning approach which should be used only if you have to because otherwise there's problem one that is when you are trying for inference you have to Take everything from each and every class related data set original data set and store it so that you can check against closest when you are infected. So that's one problem is networks. So I think there's not much shade of it was a good thing that we use and later with more data now coming to your question about how we generalize the data with this particular. Service networks, even though it was a small data set. Well one thing was used a lot of data augmentation because of the small amount of data set. We had we changed the angles the blooding effect we added. Bluntness, we are lines where so many different types of data augmentation methods. We have applied to make sure that we do not offer it in this particular scenario. And then we didn't experiment of CNN versus semester at that level that amount of data set. The time is network is providing more generalization.",2,True
734,734,734,734,"Interviewer: That's a solid approach to validation. Now, shifting focus to your work at VotaryTech, you mentioned developing a Face Recognition authorization system for Qualcomm's QCS 610 board. Can you discuss the specific challenges you faced in optimizing the performance of the Face Recognition system for this embedded platform, and what techniques or strategies you used to address these challenges? 


Interviewee: The first challenge which we faced was this is a relatively new hardware which has just been launched by Qualcomm, and the company had just acquired it. So we were the first people who were working on the hardware. The major issue was setting up the hardware because there was lack of documentation. The only documentation which we had was the one provided by Qualcomm. And when we face any bugs or issues, there were no online forums where people had discussed these issues before. That was the first major challenge which we came across when we were working on this project. As far as working on the facial recognition software is concerned, we developed the software on our independent separate Linux machine. And trained and tested the software there. And then we ported the software in a compatible format to the Qualcomm board.",3,"That's a solid approach to validation. Now, shifting focus to your work at VotaryTech, you mentioned developing a Face Recognition authorization system for Qualcomm's QCS 610 board. Can you discuss the specific challenges you faced in optimizing the performance of the Face Recognition system for this embedded platform, and what techniques or strategies you used to address these challenges?","The first challenge which we faced was this is a relatively new hardware which has just been launched by Qualcomm, and the company had just acquired it. So we were the first people who were working on the hardware. The major issue was setting up the hardware because there was lack of documentation. The only documentation which we had was the one provided by Qualcomm. And when we face any bugs or issues, there were no online forums where people had discussed these issues before. That was the first major challenge which we came across when we were working on this project. As far as working on the facial recognition software is concerned, we developed the software on our independent separate Linux machine. And trained and tested the software there. And then we ported the software in a compatible format to the Qualcomm board.",3,True
735,735,735,735,"Interviewer: It's great to hear about your hands-on experience with YOLO and Faster R-CNN, and the real-world challenges you've encountered. Addressing occlusion and class imbalance is crucial for practical applications. Can you elaborate on the strategies you've used or are considering to mitigate these issues, particularly the class imbalance problem, which is quite common in machine learning tasks?  

Interviewee: Exactly. So first of all, we have to do data augmentation to do to solve the problem of class imbalance. Then we have employed various strategies new strategies in which we have used couple of what we can say extracting new features from the images which have prohibited items. So our main task would be to first of all find the images which have prohibited items in them. And then we will try to focus more upon them. So we will be using attention mechanism to do this and to remove the background tribe tribals, then let's imagine we have 100 images. What we will do is we will pay each and every image with each other so they will be 100 C2 combinations of these images and will try to what we can say find out the features from each of them so that they will focus upon the similarities and dissimilarities between the images which have the prohibited items. So in this way, we will be solving the we will be having a lot of features new features specially so this will solve the data augmentation problem or the class and balance problem. So and we are employing new techniques like few short and 0 short learning so and apart from that regarding the task of occlusion problem. We are trying to go towards the what we can say counter based learning and cascaded learning recently. We have seen that a lot of research papers are employing this technique and specially the counter bass problems contest learning. So in the counter based learning, we will be employing attention mechanism and transform visual Transformer learning, which will help us to solve the occlusion problem. Thank you.",4,"It's great to hear about your hands-on experience with YOLO and Faster R-CNN, and the real-world challenges you've encountered. Addressing occlusion and class imbalance is crucial for practical applications. Can you elaborate on the strategies you've used or are considering to mitigate these issues, particularly the class imbalance problem, which is quite common in machine learning tasks?","Exactly. So first of all, we have to do data augmentation to do to solve the problem of class imbalance. Then we have employed various strategies new strategies in which we have used couple of what we can say extracting new features from the images which have prohibited items. So our main task would be to first of all find the images which have prohibited items in them. And then we will try to focus more upon them. So we will be using attention mechanism to do this and to remove the background tribe tribals, then let's imagine we have 100 images. What we will do is we will pay each and every image with each other so they will be 100 C2 combinations of these images and will try to what we can say find out the features from each of them so that they will focus upon the similarities and dissimilarities between the images which have the prohibited items. So in this way, we will be solving the we will be having a lot of features new features specially so this will solve the data augmentation problem or the class and balance problem. So and we are employing new techniques like few short and 0 short learning so and apart from that regarding the task of occlusion problem. We are trying to go towards the what we can say counter based learning and cascaded learning recently. We have seen that a lot of research papers are employing this technique and specially the counter bass problems contest learning. So in the counter based learning, we will be employing attention mechanism and transform visual Transformer learning, which will help us to solve the occlusion problem. Thank you.",3,True
737,737,737,737,"Interviewer: That's a comprehensive approach, leveraging the strengths of both algorithms. Can you elaborate on how you selected the pre-trained CNN model for this project and if any fine-tuning was necessary to achieve the desired level of accuracy in fish freshness detection?  

Interviewee: Yeah, sure. I can definitely tell you why I selected the pre-trained CNN model. So let's selecting the pre-trained CNN model involved tablet considerations aimed at maximizing accuracy and efficiency in fish freshness detection. So basic approach was firstly research and evaluation. We conducted thorough research to identify pretend CNN models that were well suited for image classification tasks particularly in the domain of food quality assessment. We evaluated various pre-trained models based on the performance on similar tasks model architecture competition or efficiency and availability of relevant features for a specific application. Secondly transfer learning potential. We prioritize speed train models set of a strong transfer learning capabilities allowing us to leverage capabilities features learned from a lost data sets such as image net and fine tune them for a specific task of fish freshness detection. Then model complexity and the source there are certain constraints like considering the computational resources available and the complexity of a data set. So we sort of pre-trained model that struck a balance between accuracy and computational efficiency. Then compatibility and Frameworks compatible with Frameworks and libraries like we ensured compatibility with the Deep learning Frameworks and libraries. We intended to use for model implementation and training such as tensorflow or pytorch. So overall our selection of the pretend CNN model was Guided by a combination of factors including its performance on similar tasks transfer learning potential competition efficiency compatibility and the need for fine-tuning to achieve the desire level of accuracy in the fish freshness detection. Yeah, that's it.",4,"That's a comprehensive approach, leveraging the strengths of both algorithms. Can you elaborate on how you selected the pre-trained CNN model for this project and if any fine-tuning was necessary to achieve the desired level of accuracy in fish freshness detection?","Yeah, sure. I can definitely tell you why I selected the pre-trained CNN model. So let's selecting the pre-trained CNN model involved tablet considerations aimed at maximizing accuracy and efficiency in fish freshness detection. So basic approach was firstly research and evaluation. We conducted thorough research to identify pretend CNN models that were well suited for image classification tasks particularly in the domain of food quality assessment. We evaluated various pre-trained models based on the performance on similar tasks model architecture competition or efficiency and availability of relevant features for a specific application. Secondly transfer learning potential. We prioritize speed train models set of a strong transfer learning capabilities allowing us to leverage capabilities features learned from a lost data sets such as image net and fine tune them for a specific task of fish freshness detection. Then model complexity and the source there are certain constraints like considering the computational resources available and the complexity of a data set. So we sort of pre-trained model that struck a balance between accuracy and computational efficiency. Then compatibility and Frameworks compatible with Frameworks and libraries like we ensured compatibility with the Deep learning Frameworks and libraries. We intended to use for model implementation and training such as tensorflow or pytorch. So overall our selection of the pretend CNN model was Guided by a combination of factors including its performance on similar tasks transfer learning potential competition efficiency compatibility and the need for fine-tuning to achieve the desire level of accuracy in the fish freshness detection. Yeah, that's it.",4,True
753,753,753,753,"Interviewer: That sounds like a solid high-level approach. Let's delve a bit deeper into some aspects of your solution. Given your experience, what potential challenges or constraints do you foresee in implementing this machine learning pipeline for detecting and predicting maintenance issues in solar panels? 


Interviewee: Okay. So one major challenge was that, the data capturing process process needs to be focused. Like, when we want to have a good ortho mosaic, it should have a certain degree of overlap. Like, a 40 degree side overlap and oh, sorry, 40% side overlap. And 40% or 50%, those, front overlap. So that was something that is the kind of not, in our control. It's a bit it's on the field team. So that was one of the challenge that we faced. Another was the the drifter problem. So initially, what we had seen was, once we have trained our model and put it into production, the thing was, it it was performing very well on, our training set that we had trained the model on, even on the validation set, which the model had not seen. But once we put it into the production, the new also that were coming in, they had very poor performance. So we wanted to understand what was the reason. So that's why we had implemented that dip drift detection system. So once we see there once we observed that there was significant drift in the training data that we had. And the latest data that we are currently getting, So we were able to pinpoint that where exactly the problem. So the problem was the field team had upgraded their drones for to a newer version, which are kind of better, and, we're able to capture high quality images. But, we were not aware that. And, we had, basically trained our model on the old image set, which was, a different drone than what the 3rd field team was currently loading. So that's when we realized that this was a problem, and so we retrained the model on that part. On the auto and images, from this new set of images by newer newer one. So yeah. So one of the challenges was, that knowing everything about data, how the data was connected. That was one of the crucial thing that I understood and will remember. Hopefully, in future, that, we also need to be aware of data collection process. So this was all one of the main problems, that we had faced. Yeah. So not too much. Rest everything kind of went smooth. And the preprocessing and experimenting with GSD to understand how the, that bounding box effect is affecting by its dimension were affected by GST. That was another thing that, we had to initially faced talent, but then we understood that, yeah, DSD is something that we also need to focus on. So, yeah, that's all I believe. Those are the challenges that face. That's it.",4,"That sounds like a solid high-level approach. Let's delve a bit deeper into some aspects of your solution. Given your experience, what potential challenges or constraints do you foresee in implementing this machine learning pipeline for detecting and predicting maintenance issues in solar panels?","Okay. So one major challenge was that, the data capturing process process needs to be focused. Like, when we want to have a good ortho mosaic, it should have a certain degree of overlap. Like, a 40 degree side overlap and oh, sorry, 40% side overlap. And 40% or 50%, those, front overlap. So that was something that is the kind of not, in our control. It's a bit it's on the field team. So that was one of the challenge that we faced. Another was the the drifter problem. So initially, what we had seen was, once we have trained our model and put it into production, the thing was, it it was performing very well on, our training set that we had trained the model on, even on the validation set, which the model had not seen. But once we put it into the production, the new also that were coming in, they had very poor performance. So we wanted to understand what was the reason. So that's why we had implemented that dip drift detection system. So once we see there once we observed that there was significant drift in the training data that we had. And the latest data that we are currently getting, So we were able to pinpoint that where exactly the problem. So the problem was the field team had upgraded their drones for to a newer version, which are kind of better, and, we're able to capture high quality images. But, we were not aware that. And, we had, basically trained our model on the old image set, which was, a different drone than what the 3rd field team was currently loading. So that's when we realized that this was a problem, and so we retrained the model on that part. On the auto and images, from this new set of images by newer newer one. So yeah. So one of the challenges was, that knowing everything about data, how the data was connected. That was one of the crucial thing that I understood and will remember. Hopefully, in future, that, we also need to be aware of data collection process. So this was all one of the main problems, that we had faced. Yeah. So not too much. Rest everything kind of went smooth. And the preprocessing and experimenting with GSD to understand how the, that bounding box effect is affecting by its dimension were affected by GST. That was another thing that, we had to initially faced talent, but then we understood that, yeah, DSD is something that we also need to focus on. So, yeah, that's all I believe. Those are the challenges that face. That's it.",3,True
759,759,759,759,"Interviewer: Using Docker with FastAPI for model deployment is a solid approach, and it's great to hear that you're considering scalability with technologies like Kubernetes and message queues. Shifting gears a bit, I noticed you've worked on a project for pseudocode to code generation using NLP. Can you discuss the challenges you faced in this project, particularly in ensuring that the generated code was both syntactically correct and functionally accurate?  

Interviewee: Shirt, so in this particular project the entire aim was to make sure that we do pseudocode to Coeur translation. to a good extent in the similar to the performance of llms, but without the computational cost of lnms so we ended up taking a very Low level approach we did not use any large language models. The views are decision tree and we used a simple lstm for Generation. So this just kind of three different phases first is the this one copy phase in this week copy all the tokens from the solo code which can be directly put in the code. For example, you have you know, sort of code you have let's say set variable X to 3. So X is something that can be directly copied. So we make sure that we Mark that as you know, this is something that can be copied. and then we use lstm to generate the code version of the suit of court, but with masks where we can take the copy tokens that we have generated. Later on the combined phase is where we take this. sentence that our lstm generated with masked tokens and we combine them with our Copy tokens, which have been generated by the decision tree. So for the final output, so what we realize here is this was in public elements not exactly better. But the performance is just there near the area performance, but for a fraction of its compute cost. Right and the challenges we Face Expressway while making sure the code is functional. syntactically, correct is obviously the first thing was we'll have to every time we ended up running the code later on once the generated it and we had errors to make sure what was going around where the issue was and retain the lstm. So what we figured out was the most problem was when the lstm which was having issues with translating the court to one from Sudoku to court and the copy Facebook was doing pretty great. So what we ended up doing was we ended up being sure the lstm generation was way better. So we focused on that we gave it more data. We tried it on this is well, so it's a little bit more generalized. we increase the Window and we tried by STM. We tried a few different things. Ah, finally by worked and the reason it worked here lstm better than llms is because the core we database we were working with was maximum 100 times of Court, which means that we would have to worry about forgetting and long context windows.",4,"Using Docker with FastAPI for model deployment is a solid approach, and it's great to hear that you're considering scalability with technologies like Kubernetes and message queues. Shifting gears a bit, I noticed you've worked on a project for pseudocode to code generation using NLP. Can you discuss the challenges you faced in this project, particularly in ensuring that the generated code was both syntactically correct and functionally accurate?","Shirt, so in this particular project the entire aim was to make sure that we do pseudocode to Coeur translation. to a good extent in the similar to the performance of llms, but without the computational cost of lnms so we ended up taking a very Low level approach we did not use any large language models. The views are decision tree and we used a simple lstm for Generation. So this just kind of three different phases first is the this one copy phase in this week copy all the tokens from the solo code which can be directly put in the code. For example, you have you know, sort of code you have let's say set variable X to 3. So X is something that can be directly copied. So we make sure that we Mark that as you know, this is something that can be copied. and then we use lstm to generate the code version of the suit of court, but with masks where we can take the copy tokens that we have generated. Later on the combined phase is where we take this. sentence that our lstm generated with masked tokens and we combine them with our Copy tokens, which have been generated by the decision tree. So for the final output, so what we realize here is this was in public elements not exactly better. But the performance is just there near the area performance, but for a fraction of its compute cost. Right and the challenges we Face Expressway while making sure the code is functional. syntactically, correct is obviously the first thing was we'll have to every time we ended up running the code later on once the generated it and we had errors to make sure what was going around where the issue was and retain the lstm. So what we figured out was the most problem was when the lstm which was having issues with translating the court to one from Sudoku to court and the copy Facebook was doing pretty great. So what we ended up doing was we ended up being sure the lstm generation was way better. So we focused on that we gave it more data. We tried it on this is well, so it's a little bit more generalized. we increase the Window and we tried by STM. We tried a few different things. Ah, finally by worked and the reason it worked here lstm better than llms is because the core we database we were working with was maximum 100 times of Court, which means that we would have to worry about forgetting and long context windows.",3,True
763,763,763,763,"Interviewer: Great! Let's dive into your resume. I see you've done some interesting work with IoT malware detection and face mask detection using deep learning. Can you elaborate on the specific deep learning models you used for these projects and why you chose them?  

Interviewee: So first of all in my iot malware detection project, I use the I use the Deep learning CNN model which which is I used to know I tried I tried and tried to implement various models such as vgt 16 PG 19 and instruction V3 and out of them gdg, 16 gave the best accuracy. So I did some hyperparameter tuning and the and the atom Optimizer of bg16 gave the best result which was 96.3% accuracy. And in my face mask detection. I used last night 50 grass that 101 that's that institution. No Inception weekly and vgg 16 and we did 19 out of which again, we need 16 the iron model if the best I am super Optimizer, which was 19917.",3,Great! Let's dive into your resume. I see you've done some interesting work with IoT malware detection and face mask detection using deep learning. Can you elaborate on the specific deep learning models you used for these projects and why you chose them?,"So first of all in my iot malware detection project, I use the I use the Deep learning CNN model which which is I used to know I tried I tried and tried to implement various models such as vgt 16 PG 19 and instruction V3 and out of them gdg, 16 gave the best accuracy. So I did some hyperparameter tuning and the and the atom Optimizer of bg16 gave the best result which was 96.3% accuracy. And in my face mask detection. I used last night 50 grass that 101 that's that institution. No Inception weekly and vgg 16 and we did 19 out of which again, we need 16 the iron model if the best I am super Optimizer, which was 19917.",2,True
773,773,773,773,"Interviewer: I see, that's a crucial step for ensuring the relevance of the responses. Shifting focus a bit, could you elaborate on the model architecture you used for the computer vision system to prevent accidents in warehouses during your internship at Optisol Business Solutions? What led you to choose that particular architecture?  

Interviewee: Yeah for forklift safety automation. We have used several pre-trained competition models like yellow and efficient that d0 model and other other models, but specifically for the efficient at d 0 so it will reduce the false positives and false negatives in that particular scenario since the folk of a real-time one. So if the person is present in that, but it failure to detect the person it might end up with an accident for the purpose. We have to reduce both the false positive and false negative. So if you consider this the efficient at d0 model is more efficient in that and we just retrained or a fine tune with the particular model for directing the forklift because the efficient at model is trying to Coco data set which does not contain the class called forklift for the purpose. We retrain the official model and it or production is a it is build upon the efficient model and upon the they have used to the buy Polar classifier upon the which is more effective on",3,"I see, that's a crucial step for ensuring the relevance of the responses. Shifting focus a bit, could you elaborate on the model architecture you used for the computer vision system to prevent accidents in warehouses during your internship at Optisol Business Solutions? What led you to choose that particular architecture?","Yeah for forklift safety automation. We have used several pre-trained competition models like yellow and efficient that d0 model and other other models, but specifically for the efficient at d 0 so it will reduce the false positives and false negatives in that particular scenario since the folk of a real-time one. So if the person is present in that, but it failure to detect the person it might end up with an accident for the purpose. We have to reduce both the false positive and false negative. So if you consider this the efficient at d0 model is more efficient in that and we just retrained or a fine tune with the particular model for directing the forklift because the efficient at model is trying to Coco data set which does not contain the class called forklift for the purpose. We retrain the official model and it or production is a it is build upon the efficient model and upon the they have used to the buy Polar classifier upon the which is more effective on",2,True
781,781,781,781,"Interviewer: Great! I see from your resume that you've worked on AI solutions for financial models at Riskfuel. Can you walk me through the approach you took to develop and deploy these AI solutions, particularly in terms of the models you chose and the challenges you faced with deployment?  


Interviewee: Up, right? So the models that I developed for risk fuel corresponded to financial asset pricing or non-standard vanilla options effectively. What we did was we would replace a Monte Carlo based pricer for a bank with a regression based or a standard rectangular and that for basically solving a regression problem. What would happen is is that because it's not because it is neural network-based. It was extremely fast compared to a Monte Carlo based Chrysler and because of the matrix multiplication and nonlinear activation functions, which are easily parallelizable. You can reduce computation time from let's say a few minutes for each option price down to like less than 70 milliseconds the types of training or the models that we worked on were resnet models with residual skip connections or if we had to do generative modeling. We use something called a variational auto encoder.",5,"Great! I see from your resume that you've worked on AI solutions for financial models at Riskfuel. Can you walk me through the approach you took to develop and deploy these AI solutions, particularly in terms of the models you chose and the challenges you faced with deployment?","Up, right? So the models that I developed for risk fuel corresponded to financial asset pricing or non-standard vanilla options effectively. What we did was we would replace a Monte Carlo based pricer for a bank with a regression based or a standard rectangular and that for basically solving a regression problem. What would happen is is that because it's not because it is neural network-based. It was extremely fast compared to a Monte Carlo based Chrysler and because of the matrix multiplication and nonlinear activation functions, which are easily parallelizable. You can reduce computation time from let's say a few minutes for each option price down to like less than 70 milliseconds the types of training or the models that we worked on were resnet models with residual skip connections or if we had to do generative modeling. We use something called a variational auto encoder.",3,True
784,784,784,784,"Interviewer: Great! I see from your resume that you've worked on a variety of projects, including the Advanced Traffic Monitoring and Analysis System. Can you walk me through how you improved the F1 score by 20% in this project? What specific techniques or models did you employ to achieve this improvement?  

Interviewee: Yeah, so this is a research project at my University. I developed and designed a multi object detection model which tracks with yolo V5 Nano small medium and large data sets using deep sort algorithm. It actually classifies four kinds of vehicles on highways such as different trucks type of trucks like arterial truck pickup truck single unit trucks and vans and it estimates the traffic flow based on that. Now how the traffic flow is exactly estimated. So there is an algorithm which is special temporal unsupervised clustering algorithm, which captures the Which captures the information using mobile devices of the driver and using a nearest neighbor of with the count of ks3 and some rule mapping algorithms to segment the tragedy of vehicles we can forecast the F1 score and increase it by 20%",3,"Great! I see from your resume that you've worked on a variety of projects, including the Advanced Traffic Monitoring and Analysis System. Can you walk me through how you improved the F1 score by 20% in this project? What specific techniques or models did you employ to achieve this improvement?","Yeah, so this is a research project at my University. I developed and designed a multi object detection model which tracks with yolo V5 Nano small medium and large data sets using deep sort algorithm. It actually classifies four kinds of vehicles on highways such as different trucks type of trucks like arterial truck pickup truck single unit trucks and vans and it estimates the traffic flow based on that. Now how the traffic flow is exactly estimated. So there is an algorithm which is special temporal unsupervised clustering algorithm, which captures the Which captures the information using mobile devices of the driver and using a nearest neighbor of with the count of ks3 and some rule mapping algorithms to segment the tragedy of vehicles we can forecast the F1 score and increase it by 20%",3,True
785,785,785,785,"Interviewer: Deploying multiple smaller models in parallel for Confidence Code calculation is a clever strategy. How do you plan to handle the integration of these multiple models' outputs and the Confidence Code calculation into a cohesive and easily interpretable response for the end user?  


Interviewee: So I think in different models, like let's say we have Mistral Gemma and llama we can deploy them. There are already API Services which provide you access to different these different models. In initialized different instances for you, so that can be directly done or we can deploy these models using AWS AWS specifically instances that can be used to deploy large language models. and using these Services we can apply these models easily and if there are a lot of users interacting with these models, so AWS also provides auto-scaling autoscaling methods that automatically scales the number of requests that can be handled by a particular system. So and all the pre-processing that is done, like combining model outputs from combining outputs from different model can be done on like using a different API and kind of Ah, like creating a simple fast API based method for handling requests. So this basic first API will get the input from user create a prompt send this prompt create different prompts for different models hit different apis which are which have different models deployed on them get the results from their and then this calculates a score and create a separate. And create the final output or what we can do is once we get outputs from the smaller models, we can combine them together and ask another large language model to summarize those results or to giving these outputs how to better. What is the best response that can be provided to the users giving outputs and scores from different small skin models.",3,Deploying multiple smaller models in parallel for Confidence Code calculation is a clever strategy. How do you plan to handle the integration of these multiple models' outputs and the Confidence Code calculation into a cohesive and easily interpretable response for the end user?,"So I think in different models, like let's say we have Mistral Gemma and llama we can deploy them. There are already API Services which provide you access to different these different models. In initialized different instances for you, so that can be directly done or we can deploy these models using AWS AWS specifically instances that can be used to deploy large language models. and using these Services we can apply these models easily and if there are a lot of users interacting with these models, so AWS also provides auto-scaling autoscaling methods that automatically scales the number of requests that can be handled by a particular system. So and all the pre-processing that is done, like combining model outputs from combining outputs from different model can be done on like using a different API and kind of Ah, like creating a simple fast API based method for handling requests. So this basic first API will get the input from user create a prompt send this prompt create different prompts for different models hit different apis which are which have different models deployed on them get the results from their and then this calculates a score and create a separate. And create the final output or what we can do is once we get outputs from the smaller models, we can combine them together and ask another large language model to summarize those results or to giving these outputs how to better. What is the best response that can be provided to the users giving outputs and scores from different small skin models.",3,True
786,786,786,786,"Interviewer: I'm sorry for the interruption. Could you please continue your response?  


Interviewee: conversation part was mostly in yeah. So, basically, the quantization of the original stable division model released by the authors was in, floating point 16. So what we tried is we reduced it to floating point 8. And, because of the reduction, there was a reduction in the size. And, this was done by, fixed type of quantization. So we fixed the weight size to 14.8, and we tried to reduce the size of the weights.",3,I'm sorry for the interruption. Could you please continue your response?,"conversation part was mostly in yeah. So, basically, the quantization of the original stable division model released by the authors was in, floating point 16. So what we tried is we reduced it to floating point 8. And, because of the reduction, there was a reduction in the size. And, this was done by, fixed type of quantization. So we fixed the weight size to 14.8, and we tried to reduce the size of the weights.",3,True
796,796,796,796,"Interviewer: It's good to hear that you have hands-on experience with AWS services like EC2, Docker, and SageMaker, and that you've been involved in CI/CD pipelines and model deployment. Since you mentioned your work with NLP Transformer models, can you tell me about the research project you did on understanding idioms in these models? What were the key challenges and findings from that project? 

Interviewee: Yeah, great question. So the project that I was doing was a research project solely based on, you know understanding how and to what extent Transformers, you know process idioms. So idioms Have non-literal meanings. So for example, if somebody says kick the bucket the meaning of the idiom translates to somebody dying rather than somebody literally clicking a bucket so things like that are very context-based and as we know transform our specialty some, you know, initial Farmers like bird, they are they might not be very good at recognizing the hidden meaning of the words. So I read a good amount of research papers on what people did and stuff like that. So I am employed. I actually perform two experiments as part of a project. The first was seeing whether the Transformers understand idioms as part of a language or as a no Downstream tasks, right? So to this extent we employed something called adapters. So adapters are, you know Common these days but back then it was something really new. So we used a framework called Maddox, so it was made by even Village from Cambridge. So we provide language adapters and tasks adapters towards learning whether the idiom the potentially idiomatic phrase in the sentence was used in a literal sense or idiomatic sense. So the data set was a binary data set. And the Transformer was, you know trained and evaluated to predict whether the potentially idiomatic phrase was used in literal sense or you know idiomatic sense. So this data set of multilingual Portuguese plus English. So to understand how Transformers, you know, understand this. So I trained the, you know Transformer to identify idiomatic meaning in English then evaluated it on Portuguese and the same way. I trained it on Portuguese and evaluated evaluated it on English, so if the Transformer learns how to recognize idioms as a downstream task the transfer across languages would be high.Um considering we have language adapters just for understanding the language. So the only task essentially is to understand how to or what to do to identify the idioms, right? So, so I give me one minute. Sorry. Yeah, so the result was that the transfer was not really good. So training it on English did not translate well into Portuguese so that experiment really concluded that idioms and their idiomatic meanings are lit are really learned as a part of a language rather than So that was the first experiment that I bought from. My second experiment is you know.",4,"It's good to hear that you have hands-on experience with AWS services like EC2, Docker, and SageMaker, and that you've been involved in CI/CD pipelines and model deployment. Since you mentioned your work with NLP Transformer models, can you tell me about the research project you did on understanding idioms in these models? What were the key challenges and findings from that project?","Yeah, great question. So the project that I was doing was a research project solely based on, you know understanding how and to what extent Transformers, you know process idioms. So idioms Have non-literal meanings. So for example, if somebody says kick the bucket the meaning of the idiom translates to somebody dying rather than somebody literally clicking a bucket so things like that are very context-based and as we know transform our specialty some, you know, initial Farmers like bird, they are they might not be very good at recognizing the hidden meaning of the words. So I read a good amount of research papers on what people did and stuff like that. So I am employed. I actually perform two experiments as part of a project. The first was seeing whether the Transformers understand idioms as part of a language or as a no Downstream tasks, right? So to this extent we employed something called adapters. So adapters are, you know Common these days but back then it was something really new. So we used a framework called Maddox, so it was made by even Village from Cambridge. So we provide language adapters and tasks adapters towards learning whether the idiom the potentially idiomatic phrase in the sentence was used in a literal sense or idiomatic sense. So the data set was a binary data set. And the Transformer was, you know trained and evaluated to predict whether the potentially idiomatic phrase was used in literal sense or you know idiomatic sense. So this data set of multilingual Portuguese plus English. So to understand how Transformers, you know, understand this. So I trained the, you know Transformer to identify idiomatic meaning in English then evaluated it on Portuguese and the same way. I trained it on Portuguese and evaluated evaluated it on English, so if the Transformer learns how to recognize idioms as a downstream task the transfer across languages would be high.Um considering we have language adapters just for understanding the language. So the only task essentially is to understand how to or what to do to identify the idioms, right? So, so I give me one minute. Sorry. Yeah, so the result was that the transfer was not really good. So training it on English did not translate well into Portuguese so that experiment really concluded that idioms and their idiomatic meanings are lit are really learned as a part of a language rather than So that was the first experiment that I bought from. My second experiment is you know.",3,True
