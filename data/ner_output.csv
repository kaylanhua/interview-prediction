Response,NER_Entities,Label,Prediction
"That's a very good question. So for dashboard what I did was I had in my complete if we had some we had a team for business intelligence. They used to work in on creating dashboards and Tableau as well as power bi so I had sit with them and ask them like if I give you just data of reviews, what do you want to see? I had talked with multiple people like what they want to see in a just in the reviews in kpis. Let's say I am developing a KP what they want to see what they wanted to know apart from it. I've been working for a client who was also working on reviews and I got some ideas over there. Like let's say showing them the how the product has evolved over here. So let's say we have a Samsung a series phone. So how the A52 a53 a 54 so how it has evolved over years. What do people are talking about it? When it was launched people were completely about camera. The next iteration people they didn't complain about cameras. They were complaining about something else or let's say, oh they had perfected the phone something like this. So they wanna show a line chart for this and then showing them top keywords to people are using so let's see people are using the word. battery a lot since it is a since it's a very good phone for have that has a amazing battery life of six or seven hours and people are talking about battery. I can just show them a chart or bar graph that showed that battery has a most expensive word similarly to English sentences. They told me something else like display or brightness. So brightens is very low for the phone. So people are talking Negatively about this brightness so we can have a bar chart that shows that brightness is on the top of negative words apart from the other. other complaints that people have and yeah, so that's it.","(Tableau, Samsung, the A52 a53, 54, years, six or seven hours, English)",3,3
"Yeah, initially since we are at the correlation Matrix, we found out the various features, which have direct impact on the target value. So based on those Target features and also soon after evaluating our logistic regression model we add the feature importance being checked so I identified that the feature which were selected were almost I mean full in the prediction. So those features are playing a big role in the prediction? Or all the accuracy was good, and the model has been being implemented.","(Target,)",2,2
"In web scraping, I have faced multiple challenges. I will describe each one in detail earlier first. We are used to scrape any very few sites like not to sites and scraping them is they are just static Pages like basic HTML if you load the website, you'll get the HTML you can just scrape it from there. But with the with the number of parties that are growing with the chatbot and the company we need to scrape multiple websites and each website comes in different formats. So I'm asked static websites some come as an infinite scroll upside and some commas like toggle ones like you have to use some feature like sidebar or something to scroll between the news articles. So yeah, what have done at this time was I used to study how these website is frame. Like what is the essential HTML structure that is followed and how is the class structure given to each component in the website and if there is any common class, how can you differentiate? Particular content I want from other ones which share this common class like using other properties or Keys available in the HTML tag and some kind of that. I had landed through debugging the site and doing some test runs and in finance clothing have done that with the puppeteer. Failure. We used access for simple scripting which doesn't work. Then we shifted to Puppeteer and when we shifted to Puppeteer when we automated this whole scraping and cleaning and everything process, we Face difficulties because of multi-threading what happened what used to happen was there used to be the scraper used to work on some websites. And if any website is loading slow, it used to exit after 30 seconds and it will not address it again. So then I resist about what are the other ways we can do then I got to know about Puppeteer clusters, which is essentially you open fights in a chrome instance and do balance scraping which increases the speed of scraping and also that also has a feature of sending back to the cube suppose if any site is a or any subside in a website, which we are supposed to scrape. He's a not loading fast. It could be due to network reason or it could be due to the server problem at the website holder itself What we have done was if any exception has happened during the scraping. We used to recent back into the cluster. So this class thank you maintains a list of URLs. It needs to scrape and when something is filled it is added back to the scraping happens until this all these URLs are scraped safely and embedded into the database and that is one difficult. I saw the property clusters and a we used to scrape only static sites, and then I converted into scrolling sites and INF. school websites, these are all I just done with Puppeteer and various extensions it has","(first, Puppeteer, Puppeteer, 30 seconds, chrome, one, Puppeteer)",4,3
You could fit a gaussian mixture model to the multivariate low latent data set and then you can see whether or not the number of components match the same where the distribution of the mixtures is. Roughly the same as the actual Market data. In terms of training the vae on the best method that actually worked was actually adding in a penalization term. For the wings on a VA and by penalizing for the variation the edges you're able to generate services that are more coherent in terms of the market space. So there are two factors. They're so the learning rate when training a v But ultimately the pin isation term is the best way.,"(Market, two)",2,3
a custom callback,(),1,3
"So I have worked on many projects where I have used python so you can in my resume you can see my Publications. So all of my Publications the code I wrote was in Python and in technical projects also, you can see three projects and all the three projects are implemented in Python long language and for for practicing data structures and algorithms and for participating in competitive programming I C plus plus has my language for coding. So you see plus plus for combative programming and python for development of applications.","(Publications, Publications, Python, three, three, Python)",1,2
"So I would like to Define my Acura learning Loop, which can which runs continuously throughout the training model. So in that Loop, I'll use a while loop say that I'll use a while loop in that I'll Define like outside the loop. I'll Define the functions like information is calculate entropy and marginal difference and add them representative as calculate tsne pair was distances and return them and function called as combined scores where I combined these two functions. We are normally those scores. So I Define the functions outside the loop and I call them from inside the loop. So this wise this way I can prevent generally functions for not losing its generated characteristics. Okay, then I trained the model inside the loop and I'll give the users the circular uncertainty samples. Like I'll calculate uncertainty through this information and I will fix a threshold if our my uncertainty score is greater than the threshold the particular samples to my human to label them. So in this way, I can make sure that the functions don't close its generator characteristics, but placing them outside of the loop, even they were operative inside of the loop.","(Acura, two)",2,2
"A transfer learning is where we use the existing domain knowledge of a given neural network or any machine learning algorithm, right? Whatever parameters we have extracted and we applied to a new domain. So for example domain domain Knowledge from extracting. Parts of speech by but right you can use the same domain knowledge for a different application. for example If you can nowadays last language models essentially predict the next word in the sequence the sequence right? So essentially you can use the same. Weights of these networks and fine tune them for different tasks. The different tasks can be questioned answered. It can be generating essays. It can be. You know, very fine. The authenticity of a particular paragraph, right? So this is what transfer learning essentially is you transfer the knowledge obtained in a given domain and then you apply it in a different domain.",(),3,3
"All right. So rasa and LP architecture is known to be high low latency and high performance architecture. That's why it became a choice a good choice for the particular implementation. It is highly used in online settings and hence is known for low latency use cases dealing with low latency use cases. Secondly. It's not actually an llm based model. It is more on the NLP side and neural network model that is not a large language model, but it works really well on the intended entity. Use use cases like named entity text extraction and intent classifiers in terms of metric that we had to incorporate. There was a there were different sorts of metrics that we use to evaluate the success of our model or the performance in general it is it was something like the out of the top 10 recommendation how many recommendations were getting clicks how many recommendation turned out to be useful apart from that? We also had other metric like other metric like taking into account. Whether the recommendations were ordered in the manner that was most useful to the most useful to the users. Therein. We incorporated the normalized discounted cumulative gain metric wherein we could judge the ranking of the recommendation that we were providing.","(Secondly, 10)",3,3
Actually at that time it was actually three years ago. My internship was a three years ago. So at that time we didn't have and we were at a very base level of startup. So we didn't have like the much more data to like evaluate or two filter out the noise from so we had a very compact data and we had to use all the data actually and it was select the data selected by the human experts so we didn't have to worry about the noise actually. So that's the thing.,"(three years ago, three years ago, two)",1,2
